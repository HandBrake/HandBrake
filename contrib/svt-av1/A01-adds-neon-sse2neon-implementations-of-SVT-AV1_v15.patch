From c2466e6f97a2975dbfddb140f442902442427415 Mon Sep 17 00:00:00 2001
From: Ghost
Date: Wed, 20 Sep 2023 09:31:25 +0200
Subject: [PATCH] Neon optimizations.

---
 CMakeLists.txt                                |   33 +
 Source/API/EbSvtAv1.h                         |    6 +
 Source/App/EncApp/EbAppConfig.c               |    2 +-
 Source/Lib/Common/ASM_AVX2/convolve_avx2.h    |  145 +-
 Source/Lib/Common/ASM_NEON/CMakeLists.txt     |   14 +
 .../Common/ASM_NEON/aom_convolve_copy_neon.c  |   51 +
 .../ASM_NEON/aom_subpixel_8t_intrin_neon.c    |  524 +++++
 Source/Lib/Common/ASM_NEON/convolve.c         | 1288 ++++++++++++
 Source/Lib/Common/ASM_NEON/convolve.h         |  139 ++
 Source/Lib/Common/ASM_NEON/convolve8_neon.c   |  137 ++
 Source/Lib/Common/ASM_NEON/convolve8_neon.h   |   31 +
 Source/Lib/Common/ASM_NEON/convolve_neon.c    | 1858 +++++++++++++++++
 Source/Lib/Common/ASM_NEON/convolve_neon.h    |  298 +++
 Source/Lib/Common/ASM_NEON/mem_neon.h         |  546 +++++
 Source/Lib/Common/ASM_NEON/transpose_neon.h   |  755 +++++++
 .../Common/ASM_NEON/wiener_convolve_neon.c    |  544 +++++
 Source/Lib/Common/ASM_SSE2/CMakeLists.txt     |   19 +-
 .../ASM_SSE2/EbAvcStyleMcp_Intrinsic_SSE2.c   |    2 +-
 .../EbDeblockingFilter_Intrinsic_SSE2.c       |    2 +-
 .../ASM_SSE2/EbHighbdIntraPrediction_SSE2.h   |    2 +-
 .../EbIntraPrediction_AV1_Intrinsic_SSE2.c    |    2 +-
 .../ASM_SSE2/EbPackUnPack_Intrinsic_SSE2.c    |    2 +-
 .../EbPictureOperators_Intrinsic_SSE2.c       |    6 +-
 .../Common/ASM_SSE2/EbPictureOperators_SSE2.h |    2 +-
 Source/Lib/Common/ASM_SSE2/av1_txfm_sse2.h    |    2 +-
 Source/Lib/Common/ASM_SSE2/convolve_2d_sse2.c |    2 +-
 Source/Lib/Common/ASM_SSE2/convolve_sse2.c    |    2 +-
 .../Common/ASM_SSE2/highbd_intrapred_sse2.c   |    2 +-
 .../Common/ASM_SSE2/highbd_subtract_sse2.c    |    2 +-
 .../Common/ASM_SSE2/jnt_convolve_2d_sse2.c    |    2 +-
 .../Lib/Common/ASM_SSE2/jnt_convolve_sse2.c   |    2 +-
 Source/Lib/Common/ASM_SSE2/lpf_common_sse2.h  |    2 +-
 Source/Lib/Common/ASM_SSE2/synonyms.h         |    2 +-
 Source/Lib/Common/ASM_SSE2/transpose_sse2.h   |    2 +-
 .../Common/ASM_SSE2/wiener_convolve_sse2.c    |    2 +-
 Source/Lib/Common/ASM_SSE4_1/CMakeLists.txt   |    4 +-
 .../Common/ASM_SSE4_1/EbBlend_a64_mask_sse4.c |    2 +-
 Source/Lib/Common/ASM_SSE4_1/EbBlend_sse4.h   |    2 +-
 .../EbIntraPrediction16bit_Intrinsic_SSE4_1.c |    2 +-
 .../Lib/Common/ASM_SSE4_1/EbMemory_SSE4_1.h   |    2 +-
 .../EbPictureOperators_Intrinsic_SSE4_1.c     |    2 +-
 .../ASM_SSE4_1/av1_convolve_scale_sse4.c      |    2 +-
 .../Lib/Common/ASM_SSE4_1/cdef_block_sse4_1.c |    2 +-
 .../Lib/Common/ASM_SSE4_1/filterintra_sse4.c  |    2 +-
 .../ASM_SSE4_1/highbd_convolve_2d_sse4.c      |    3 +-
 .../Common/ASM_SSE4_1/highbd_inv_txfm_sse4.c  |    2 +-
 .../ASM_SSE4_1/highbd_jnt_convolve_sse4.c     |    2 +-
 .../ASM_SSE4_1/highbd_txfm_utility_sse4.h     |    2 +-
 .../Lib/Common/ASM_SSE4_1/reconinter_sse4.c   |    3 +-
 .../Lib/Common/ASM_SSE4_1/selfguided_sse4.c   |    2 +-
 .../Lib/Common/ASM_SSE4_1/warp_plane_sse4.c   |  169 +-
 Source/Lib/Common/ASM_SSSE3/CMakeLists.txt    |   12 +-
 .../EbHighbdIntraPrediction_Intrinsic_SSSE3.c |    2 +-
 .../ASM_SSSE3/aom_subpixel_8t_intrin_ssse3.c  |    5 +-
 .../Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.c |    2 +-
 .../Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.h |    3 +-
 .../ASM_SSSE3/highbd_convolve_2d_ssse3.c      |    2 +-
 .../Common/ASM_SSSE3/highbd_convolve_ssse3.c  |    2 +-
 .../ASM_SSSE3/highbd_wiener_convolve_ssse3.c  |    2 +-
 Source/Lib/Common/ASM_SSSE3/intrapred_ssse3.c |    2 +-
 .../Lib/Common/ASM_SSSE3/jnt_convolve_ssse3.c |    2 +-
 .../Lib/Common/ASM_SSSE3/reconinter_ssse3.c   |    2 +-
 Source/Lib/Common/CMakeLists.txt              |    7 +
 Source/Lib/Common/Codec/EbDefinitions.h       |    4 +-
 Source/Lib/Common/Codec/common_dsp_rtcd.c     |  280 ++-
 Source/Lib/Common/Codec/common_dsp_rtcd.h     |   28 +-
 Source/Lib/Common/Codec/simd.h                |   20 +
 Source/Lib/Decoder/CMakeLists.txt             |   10 +
 Source/Lib/Encoder/ASM_NEON/CMakeLists.txt    |   15 +
 .../ASM_NEON/EbComputeSAD_Intrinsic_NEON.c    |  470 +++++
 .../Lib/Encoder/ASM_NEON/EbComputeSAD_NEON.h  |   79 +
 .../Encoder/ASM_NEON/EbRestorationPick_NEON.c |  304 +++
 Source/Lib/Encoder/ASM_NEON/picksrt_neon.c    |  149 ++
 .../Encoder/ASM_NEON/subpel_variance_neon.c   |  481 +++++
 .../Encoder/ASM_NEON/subpel_variance_neon.h   |  116 +
 Source/Lib/Encoder/ASM_NEON/sum_neon.h        |   74 +
 Source/Lib/Encoder/ASM_NEON/variance_neon.c   |  647 ++++++
 Source/Lib/Encoder/ASM_SSE2/CMakeLists.txt    |   14 +-
 .../ASM_SSE2/EbComputeMean_Intrinsic_SSE2.c   |    2 +-
 .../EbMeSadCalculation_Intrinsic_SSE2.c       |    2 +-
 Source/Lib/Encoder/ASM_SSE2/EbVariance_SSE2.h |    2 +-
 Source/Lib/Encoder/ASM_SSE2/encodetxb_sse2.c  |    2 +-
 Source/Lib/Encoder/ASM_SSE2/fft_sse2.c        |    2 +-
 .../Encoder/ASM_SSE2/highbd_variance_sse2.c   |    3 +
 Source/Lib/Encoder/ASM_SSE2/variance_sse2.c   |    8 +-
 Source/Lib/Encoder/ASM_SSE4_1/CMakeLists.txt  |    4 +-
 Source/Lib/Encoder/ASM_SSE4_1/EbCdef_sse4.c   |    2 +-
 .../EbComputeSAD_Intrinsic_SSE4_1.c           |    2 +-
 .../ASM_SSE4_1/EbTemporalFiltering_SSE4_1.c   |    3 +-
 .../Encoder/ASM_SSE4_1/av1_quantize_sse4_1.c  |    4 +-
 .../Lib/Encoder/ASM_SSE4_1/av1_txfm1d_sse4.h  |    3 +-
 Source/Lib/Encoder/ASM_SSE4_1/av1_txfm_sse4.h |    2 +-
 .../Encoder/ASM_SSE4_1/corner_match_sse4.c    |    2 +-
 .../Lib/Encoder/ASM_SSE4_1/encodetxb_sse4.c   |    3 +-
 .../Encoder/ASM_SSE4_1/highbd_fwd_txfm_sse4.c |    3 +-
 Source/Lib/Encoder/ASM_SSE4_1/pickrst_sse4.c  |    3 +-
 Source/Lib/Encoder/ASM_SSSE3/CMakeLists.txt   |   11 +-
 .../Encoder/ASM_SSSE3/variance_impl_ssse3.c   |    8 +-
 Source/Lib/Encoder/CMakeLists.txt             |   43 +
 Source/Lib/Encoder/Codec/CMakeLists.txt       |   12 +
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.c       |  195 +-
 Source/Lib/Encoder/Codec/aom_dsp_rtcd.h       |  157 +-
 Source/Lib/Encoder/Globals/EbEncHandle.c      |    6 +-
 Source/Lib/Encoder/Globals/EbEncSettings.c    |   11 +
 104 files changed, 9542 insertions(+), 320 deletions(-)
 create mode 100644 Source/Lib/Common/ASM_NEON/CMakeLists.txt
 create mode 100644 Source/Lib/Common/ASM_NEON/aom_convolve_copy_neon.c
 create mode 100644 Source/Lib/Common/ASM_NEON/aom_subpixel_8t_intrin_neon.c
 create mode 100644 Source/Lib/Common/ASM_NEON/convolve.c
 create mode 100644 Source/Lib/Common/ASM_NEON/convolve.h
 create mode 100644 Source/Lib/Common/ASM_NEON/convolve8_neon.c
 create mode 100644 Source/Lib/Common/ASM_NEON/convolve8_neon.h
 create mode 100644 Source/Lib/Common/ASM_NEON/convolve_neon.c
 create mode 100644 Source/Lib/Common/ASM_NEON/convolve_neon.h
 create mode 100644 Source/Lib/Common/ASM_NEON/mem_neon.h
 create mode 100644 Source/Lib/Common/ASM_NEON/transpose_neon.h
 create mode 100644 Source/Lib/Common/ASM_NEON/wiener_convolve_neon.c
 create mode 100644 Source/Lib/Common/Codec/simd.h
 create mode 100644 Source/Lib/Encoder/ASM_NEON/CMakeLists.txt
 create mode 100644 Source/Lib/Encoder/ASM_NEON/EbComputeSAD_Intrinsic_NEON.c
 create mode 100644 Source/Lib/Encoder/ASM_NEON/EbComputeSAD_NEON.h
 create mode 100644 Source/Lib/Encoder/ASM_NEON/EbRestorationPick_NEON.c
 create mode 100644 Source/Lib/Encoder/ASM_NEON/picksrt_neon.c
 create mode 100644 Source/Lib/Encoder/ASM_NEON/subpel_variance_neon.c
 create mode 100644 Source/Lib/Encoder/ASM_NEON/subpel_variance_neon.h
 create mode 100644 Source/Lib/Encoder/ASM_NEON/sum_neon.h
 create mode 100644 Source/Lib/Encoder/ASM_NEON/variance_neon.c

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 58642d1..b8f3e17 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -10,6 +10,7 @@
 #
 
 cmake_minimum_required(VERSION 3.16)
+INCLUDE( FindPackageHandleStandardArgs )
 
 if("${CMAKE_CURRENT_SOURCE_DIR}" STREQUAL "${CMAKE_CURRENT_BINARY_DIR}")
     message(WARNING "Building in-source is highly not recommended\n"
@@ -52,6 +53,30 @@ check_c_source_compiles("
 int main(void) {}
 " HAVE_X86_PLATFORM)
 
+check_c_source_compiles("
+#if defined(__aarch64__)
+#else
+#error \"Non-aarch64\"
+#endif
+int main(void) {}
+" HAVE_AARCH64_PLATFORM)
+
+if(NOT COMPILE_C_ONLY AND HAVE_AARCH64_PLATFORM)
+    find_path(SSE2NEON_INCLUDE
+    NAMES
+        sse2neon.h
+    HINTS
+      ${SSE2NEON_INCLUDE_DIR}
+    )
+    find_package_handle_standard_args( sse2neon "DEFAULT_MSG"
+    SSE2NEON_INCLUDE
+    )
+    if( SSE2NEON_FOUND )
+        include_directories(${SSE2NEON_INCLUDE})
+    endif()
+    add_definitions(-DARCH_AARCH64=1)
+endif()
+
 if(NOT COMPILE_C_ONLY AND HAVE_X86_PLATFORM)
     find_program(YASM_EXE yasm)
     option(ENABLE_NASM "Use nasm if available (Uses yasm by default if found)" OFF)
@@ -304,6 +329,9 @@ if(SVT_AV1_LTO)
     endif()
 endif()
 
+if(HAVE_AARCH64_PLATFORM)
+    check_both_flags_add(PREPEND -march=armv8-a+fp+simd)
+endif()
 check_both_flags_add(PREPEND -Wextra -Wformat -Wformat-security)
 
 if(MSVC)
@@ -343,6 +371,7 @@ endif()
 
 include(CheckSymbolExists)
 
+if(HAVE_X86_PLATFORM)
 check_symbol_exists("_mm512_extracti64x4_epi64" "immintrin.h" HAS_AVX512)
 if(HAS_AVX512)
     option(ENABLE_AVX512 "Enable building avx512 code" OFF)
@@ -351,7 +380,11 @@ else()
 endif()
 if(ENABLE_AVX512)
     add_definitions(-DEN_AVX512_SUPPORT=1)
+    else()
+        add_definitions(-DEN_AVX512_SUPPORT=0)
+    endif()
 else()
+    option(ENABLE_AVX512 "Enable building avx512 code" OFF)
     add_definitions(-DEN_AVX512_SUPPORT=0)
 endif()
 
diff --git a/Source/API/EbSvtAv1.h b/Source/API/EbSvtAv1.h
index ad79044..e2f63d7 100644
--- a/Source/API/EbSvtAv1.h
+++ b/Source/API/EbSvtAv1.h
@@ -341,7 +341,13 @@ typedef uint64_t EbCpuFlags;
 #define EB_CPU_FLAGS_AVX512PF (1 << 13)
 #define EB_CPU_FLAGS_AVX512BW (1 << 14)
 #define EB_CPU_FLAGS_AVX512VL (1 << 15)
+#ifdef ARCH_X86_64
 #define EB_CPU_FLAGS_ALL ((EB_CPU_FLAGS_AVX512VL << 1) - 1)
+#endif // ARCH_X86_64
+#ifdef ARCH_AARCH64
+#define EB_CPU_FLAGS_NEON (1 << 0)
+#define EB_CPU_FLAGS_ALL ((EB_CPU_FLAGS_NEON << 1) - 1)
+#endif // ARCH_AARCH64
 #define EB_CPU_FLAGS_INVALID (1ULL << (sizeof(EbCpuFlags) * 8ULL - 1ULL))
 
 #ifdef __cplusplus
diff --git a/Source/App/EncApp/EbAppConfig.c b/Source/App/EncApp/EbAppConfig.c
index 4c4050f..1a6e180 100644
--- a/Source/App/EncApp/EbAppConfig.c
+++ b/Source/App/EncApp/EbAppConfig.c
@@ -1,4 +1,4 @@
-﻿/*
+/*
 * Copyright(c) 2019 Intel Corporation
 *
 * This source code is subject to the terms of the BSD 3-Clause Clear License and
diff --git a/Source/Lib/Common/ASM_AVX2/convolve_avx2.h b/Source/Lib/Common/ASM_AVX2/convolve_avx2.h
index 9a26444..b376bbf 100644
--- a/Source/Lib/Common/ASM_AVX2/convolve_avx2.h
+++ b/Source/Lib/Common/ASM_AVX2/convolve_avx2.h
@@ -15,7 +15,9 @@
 #include "convolve.h"
 #include "EbDefinitions.h"
 #include "EbInterPrediction.h"
+#ifndef ARCH_AARCH64
 #include "EbMemory_AVX2.h"
+#endif
 #include "EbMemory_SSE4_1.h"
 #include "synonyms.h"
 
@@ -84,7 +86,7 @@ static INLINE int32_t get_convolve_tap(const int16_t *const filter) {
     else
         return 8;
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void populate_coeffs_4tap_avx2(const __m128i coeffs_128, __m256i coeffs[2]) {
     const __m256i coeffs_256 = _mm256_broadcastsi128_si256(coeffs_128);
 
@@ -93,7 +95,7 @@ static INLINE void populate_coeffs_4tap_avx2(const __m128i coeffs_128, __m256i c
     // coeffs 4 5 4 5 4 5 4 5
     coeffs[1] = _mm256_shuffle_epi8(coeffs_256, _mm256_set1_epi16(0x0a08u));
 }
-
+#endif
 static INLINE void populate_coeffs_6tap_avx2(const __m128i coeffs_128, __m256i coeffs[3]) {
     const __m256i coeffs_256 = _mm256_broadcastsi128_si256(coeffs_128);
 
@@ -201,7 +203,7 @@ static INLINE void prepare_half_coeffs_8tap_ssse3(const InterpFilterParams *cons
     // coeffs 6 7 6 7 6 7 6 7
     coeffs[3] = _mm_shuffle_epi8(coeffs_1, _mm_set1_epi16(0x0e0cu));
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void prepare_half_coeffs_2tap_avx2(const InterpFilterParams *const filter_params, const int32_t subpel_q4,
                                                  __m256i *const coeffs /* [1] */) {
     const int16_t *const filter        = av1_get_interp_filter_subpel_kernel(*filter_params, subpel_q4 & SUBPEL_MASK);
@@ -220,7 +222,6 @@ static INLINE void prepare_half_coeffs_2tap_avx2(const InterpFilterParams *const
     // coeffs 3 4 3 4 3 4 3 4
     *coeffs = _mm256_shuffle_epi8(coeffs_1, _mm256_set1_epi16(0x0200u));
 }
-
 static INLINE void prepare_half_coeffs_4tap_avx2(const InterpFilterParams *const filter_params, const int32_t subpel_q4,
                                                  __m256i *const coeffs /* [2] */) {
     const int16_t *const filter   = av1_get_interp_filter_subpel_kernel(*filter_params, subpel_q4 & SUBPEL_MASK);
@@ -265,7 +266,7 @@ static INLINE void prepare_half_coeffs_8tap_avx2(const InterpFilterParams *const
     const __m128i coeffs_1 = _mm_srai_epi16(coeffs_8, 1);
     populate_coeffs_8tap_avx2(coeffs_1, coeffs);
 }
-
+#endif
 static INLINE void prepare_coeffs_2tap_sse2(const InterpFilterParams *const filter_params, const int32_t subpel_q4,
                                             __m128i *const coeffs /* [1] */) {
     const int16_t *filter = av1_get_interp_filter_subpel_kernel(*filter_params, subpel_q4 & SUBPEL_MASK);
@@ -316,7 +317,7 @@ static INLINE void prepare_coeffs_8tap_sse2(const InterpFilterParams *const filt
     // coeffs 6 7 6 7 6 7 6 7
     coeffs[3] = _mm_shuffle_epi32(coeff, 0xff);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void prepare_coeffs_2tap_avx2(const InterpFilterParams *const filter_params, const int32_t subpel_q4,
                                             __m256i *const coeffs /* [1] */) {
     const int16_t *filter = av1_get_interp_filter_subpel_kernel(*filter_params, subpel_q4 & SUBPEL_MASK);
@@ -327,7 +328,6 @@ static INLINE void prepare_coeffs_2tap_avx2(const InterpFilterParams *const filt
     // coeffs 3 4 3 4 3 4 3 4
     coeffs[0] = _mm256_shuffle_epi32(coeff, 0x00);
 }
-
 static INLINE void prepare_coeffs_4tap_avx2(const InterpFilterParams *const filter_params, const int32_t subpel_q4,
                                             __m256i *const coeffs /* [2] */) {
     const int16_t *filter = av1_get_interp_filter_subpel_kernel(*filter_params, subpel_q4 & SUBPEL_MASK);
@@ -441,6 +441,7 @@ static INLINE void convolve_8tap_unapck_avx2(const __m256i s[6], __m256i ss[7])
     ss[5] = _mm256_unpackhi_epi16(s[2], s[3]);
     ss[6] = _mm256_unpackhi_epi16(s[4], s[5]);
 }
+#endif
 
 static INLINE __m128i convolve_2tap_ssse3(const __m128i ss[1], const __m128i coeffs[1]) {
     return _mm_maddubs_epi16(ss[0], coeffs[0]);
@@ -469,11 +470,10 @@ static INLINE __m128i convolve_8tap_ssse3(const __m128i ss[4], const __m128i coe
     const __m128i res_2367 = _mm_add_epi16(res_23, res_67);
     return _mm_add_epi16(res_0145, res_2367);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i convolve_2tap_avx2(const __m256i ss[1], const __m256i coeffs[1]) {
     return _mm256_maddubs_epi16(ss[0], coeffs[0]);
 }
-
 static INLINE __m256i convolve_4tap_avx2(const __m256i ss[2], const __m256i coeffs[2]) {
     const __m256i res_23 = _mm256_maddubs_epi16(ss[0], coeffs[0]);
     const __m256i res_45 = _mm256_maddubs_epi16(ss[1], coeffs[1]);
@@ -497,6 +497,7 @@ static INLINE __m256i convolve_8tap_avx2(const __m256i ss[4], const __m256i coef
     const __m256i res_2367 = _mm256_add_epi16(res_23, res_67);
     return _mm256_add_epi16(res_0145, res_2367);
 }
+#endif
 
 static INLINE __m128i convolve16_2tap_sse2(const __m128i ss[1], const __m128i coeffs[1]) {
     return _mm_madd_epi16(ss[0], coeffs[0]);
@@ -525,7 +526,7 @@ static INLINE __m128i convolve16_8tap_sse2(const __m128i ss[4], const __m128i co
     const __m128i res_4567 = _mm_add_epi32(res_45, res_67);
     return _mm_add_epi32(res_0123, res_4567);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i convolve16_2tap_avx2(const __m256i ss[1], const __m256i coeffs[1]) {
     return _mm256_madd_epi16(ss[0], coeffs[0]);
 }
@@ -589,19 +590,19 @@ static INLINE __m256i sr_y_round_avx2(const __m256i src) {
     const __m256i dst   = _mm256_add_epi16(src, round);
     return _mm256_srai_epi16(dst, FILTER_BITS - 1);
 }
-
+#endif
 static INLINE __m128i xy_x_round_sse2(const __m128i src) {
     const __m128i round = _mm_set1_epi16(2);
     const __m128i dst   = _mm_add_epi16(src, round);
     return _mm_srai_epi16(dst, 2);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i xy_x_round_avx2(const __m256i src) {
     const __m256i round = _mm256_set1_epi16(2);
     const __m256i dst   = _mm256_add_epi16(src, round);
     return _mm256_srai_epi16(dst, 2);
 }
-
+#endif
 static INLINE void xy_x_round_store_2x2_sse2(const __m128i res, int16_t *const dst) {
     const __m128i d = xy_x_round_sse2(res);
     _mm_storel_epi64((__m128i *)dst, d);
@@ -620,7 +621,7 @@ static INLINE void xy_x_round_store_8x2_sse2(const __m128i res[2], int16_t *cons
     _mm_storeu_si128((__m128i *)dst, r[0]);
     _mm_storeu_si128((__m128i *)(dst + 8), r[1]);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void xy_x_round_store_8x2_avx2(const __m256i res, int16_t *const dst) {
     const __m256i d = xy_x_round_avx2(res);
     _mm256_storeu_si256((__m256i *)dst, d);
@@ -636,7 +637,7 @@ static INLINE void xy_x_round_store_32_avx2(const __m256i res[2], int16_t *const
     _mm256_storeu_si256((__m256i *)dst, d0);
     _mm256_storeu_si256((__m256i *)(dst + 16), d1);
 }
-
+#endif
 static INLINE __m128i xy_y_round_sse2(const __m128i src) {
     const __m128i round = _mm_set1_epi32(1024);
     const __m128i dst   = _mm_add_epi32(src, round);
@@ -648,7 +649,7 @@ static INLINE __m128i xy_y_round_half_pel_sse2(const __m128i src) {
     const __m128i dst   = _mm_add_epi16(src, round);
     return _mm_srai_epi16(dst, 5);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i xy_y_round_avx2(const __m256i src) {
     const __m256i round = _mm256_set1_epi32(1024);
     const __m256i dst   = _mm256_add_epi32(src, round);
@@ -666,39 +667,40 @@ static INLINE __m256i xy_y_round_half_pel_avx2(const __m256i src) {
     const __m256i dst   = _mm256_add_epi16(src, round);
     return _mm256_srai_epi16(dst, 5);
 }
-
+#endif
 static INLINE __m128i jnt_y_round_sse2(const __m128i src) {
     const __m128i round = _mm_set1_epi16(2);
     const __m128i dst   = _mm_add_epi16(src, round);
     return _mm_srai_epi16(dst, 2);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i jnt_y_round_avx2(const __m256i src) {
     const __m256i round = _mm256_set1_epi16(2);
     const __m256i dst   = _mm256_add_epi16(src, round);
     return _mm256_srai_epi16(dst, 2);
 }
-
+#endif
 static INLINE __m128i jnt_avg_round_sse2(const __m128i src, const __m128i offset) {
     const __m128i dst = _mm_add_epi16(src, offset);
     return _mm_srai_epi16(dst, 2);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i jnt_avg_round_avx2(const __m256i src, const __m256i offset) {
     const __m256i dst = _mm256_add_epi16(src, offset);
     return _mm256_srai_epi16(dst, 2);
 }
-
+#endif
 static INLINE __m128i jnt_no_avg_round_sse2(const __m128i src, const __m128i offset) {
     const __m128i dst = _mm_add_epi16(src, offset);
     return _mm_srli_epi16(dst, 2);
 }
+#ifndef ARCH_AARCH64
 
 static INLINE __m256i jnt_no_avg_round_avx2(const __m256i src, const __m256i offset) {
     const __m256i dst = _mm256_add_epi16(src, offset);
     return _mm256_srli_epi16(dst, 2);
 }
-
+#endif
 static INLINE void pack_store_2x2_sse2(const __m128i res, uint8_t *const dst, const ptrdiff_t stride) {
     const __m128i d            = _mm_packus_epi16(res, res);
     *(int16_t *)dst            = (int16_t)_mm_cvtsi128_si32(d);
@@ -709,7 +711,7 @@ static INLINE void pack_store_4x2_sse2(const __m128i res, uint8_t *const dst, co
     const __m128i d = _mm_packus_epi16(res, res);
     store_u8_4x2_sse2(d, dst, stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void pack_store_4x2_avx2(const __m256i res, uint8_t *const dst, const ptrdiff_t stride) {
     const __m256i d  = _mm256_packus_epi16(res, res);
     const __m128i d0 = _mm256_castsi256_si128(d);
@@ -744,13 +746,13 @@ static INLINE void pack_store_32_avx2(const __m256i res0, const __m256i res1, ui
     const __m256i d = _mm256_permute4x64_epi64(t, 0xD8);
     _mm256_storeu_si256((__m256i *)dst, d);
 }
-
+#endif
 static INLINE void xy_y_round_store_2x2_sse2(const __m128i res, uint8_t *const dst, const ptrdiff_t stride) {
     const __m128i r  = xy_y_round_sse2(res);
     const __m128i rr = _mm_packs_epi32(r, r);
     pack_store_2x2_sse2(rr, dst, stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void xy_y_round_store_4x2_avx2(const __m256i res, uint8_t *const dst, const ptrdiff_t stride) {
     const __m256i r  = xy_y_round_avx2(res);
     const __m256i rr = _mm256_packs_epi32(r, r);
@@ -773,24 +775,25 @@ static INLINE void convolve_store_32_avx2(const __m256i res0, const __m256i res1
     const __m256i d = _mm256_packus_epi16(res0, res1);
     _mm256_storeu_si256((__m256i *)dst, d);
 }
-
+#endif
 static INLINE __m128i sr_x_round_sse2(const __m128i src) {
     const __m128i round = _mm_set1_epi16(34);
     const __m128i dst   = _mm_add_epi16(src, round);
     return _mm_srai_epi16(dst, 6);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i sr_x_round_avx2(const __m256i src) {
     const __m256i round = _mm256_set1_epi16(34);
     const __m256i dst   = _mm256_add_epi16(src, round);
     return _mm256_srai_epi16(dst, 6);
 }
-
+#endif
 static INLINE __m128i sr_y_round_sse2(const __m128i src) {
     const __m128i round = _mm_set1_epi16(32);
     const __m128i dst   = _mm_add_epi16(src, round);
     return _mm_srai_epi16(dst, FILTER_BITS - 1);
 }
+#ifndef ARCH_AARCH64
 
 static INLINE void sr_x_round_store_8x2_avx2(const __m256i res, uint8_t *const dst, const ptrdiff_t dst_stride) {
     const __m256i r = sr_x_round_avx2(res);
@@ -847,7 +850,7 @@ static INLINE void jnt_no_avg_store_16x2_avx2(const __m256i src0, const __m256i
     _mm256_storeu_si256((__m256i *)dst, d0);
     _mm256_storeu_si256((__m256i *)(dst + stride), d1);
 }
-
+#endif
 static INLINE __m128i x_convolve_2tap_2x2_sse4_1(const uint8_t *const src, const ptrdiff_t stride,
                                                  const __m128i coeffs[1]) {
     const __m128i sfl   = _mm_setr_epi8(0, 1, 1, 2, 4, 5, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0);
@@ -877,7 +880,7 @@ static INLINE void x_convolve_2tap_8x2_ssse3(const uint8_t *const src, const ptr
     r[0] = convolve_2tap_ssse3(&ss[0], coeffs);
     r[1] = convolve_2tap_ssse3(&ss[1], coeffs);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i x_convolve_2tap_8x2_avx2(const uint8_t *const src, const ptrdiff_t stride,
                                                const __m256i coeffs[1]) {
     __m128i s_128[2][2];
@@ -912,7 +915,7 @@ static INLINE void x_convolve_2tap_32_avx2(const uint8_t *const src, const __m25
     r[0] = convolve_2tap_avx2(&ss0, coeffs);
     r[1] = convolve_2tap_avx2(&ss1, coeffs);
 }
-
+#endif
 static INLINE __m128i x_convolve_4tap_2x2_ssse3(const uint8_t *const src, const ptrdiff_t stride,
                                                 const __m128i coeffs[2]) {
     const __m128i sfl0 = _mm_setr_epi8(0, 1, 1, 2, 8, 9, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0);
@@ -936,7 +939,7 @@ static INLINE __m128i x_convolve_4tap_4x2_ssse3(const uint8_t *const src, const
     ss[1] = _mm_shuffle_epi8(s, sfl1);
     return convolve_4tap_ssse3(ss, coeffs);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i x_convolve_6tap_8x2_avx2(const uint8_t *const src, const ptrdiff_t stride,
                                                const __m256i coeffs[3], const __m256i filt[3]) {
     const __m256i s_256 = loadu_8bit_16x2_avx2(src, stride);
@@ -978,7 +981,7 @@ SIMD_INLINE void x_convolve_8tap_32_avx2(const uint8_t *const src, const __m256i
     r[0] = x_convolve_8tap_avx2(s0_256, coeffs, filt);
     r[1] = x_convolve_8tap_avx2(s1_256, coeffs, filt);
 }
-
+#endif
 static INLINE __m128i y_convolve_2tap_2x2_ssse3(const uint8_t *const src, const ptrdiff_t stride,
                                                 const __m128i coeffs[1], __m128i s_16[2]) {
     __m128i s_128[2];
@@ -1002,7 +1005,7 @@ static INLINE __m128i y_convolve_2tap_4x2_ssse3(const uint8_t *const src, const
     const __m128i ss = _mm_unpacklo_epi8(s_128[0], s_128[1]);
     return convolve_2tap_ssse3(&ss, coeffs);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i y_convolve_2tap_8x2_avx2(const uint8_t *const src, const ptrdiff_t stride,
                                                const __m256i coeffs[1], __m128i s_64[2]) {
     __m256i s_256[2];
@@ -1037,7 +1040,7 @@ static INLINE void y_convolve_2tap_32_avx2(const uint8_t *const src, const __m25
     r[0]              = convolve_2tap_avx2(&ss0, coeffs);
     r[1]              = convolve_2tap_avx2(&ss1, coeffs);
 }
-
+#endif
 static INLINE __m128i y_convolve_4tap_2x2_ssse3(const uint8_t *const src, const ptrdiff_t stride,
                                                 const __m128i coeffs[2], __m128i s_16[4], __m128i ss_128[2]) {
     s_16[3]             = _mm_cvtsi32_si128(*(int16_t *)(src + stride));
@@ -1057,7 +1060,7 @@ static INLINE __m128i y_convolve_4tap_4x2_ssse3(const uint8_t *const src, const
     ss_128[1]           = _mm_unpacklo_epi8(src23, src34);
     return convolve_4tap_ssse3(ss_128, coeffs);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i y_convolve_4tap_8x2_avx2(const uint8_t *const src, const ptrdiff_t stride,
                                                const __m256i coeffs[2], __m128i s_64[4], __m256i ss_256[2]) {
     s_64[3]             = _mm_loadl_epi64((__m128i *)(src + stride));
@@ -1079,7 +1082,7 @@ static INLINE void y_convolve_4tap_16x2_avx2(const uint8_t *const src, const ptr
     r[0]                = convolve_4tap_avx2(ss_256, coeffs);
     r[1]                = convolve_4tap_avx2(ss_256 + 2, coeffs);
 }
-
+#endif
 static INLINE __m128i y_convolve_6tap_2x2_ssse3(const uint8_t *const src, const ptrdiff_t stride,
                                                 const __m128i coeffs[3], __m128i s_16[6], __m128i ss_128[3]) {
     s_16[5]             = _mm_cvtsi32_si128(*(int16_t *)(src + 3 * stride));
@@ -1089,7 +1092,7 @@ static INLINE __m128i y_convolve_6tap_2x2_ssse3(const uint8_t *const src, const
     ss_128[2]           = _mm_unpacklo_epi8(src45, src56);
     return convolve_6tap_ssse3(ss_128, coeffs);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void y_convolve_4tap_32x2_avx2(const uint8_t *const src, const ptrdiff_t stride, const __m256i coeffs[2],
                                              __m256i s_256[4], __m256i ss_256[4], __m256i tt_256[4], __m256i r[4]) {
     s_256[3]  = _mm256_loadu_si256((__m256i *)(src + 1 * stride));
@@ -1103,7 +1106,7 @@ static INLINE void y_convolve_4tap_32x2_avx2(const uint8_t *const src, const ptr
     r[2]      = convolve_4tap_avx2(tt_256 + 0, coeffs);
     r[3]      = convolve_4tap_avx2(tt_256 + 2, coeffs);
 }
-
+#endif
 static INLINE __m128i y_convolve_6tap_4x2_ssse3(const uint8_t *const src, const ptrdiff_t stride,
                                                 const __m128i coeffs[3], __m128i s_32[6], __m128i ss_128[3]) {
     s_32[5]             = _mm_cvtsi32_si128(*(int32_t *)(src + 3 * stride));
@@ -1113,7 +1116,7 @@ static INLINE __m128i y_convolve_6tap_4x2_ssse3(const uint8_t *const src, const
     ss_128[2]           = _mm_unpacklo_epi8(src45, src56);
     return convolve_6tap_ssse3(ss_128, coeffs);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i y_convolve_6tap_8x2_avx2(const uint8_t *const src, const ptrdiff_t stride,
                                                const __m256i coeffs[3], __m128i s_64[6], __m256i ss_256[3]) {
     s_64[5]             = _mm_loadl_epi64((__m128i *)(src + 3 * stride));
@@ -1149,7 +1152,7 @@ static INLINE void y_convolve_6tap_32x2_avx2(const uint8_t *const src, const ptr
     r[2]      = convolve_6tap_avx2(tt_256 + 0, coeffs);
     r[3]      = convolve_6tap_avx2(tt_256 + 3, coeffs);
 }
-
+#endif
 static INLINE __m128i y_convolve_8tap_2x2_ssse3(const uint8_t *const src, const ptrdiff_t stride,
                                                 const __m128i coeffs[4], __m128i s_16[8], __m128i ss_128[4]) {
     s_16[7]             = _mm_cvtsi32_si128(*(int16_t *)(src + 7 * stride));
@@ -1169,7 +1172,7 @@ static INLINE __m128i y_convolve_8tap_4x2_ssse3(const uint8_t *const src, const
     ss_128[3]           = _mm_unpacklo_epi8(src67, src78);
     return convolve_8tap_ssse3(ss_128, coeffs);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i y_convolve_8tap_8x2_avx2(const uint8_t *const src, const ptrdiff_t stride,
                                                const __m256i coeffs[4], __m128i s_64[8], __m256i ss_256[4]) {
     s_64[7]             = _mm_loadl_epi64((__m128i *)(src + 7 * stride));
@@ -1249,7 +1252,7 @@ static INLINE void xy_x_8tap_32_avx2(const uint8_t *const src, const __m256i coe
     _mm256_storeu_si256((__m256i *)dst, d0);
     _mm256_storeu_si256((__m256i *)(dst + 16), d1);
 }
-
+#endif
 static INLINE __m128i xy_y_convolve_2tap_2x2_sse2(const int16_t *const src, __m128i s_32[2], const __m128i coeffs[1]) {
     __m128i s_128[2];
 
@@ -1294,7 +1297,7 @@ static INLINE __m128i xy_y_convolve_2tap_4x2_half_pel_sse2(const int16_t *const
     s_128[1] = _mm_unpacklo_epi64(s_64[1], s_64[0]);
     return _mm_add_epi16(s_128[0], s_128[1]);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void xy_y_convolve_2tap_16_avx2(const __m256i s0, const __m256i s1, const __m256i coeffs[1],
                                               __m256i r[2]) {
     const __m256i ss0 = _mm256_unpacklo_epi16(s0, s1);
@@ -1376,7 +1379,7 @@ static INLINE void xy_y_convolve_2tap_half_pel_32_all_avx2(const int16_t *const
     r[1] = xy_y_round_half_pel_avx2(r[1]);
     xy_y_pack_store_32_avx2(r[0], r[1], dst);
 }
-
+#endif
 static INLINE __m128i xy_y_convolve_4tap_2x2_sse2(const int16_t *const src, __m128i s_32[4], __m128i ss_128[2],
                                                   const __m128i coeffs[2]) {
     s_32[3]             = _mm_cvtsi32_si128(*(int32_t *)(src + 3 * 2));
@@ -1388,7 +1391,7 @@ static INLINE __m128i xy_y_convolve_4tap_2x2_sse2(const int16_t *const src, __m1
     ss_128[0]           = ss_128[1];
     return r;
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i xy_y_convolve_4tap_4x2_avx2(const int16_t *const src, __m128i s_64[4], __m256i ss_256[2],
                                                   const __m256i coeffs[2]) {
     __m256i s_256[2];
@@ -1483,7 +1486,7 @@ static INLINE void xy_y_convolve_4tap_16x2_half_pelavx2(const int16_t *const src
     s_256[1] = s_256[3];
     s_256[2] = s_256[4];
 }
-
+#endif
 static INLINE __m128i xy_y_convolve_6tap_2x2_sse2(const int16_t *const src, __m128i s_32[6], __m128i ss_128[3],
                                                   const __m128i coeffs[3]) {
     s_32[5]             = _mm_cvtsi32_si128(*(int32_t *)(src + 5 * 2));
@@ -1496,7 +1499,7 @@ static INLINE __m128i xy_y_convolve_6tap_2x2_sse2(const int16_t *const src, __m1
     ss_128[1]           = ss_128[2];
     return r;
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i xy_y_convolve_6tap_4x2_avx2(const int16_t *const src, __m128i s_64[6], __m256i ss_256[3],
                                                   const __m256i coeffs[3]) {
     __m256i s_256[2];
@@ -1599,7 +1602,7 @@ static INLINE void xy_y_convolve_6tap_16x2_half_pel_avx2(const int16_t *const sr
     ss_256[3] = _mm256_unpackhi_epi16(s_256[1], s_256[2]);
     xy_y_convolve_4tap_16_avx2(ss_256, coeffs, r + 2);
 }
-
+#endif
 static INLINE __m128i xy_y_convolve_8tap_2x2_sse2(const int16_t *const src, __m128i s_32[8], __m128i ss_128[4],
                                                   const __m128i coeffs[4]) {
     s_32[7]             = _mm_cvtsi32_si128(*(int32_t *)(src + 7 * 2));
@@ -1613,7 +1616,7 @@ static INLINE __m128i xy_y_convolve_8tap_2x2_sse2(const int16_t *const src, __m1
     ss_128[2]           = ss_128[3];
     return r;
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i xy_y_convolve_8tap_4x2_avx2(const int16_t *const src, __m128i s_64[8], __m256i ss_256[4],
                                                   const __m256i coeffs[4]) {
     __m256i s_256[2];
@@ -1735,7 +1738,7 @@ static INLINE void xy_y_convolve_8tap_16x2_half_pel_avx2(const int16_t *const sr
 
     xy_y_convolve_4tap_16_avx2(ss_256, coeffs, r + 2);
 }
-
+#endif
 static INLINE void jnt_comp_avg_round_store_2x2_kernel_sse2(const __m128i res, const __m128i factor,
                                                             const __m128i offset, const ConvBufType *const dst,
                                                             const ptrdiff_t dst_stride, uint8_t *const dst8,
@@ -1784,7 +1787,7 @@ static INLINE void jnt_comp_avg_round_store_4x2_sse2(const __m128i res, const __
     const __m128i r = jnt_y_round_sse2(res);
     jnt_comp_avg_round_store_4x2_kernel_sse2(r, factor, offset, dst, dst_stride, dst8, dst8_stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i jnt_comp_avg_convolve_16_avx2(const __m256i res, const __m256i dst, const __m256i factor,
                                                     const __m256i offset) {
     __m256i d[2];
@@ -1869,7 +1872,7 @@ SIMD_INLINE void jnt_comp_avg_round_store_32_avx2(const __m256i res[2], const __
     d[1] = jnt_comp_avg_convolve_16_avx2(r[1], d[1], factor, offset);
     convolve_store_32_avx2(d[0], d[1], dst8);
 }
-
+#endif
 static INLINE __m128i jnt_avg_8_sse2(const __m128i res, const __m128i dst) {
     const __m128i d = _mm_add_epi16(res, dst);
     return _mm_srai_epi16(d, 5);
@@ -1918,7 +1921,7 @@ static INLINE void jnt_avg_round_store_4x2_sse2(const __m128i res, const __m128i
     d = jnt_avg_8_sse2(r, d);
     pack_store_4x2_sse2(d, dst8, dst8_stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i jnt_avg_16_avx2(const __m256i res, const __m256i dst) {
     const __m256i d = _mm256_add_epi16(res, dst);
     return _mm256_srai_epi16(d, 5);
@@ -1961,7 +1964,7 @@ static INLINE void jnt_copy_avg_round_store_32_avx2(const __m256i res[2], const
     d[1] = jnt_avg_16_avx2(r[1], d[1]);
     pack_store_32_avx2(d[0], d[1], dst8);
 }
-
+#endif
 static INLINE __m128i jnt_copy_load_src_2x2_sse2(const uint8_t *const src, const int32_t src_stride) {
     const __m128i zero_128 = _mm_setzero_si128();
     const __m128i s        = load_u8_2x2_sse2(src, src_stride);
@@ -1975,7 +1978,7 @@ static INLINE __m128i jnt_copy_load_src_4x2_sse4_1(const uint8_t *const src, con
     const __m128i s16      = _mm_unpacklo_epi8(s, zero_128);
     return _mm_slli_epi16(s16, LEFT_SHIFT);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i jnt_copy_load_src_8x2_avx2(const uint8_t *const src, const int32_t src_stride) {
     const __m256i zero_256 = _mm256_setzero_si256();
     const __m256i s        = load_u8_8x2_avx2(src, src_stride);
@@ -2043,7 +2046,7 @@ static INLINE void jnt_avg_round_store_32_avx2(const __m256i res[2], const __m25
     d[1] = jnt_avg_16_avx2(r[1], d[1]);
     convolve_store_32_avx2(d[0], d[1], dst8);
 }
-
+#endif
 static INLINE void jnt_no_avg_round_store_2x2_sse2(const __m128i res, const __m128i offset, ConvBufType *const dst,
                                                    const ptrdiff_t dst_stride) {
     const __m128i d = jnt_no_avg_round_sse2(res, offset);
@@ -2055,7 +2058,7 @@ static INLINE void jnt_no_avg_round_store_4x2_sse2(const __m128i res, const __m1
     const __m128i d = jnt_no_avg_round_sse2(res, offset);
     store_u16_4x2_sse2(d, dst, dst_stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void jnt_no_avg_round_store_8x2_avx2(const __m256i res, const __m256i offset, ConvBufType *const dst,
                                                    const ptrdiff_t dst_stride) {
     const __m256i d = jnt_no_avg_round_avx2(res, offset);
@@ -2089,7 +2092,7 @@ static INLINE void xy_y_round_store_16x2_avx2(const __m256i res[4], uint8_t *con
     const __m256i r1 = xy_y_round_16_avx2(res + 2);
     xy_y_pack_store_16x2_avx2(r0, r1, dst, stride);
 }
-
+#endif
 static INLINE __m128i jnt_2d_comp_avg_round_4_sse2(const __m128i src) {
     const __m128i round = _mm_set1_epi32(1 << (COMPOUND_ROUND1_BITS - 1));
     const __m128i dst   = _mm_add_epi32(src, round);
@@ -2111,7 +2114,7 @@ static INLINE __m128i jnt_2d_comp_avg_round_4x2_sse2(const __m128i src[2]) {
     const __m128i d1    = _mm_srai_epi32(dst1, COMPOUND_ROUND1_BITS);
     return _mm_packs_epi32(d0, d1);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i jnt_2d_comp_avg_round_8_avx2(const __m256i src) {
     const __m256i round = _mm256_set1_epi32(1 << (COMPOUND_ROUND1_BITS - 1));
     const __m256i dst   = _mm256_add_epi32(src, round);
@@ -2133,7 +2136,7 @@ static INLINE __m256i jnt_2d_comp_avg_round_half_pel_avx2(const __m256i src) {
     const __m256i dst   = _mm256_add_epi16(src, round);
     return _mm256_srai_epi16(dst, 1);
 }
-
+#endif
 static INLINE void jnt_2d_comp_avg_round_store_2x2_sse2(const __m128i res, const __m128i factor, const __m128i offset,
                                                         const ConvBufType *const dst, const ptrdiff_t dst_stride,
                                                         uint8_t *const dst8, const int32_t dst8_stride) {
@@ -2204,7 +2207,7 @@ static INLINE void jnt_2d_comp_avg_round_store_half_pel_4x2_sse2(const __m128i r
     dd[0] = _mm_packs_epi32(dd[0], dd[1]);
     pack_store_4x2_sse2(dd[0], dst8, dst8_stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i jnt_2d_comp_avg_round_pack_8_avx2(const __m256i res, const __m256i factor, const __m256i offset,
                                                         const __m256i dst) {
     const __m256i r = jnt_2d_comp_avg_round_8_avx2(res);
@@ -2330,7 +2333,7 @@ SIMD_INLINE void jnt_2d_comp_avg_round_store_half_pel_32_avx2(const __m256i res[
     d[1] = jnt_2d_comp_avg_round_pack_half_pel_avx2(res[1], factor, offset, d[1]);
     convolve_store_32_avx2(d[0], d[1], dst8);
 }
-
+#endif
 static INLINE __m128i jnt_2d_round_4_sse2(const __m128i src, const __m128i offset) {
     const __m128i dst = _mm_add_epi32(src, offset);
     const __m128i d   = _mm_srai_epi32(dst, COMPOUND_ROUND1_BITS);
@@ -2349,7 +2352,7 @@ static INLINE __m128i jnt_2d_round_4x2_sse2(const __m128i src[2], const __m128i
     const __m128i d1   = _mm_srai_epi32(dst1, COMPOUND_ROUND1_BITS);
     return _mm_packs_epi32(d0, d1);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE __m256i jnt_2d_round_4x2_avx2(const __m256i src, const __m256i offset) {
     const __m256i dst = _mm256_add_epi32(src, offset);
     const __m256i d   = _mm256_srai_epi32(dst, COMPOUND_ROUND1_BITS);
@@ -2368,7 +2371,7 @@ static INLINE __m256i jnt_2d_round_half_pel_avx2(const __m256i src, const __m256
     const __m256i dst0 = _mm256_add_epi16(src, offset);
     return _mm256_srai_epi16(dst0, 1);
 }
-
+#endif
 static INLINE void jnt_2d_avg_round_store_2x2_sse2(const __m128i res, const __m128i offset,
                                                    const ConvBufType *const dst, const ptrdiff_t dst_stride,
                                                    uint8_t *const dst8, const int32_t dst8_stride) {
@@ -2412,7 +2415,7 @@ static INLINE void jnt_2d_avg_round_store_half_pel_4x2_sse2(const __m128i res, c
     d = jnt_avg_8_sse2(r, d);
     pack_store_4x2_sse2(d, dst8, dst8_stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void jnt_2d_avg_round_store_4x2_avx2(const __m256i res, const __m256i offset,
                                                    const ConvBufType *const dst, const ptrdiff_t dst_stride,
                                                    uint8_t *const dst8, const int32_t dst8_stride) {
@@ -2506,7 +2509,7 @@ static INLINE void jnt_2d_avg_round_store_half_pel_32_avx2(const __m256i res[2],
     d[1] = jnt_avg_16_avx2(r[1], d[1]);
     convolve_store_32_avx2(d[0], d[1], dst8);
 }
-
+#endif
 static INLINE void jnt_2d_no_avg_round_store_2x2_sse2(const __m128i res, const __m128i offset, ConvBufType *const dst,
                                                       const ptrdiff_t dst_stride) {
     const __m128i d = jnt_2d_round_4_sse2(res, offset);
@@ -2524,7 +2527,7 @@ static INLINE void jnt_2d_no_avg_round_store_4x2_sse2(const __m128i res[2], cons
     const __m128i d = jnt_2d_round_4x2_sse2(res, offset);
     store_u16_4x2_sse2(d, dst, dst_stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void jnt_2d_no_avg_round_store_4x2_avx2(const __m256i res, const __m256i offset, ConvBufType *const dst,
                                                       const ptrdiff_t dst_stride) {
     const __m256i d  = jnt_2d_round_4x2_avx2(res, offset);
@@ -2533,13 +2536,13 @@ static INLINE void jnt_2d_no_avg_round_store_4x2_avx2(const __m256i res, const _
     _mm_storel_epi64((__m128i *)dst, d0);
     _mm_storel_epi64((__m128i *)(dst + dst_stride), d1);
 }
-
+#endif
 static INLINE void jnt_2d_no_avg_round_store_half_pel_4x2_sse2(const __m128i res, const __m128i offset,
                                                                ConvBufType *const dst, const ptrdiff_t dst_stride) {
     const __m128i d = jnt_2d_round_half_pel_sse2(res, offset);
     store_u16_4x2_sse2(d, dst, dst_stride);
 }
-
+#ifndef ARCH_AARCH64
 static INLINE void jnt_2d_no_avg_round_store_8x2_avx2(const __m256i res[2], const __m256i offset,
                                                       ConvBufType *const dst, const ptrdiff_t dst_stride) {
     const __m256i d = jnt_2d_round_16_avx2(res, offset);
@@ -2605,5 +2608,5 @@ static INLINE __m256i highbd_convolve_rounding(const __m256i *const res_unsigned
 
     return res_round;
 }
-
+#endif
 #endif
diff --git a/Source/Lib/Common/ASM_NEON/CMakeLists.txt b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
new file mode 100644
index 0000000..5dffc0c
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/CMakeLists.txt
@@ -0,0 +1,14 @@
+# ASM_NEON Directory CMakeLists.txt
+
+# Include Encoder Subdirectories
+include_directories(${PROJECT_SOURCE_DIR}/Source/API/
+    ${PROJECT_SOURCE_DIR}/Source/Lib/Common/Codec/
+    ${PROJECT_SOURCE_DIR}/Source/Lib/Common/C_DEFAULT/
+    ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_NEON/)
+
+
+file(GLOB all_files
+    "*.h"
+    "*.c")
+
+add_library(COMMON_ASM_NEON OBJECT ${all_files})
diff --git a/Source/Lib/Common/ASM_NEON/aom_convolve_copy_neon.c b/Source/Lib/Common/ASM_NEON/aom_convolve_copy_neon.c
new file mode 100644
index 0000000..350d2f1
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/aom_convolve_copy_neon.c
@@ -0,0 +1,51 @@
+/*
+ *  Copyright (c) 2020, Alliance for Open Media. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <arm_neon.h>
+#include "EbDefinitions.h"
+
+void aom_convolve_copy_neon(const uint8_t *src, ptrdiff_t src_stride,
+                            uint8_t *dst, ptrdiff_t dst_stride, int w, int h) {
+  const uint8_t *src1;
+  uint8_t *dst1;
+  int y;
+
+  if (!(w & 0x0F)) {
+    for (y = 0; y < h; ++y) {
+      src1 = src;
+      dst1 = dst;
+      for (int x = 0; x < (w >> 4); ++x) {
+        vst1q_u8(dst1, vld1q_u8(src1));
+        src1 += 16;
+        dst1 += 16;
+      }
+      src += src_stride;
+      dst += dst_stride;
+    }
+  } else if (!(w & 0x07)) {
+    for (y = 0; y < h; ++y) {
+      vst1_u8(dst, vld1_u8(src));
+      src += src_stride;
+      dst += dst_stride;
+    }
+  } else if (!(w & 0x03)) {
+    for (y = 0; y < h; ++y) {
+      vst1_lane_u32((uint32_t *)(dst), vreinterpret_u32_u8(vld1_u8(src)), 0);
+      src += src_stride;
+      dst += dst_stride;
+    }
+  } else if (!(w & 0x01)) {
+    for (y = 0; y < h; ++y) {
+      vst1_lane_u16((uint16_t *)(dst), vreinterpret_u16_u8(vld1_u8(src)), 0);
+      src += src_stride;
+      dst += dst_stride;
+    }
+  }
+}
diff --git a/Source/Lib/Common/ASM_NEON/aom_subpixel_8t_intrin_neon.c b/Source/Lib/Common/ASM_NEON/aom_subpixel_8t_intrin_neon.c
new file mode 100644
index 0000000..95a011d
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/aom_subpixel_8t_intrin_neon.c
@@ -0,0 +1,524 @@
+/*
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include "simd.h"
+#include "common_dsp_rtcd.h"
+
+#include "convolve.h"
+#include "convolve8_neon.h"
+
+// filters for 8_h8 and 16_h8
+DECLARE_ALIGNED(32, static const uint8_t, filt_h4[]) = {
+    0,  1,  1, 2,  2,  3,  3,  4,  4, 5,  5,  6,  6,  7,  7,  8,  0,  1,  1,  2,  2,  3,
+    3,  4,  4, 5,  5,  6,  6,  7,  7, 8,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,
+    8,  9,  9, 10, 2,  3,  3,  4,  4, 5,  5,  6,  6,  7,  7,  8,  8,  9,  9,  10, 4,  5,
+    5,  6,  6, 7,  7,  8,  8,  9,  9, 10, 10, 11, 11, 12, 4,  5,  5,  6,  6,  7,  7,  8,
+    8,  9,  9, 10, 10, 11, 11, 12, 6, 7,  7,  8,  8,  9,  9,  10, 10, 11, 11, 12, 12, 13,
+    13, 14, 6, 7,  7,  8,  8,  9,  9, 10, 10, 11, 11, 12, 12, 13, 13, 14};
+
+DECLARE_ALIGNED(32, static const uint8_t, filtd4[]) = {
+    2, 3, 4, 5, 3, 4, 5, 6, 4, 5, 6, 7, 5, 6, 7, 8, 2, 3, 4, 5, 3, 4, 5, 6, 4, 5, 6, 7, 5, 6, 7, 8,
+};
+
+#define FUN_CONV_1D(name, step_q4, filter, dir, src_start, avg, opt)                               \
+    void svt_aom_convolve8_##name##_##opt(const uint8_t *src,                                      \
+                                          ptrdiff_t      src_stride,                               \
+                                          uint8_t       *dst,                                      \
+                                          ptrdiff_t      dst_stride,                               \
+                                          const int16_t *filter_x,                                 \
+                                          int            x_step_q4,                                \
+                                          const int16_t *filter_y,                                 \
+                                          int            y_step_q4,                                \
+                                          int            w,                                        \
+                                          int            h) {                                      \
+        (void)filter_x;                                                                            \
+        (void)x_step_q4;                                                                           \
+        (void)filter_y;                                                                            \
+        (void)y_step_q4;                                                                           \
+        assert((-128 <= filter[3]) && (filter[3] <= 127));                                         \
+        assert(step_q4 == 16);                                                                     \
+        if (((filter[0] | filter[1] | filter[6] | filter[7]) == 0) && (filter[2] | filter[5])) {   \
+            while (w >= 16) {                                                                      \
+                svt_aom_filter_block1d16_##dir##4_##avg##opt(                                      \
+                    src_start, src_stride, dst, dst_stride, h, filter);                            \
+                src += 16;                                                                         \
+                dst += 16;                                                                         \
+                w -= 16;                                                                           \
+            }                                                                                      \
+            while (w >= 8) {                                                                       \
+                svt_aom_filter_block1d8_##dir##4_##avg##opt(                                       \
+                    src_start, src_stride, dst, dst_stride, h, filter);                            \
+                src += 8;                                                                          \
+                dst += 8;                                                                          \
+                w -= 8;                                                                            \
+            }                                                                                      \
+            while (w >= 4) {                                                                       \
+                svt_aom_filter_block1d4_##dir##4_##avg##opt(                                       \
+                    src_start, src_stride, dst, dst_stride, h, filter);                            \
+                src += 4;                                                                          \
+                dst += 4;                                                                          \
+                w -= 4;                                                                            \
+            }                                                                                      \
+        }                                                                                      \
+        if (w) {                                                                                   \
+            svt_aom_convolve8_##name##_fallback_neon(                                              \
+                src, src_stride, dst, dst_stride, filter_x, x_step_q4, filter_y, y_step_q4, w, h); \
+        }                                                                                          \
+    }
+
+static void svt_aom_filter_block1d4_h4_ssse3_neon(const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line,
+                                             uint8_t *output_ptr, ptrdiff_t output_pitch,
+                                             uint32_t output_height, const int16_t *filter) {
+    __m128i      filtersReg;
+    __m128i      addFilterReg32, filt1Reg, firstFilters, srcReg32b1, srcRegFilt32b1_1;
+    unsigned int i;
+    src_ptr -= 3;
+    addFilterReg32 = _mm_set1_epi16(32);
+    filtersReg     = _mm_loadu_si128((const __m128i *)filter);
+    filtersReg     = _mm_srai_epi16(filtersReg, 1);
+    // converting the 16 bit (short) to 8 bit (byte) and have the same data
+    // in both lanes of 128 bit register.
+    filtersReg = _mm_packs_epi16(filtersReg, filtersReg);
+
+    firstFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi32(0x5040302u));
+    filt1Reg     = _mm_loadu_si128((__m128i const *)(filtd4));
+
+    for (i = output_height; i > 0; i -= 1) {
+        // load the 2 strides of source
+        srcReg32b1 = _mm_loadu_si128((const __m128i *)src_ptr);
+
+        // filter the source buffer
+        srcRegFilt32b1_1 = _mm_shuffle_epi8(srcReg32b1, filt1Reg);
+
+        // multiply 4 adjacent elements with the filter and add the result
+        srcRegFilt32b1_1 = _mm_maddubs_epi16(srcRegFilt32b1_1, firstFilters);
+
+        srcRegFilt32b1_1 = _mm_hadds_epi16(srcRegFilt32b1_1, _mm_setzero_si128());
+
+        // shift by 6 bit each 16 bit
+        srcRegFilt32b1_1 = _mm_adds_epi16(srcRegFilt32b1_1, addFilterReg32);
+        srcRegFilt32b1_1 = _mm_srai_epi16(srcRegFilt32b1_1, 6);
+
+        // shrink to 8 bit each 16 bits, the first lane contain the first
+        // convolve result and the second lane contain the second convolve result
+        srcRegFilt32b1_1 = _mm_packus_epi16(srcRegFilt32b1_1, _mm_setzero_si128());
+
+        src_ptr += src_pixels_per_line;
+
+        *((uint32_t *)(output_ptr)) = _mm_cvtsi128_si32(srcRegFilt32b1_1);
+        output_ptr += output_pitch;
+    }
+}
+
+static void svt_aom_filter_block1d4_v4_ssse3_neon(const uint8_t *src_ptr, ptrdiff_t src_pitch,
+                                             uint8_t *output_ptr, ptrdiff_t out_pitch,
+                                             uint32_t output_height, const int16_t *filter) {
+    __m128i filtersReg;
+    __m128i addFilterReg32;
+    __m128i srcReg2, srcReg3, srcReg23, srcReg4, srcReg34, srcReg5, srcReg45, srcReg6, srcReg56;
+    __m128i srcReg23_34_lo, srcReg45_56_lo;
+    __m128i srcReg2345_3456_lo, srcReg2345_3456_hi;
+    __m128i resReglo, resReghi;
+    __m128i firstFilters;
+    unsigned int i;
+    ptrdiff_t    src_stride, dst_stride;
+
+    addFilterReg32 = _mm_set1_epi16(32);
+    filtersReg     = _mm_loadu_si128((const __m128i *)filter);
+    // converting the 16 bit (short) to  8 bit (byte) and have the
+    // same data in both lanes of 128 bit register.
+    filtersReg = _mm_srai_epi16(filtersReg, 1);
+    filtersReg = _mm_packs_epi16(filtersReg, filtersReg);
+
+    firstFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi32(0x5040302u));
+
+    // multiple the size of the source and destination stride by two
+    src_stride = src_pitch << 1;
+    dst_stride = out_pitch << 1;
+
+    srcReg2  = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 2));
+    srcReg3  = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 3));
+    srcReg23 = _mm_unpacklo_epi32(srcReg2, srcReg3);
+
+    srcReg4 = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 4));
+
+    // have consecutive loads on the same 256 register
+    srcReg34 = _mm_unpacklo_epi32(srcReg3, srcReg4);
+
+    srcReg23_34_lo = _mm_unpacklo_epi8(srcReg23, srcReg34);
+
+    for (i = output_height; i > 1; i -= 2) {
+        srcReg5  = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 5));
+        srcReg45 = _mm_unpacklo_epi32(srcReg4, srcReg5);
+
+        srcReg6  = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 6));
+        srcReg56 = _mm_unpacklo_epi32(srcReg5, srcReg6);
+
+        // merge every two consecutive registers
+        srcReg45_56_lo = _mm_unpacklo_epi8(srcReg45, srcReg56);
+
+        srcReg2345_3456_lo = _mm_unpacklo_epi16(srcReg23_34_lo, srcReg45_56_lo);
+        srcReg2345_3456_hi = _mm_unpackhi_epi16(srcReg23_34_lo, srcReg45_56_lo);
+
+        // multiply 2 adjacent elements with the filter and add the result
+        resReglo = _mm_maddubs_epi16(srcReg2345_3456_lo, firstFilters);
+        resReghi = _mm_maddubs_epi16(srcReg2345_3456_hi, firstFilters);
+
+        resReglo = _mm_hadds_epi16(resReglo, _mm_setzero_si128());
+        resReghi = _mm_hadds_epi16(resReghi, _mm_setzero_si128());
+
+        // shift by 6 bit each 16 bit
+        resReglo = _mm_adds_epi16(resReglo, addFilterReg32);
+        resReghi = _mm_adds_epi16(resReghi, addFilterReg32);
+        resReglo = _mm_srai_epi16(resReglo, 6);
+        resReghi = _mm_srai_epi16(resReghi, 6);
+
+        // shrink to 8 bit each 16 bits, the first lane contain the first
+        // convolve result and the second lane contain the second convolve
+        // result
+        resReglo = _mm_packus_epi16(resReglo, resReglo);
+        resReghi = _mm_packus_epi16(resReghi, resReghi);
+
+        src_ptr += src_stride;
+
+        *((uint32_t *)(output_ptr))             = _mm_cvtsi128_si32(resReglo);
+        *((uint32_t *)(output_ptr + out_pitch)) = _mm_cvtsi128_si32(resReghi);
+
+        output_ptr += dst_stride;
+
+        // save part of the registers for next strides
+        srcReg23_34_lo = srcReg45_56_lo;
+        srcReg4        = srcReg6;
+    }
+}
+
+static void svt_aom_filter_block1d8_h4_ssse3_neon(const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line,
+                                             uint8_t *output_ptr, ptrdiff_t output_pitch,
+                                             uint32_t output_height, const int16_t *filter) {
+    __m128i      filtersReg;
+    __m128i      addFilterReg32, filt2Reg, filt3Reg;
+    __m128i      secondFilters, thirdFilters;
+    __m128i      srcRegFilt32b1_1, srcRegFilt32b2, srcRegFilt32b3;
+    __m128i      srcReg32b1;
+    unsigned int i;
+    src_ptr -= 3;
+    addFilterReg32 = _mm_set1_epi16(32);
+    filtersReg     = _mm_loadu_si128((const __m128i *)filter);
+    filtersReg     = _mm_srai_epi16(filtersReg, 1);
+    // converting the 16 bit (short) to 8 bit (byte) and have the same data
+    // in both lanes of 128 bit register.
+    filtersReg = _mm_packs_epi16(filtersReg, filtersReg);
+
+    // duplicate only the second 16 bits (third and forth byte)
+    // across 256 bit register
+    secondFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi16(0x302u));
+    // duplicate only the third 16 bits (fifth and sixth byte)
+    // across 256 bit register
+    thirdFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi16(0x504u));
+
+    filt2Reg = _mm_loadu_si128((__m128i const *)(filt_h4 + 32));
+    filt3Reg = _mm_loadu_si128((__m128i const *)(filt_h4 + 32 * 2));
+
+    for (i = output_height; i > 0; i -= 1) {
+        srcReg32b1 = _mm_loadu_si128((const __m128i *)src_ptr);
+
+        // filter the source buffer
+        srcRegFilt32b3 = _mm_shuffle_epi8(srcReg32b1, filt2Reg);
+        srcRegFilt32b2 = _mm_shuffle_epi8(srcReg32b1, filt3Reg);
+
+        // multiply 2 adjacent elements with the filter and add the result
+        srcRegFilt32b3 = _mm_maddubs_epi16(srcRegFilt32b3, secondFilters);
+        srcRegFilt32b2 = _mm_maddubs_epi16(srcRegFilt32b2, thirdFilters);
+
+        srcRegFilt32b1_1 = _mm_adds_epi16(srcRegFilt32b3, srcRegFilt32b2);
+
+        // shift by 6 bit each 16 bit
+        srcRegFilt32b1_1 = _mm_adds_epi16(srcRegFilt32b1_1, addFilterReg32);
+        srcRegFilt32b1_1 = _mm_srai_epi16(srcRegFilt32b1_1, 6);
+
+        // shrink to 8 bit each 16 bits
+        srcRegFilt32b1_1 = _mm_packus_epi16(srcRegFilt32b1_1, _mm_setzero_si128());
+
+        src_ptr += src_pixels_per_line;
+
+        _mm_storel_epi64((__m128i *)output_ptr, srcRegFilt32b1_1);
+
+        output_ptr += output_pitch;
+    }
+}
+
+static void svt_aom_filter_block1d8_v4_ssse3_neon(const uint8_t *src_ptr, ptrdiff_t src_pitch,
+                                             uint8_t *output_ptr, ptrdiff_t out_pitch,
+                                             uint32_t output_height, const int16_t *filter) {
+    __m128i      filtersReg;
+    __m128i      srcReg2, srcReg3, srcReg4, srcReg5, srcReg6;
+    __m128i      srcReg23, srcReg34, srcReg45, srcReg56;
+    __m128i      resReg23, resReg34, resReg45, resReg56;
+    __m128i      resReg23_45, resReg34_56;
+    __m128i      addFilterReg32, secondFilters, thirdFilters;
+    unsigned int i;
+    ptrdiff_t    src_stride, dst_stride;
+
+    addFilterReg32 = _mm_set1_epi16(32);
+    filtersReg     = _mm_loadu_si128((const __m128i *)filter);
+    // converting the 16 bit (short) to  8 bit (byte) and have the
+    // same data in both lanes of 128 bit register.
+    filtersReg = _mm_srai_epi16(filtersReg, 1);
+    filtersReg = _mm_packs_epi16(filtersReg, filtersReg);
+
+    // duplicate only the second 16 bits (third and forth byte)
+    // across 128 bit register
+    secondFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi16(0x302u));
+    // duplicate only the third 16 bits (fifth and sixth byte)
+    // across 128 bit register
+    thirdFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi16(0x504u));
+
+    // multiple the size of the source and destination stride by two
+    src_stride = src_pitch << 1;
+    dst_stride = out_pitch << 1;
+
+    srcReg2  = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 2));
+    srcReg3  = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 3));
+    srcReg23 = _mm_unpacklo_epi8(srcReg2, srcReg3);
+
+    srcReg4 = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 4));
+
+    // have consecutive loads on the same 256 register
+    srcReg34 = _mm_unpacklo_epi8(srcReg3, srcReg4);
+
+    for (i = output_height; i > 1; i -= 2) {
+        srcReg5 = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 5));
+
+        srcReg45 = _mm_unpacklo_epi8(srcReg4, srcReg5);
+
+        srcReg6 = _mm_loadl_epi64((const __m128i *)(src_ptr + src_pitch * 6));
+
+        srcReg56 = _mm_unpacklo_epi8(srcReg5, srcReg6);
+
+        // multiply 2 adjacent elements with the filter and add the result
+        resReg23 = _mm_maddubs_epi16(srcReg23, secondFilters);
+        resReg34 = _mm_maddubs_epi16(srcReg34, secondFilters);
+        resReg45 = _mm_maddubs_epi16(srcReg45, thirdFilters);
+        resReg56 = _mm_maddubs_epi16(srcReg56, thirdFilters);
+
+        // add and saturate the results together
+        resReg23_45 = _mm_adds_epi16(resReg23, resReg45);
+        resReg34_56 = _mm_adds_epi16(resReg34, resReg56);
+
+        // shift by 6 bit each 16 bit
+        resReg23_45 = _mm_adds_epi16(resReg23_45, addFilterReg32);
+        resReg34_56 = _mm_adds_epi16(resReg34_56, addFilterReg32);
+        resReg23_45 = _mm_srai_epi16(resReg23_45, 6);
+        resReg34_56 = _mm_srai_epi16(resReg34_56, 6);
+
+        // shrink to 8 bit each 16 bits, the first lane contain the first
+        // convolve result and the second lane contain the second convolve
+        // result
+        resReg23_45 = _mm_packus_epi16(resReg23_45, _mm_setzero_si128());
+        resReg34_56 = _mm_packus_epi16(resReg34_56, _mm_setzero_si128());
+
+        src_ptr += src_stride;
+
+        _mm_storel_epi64((__m128i *)output_ptr, (resReg23_45));
+        _mm_storel_epi64((__m128i *)(output_ptr + out_pitch), (resReg34_56));
+
+        output_ptr += dst_stride;
+
+        // save part of the registers for next strides
+        srcReg23 = srcReg45;
+        srcReg34 = srcReg56;
+        srcReg4  = srcReg6;
+    }
+}
+
+static void svt_aom_filter_block1d16_h4_ssse3_neon(const uint8_t *src_ptr, ptrdiff_t src_pixels_per_line,
+                                              uint8_t *output_ptr, ptrdiff_t output_pitch,
+                                              uint32_t output_height, const int16_t *filter) {
+    __m128i      filtersReg;
+    __m128i      addFilterReg32, filt2Reg, filt3Reg;
+    __m128i      secondFilters, thirdFilters;
+    __m128i      srcRegFilt32b1_1, srcRegFilt32b2_1, srcRegFilt32b2, srcRegFilt32b3;
+    __m128i      srcReg32b1, srcReg32b2;
+    unsigned int i;
+    src_ptr -= 3;
+    addFilterReg32 = _mm_set1_epi16(32);
+    filtersReg     = _mm_loadu_si128((const __m128i *)filter);
+    filtersReg     = _mm_srai_epi16(filtersReg, 1);
+    // converting the 16 bit (short) to 8 bit (byte) and have the same data
+    // in both lanes of 128 bit register.
+    filtersReg = _mm_packs_epi16(filtersReg, filtersReg);
+
+    // duplicate only the second 16 bits (third and forth byte)
+    // across 256 bit register
+    secondFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi16(0x302u));
+    // duplicate only the third 16 bits (fifth and sixth byte)
+    // across 256 bit register
+    thirdFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi16(0x504u));
+
+    filt2Reg = _mm_loadu_si128((__m128i const *)(filt_h4 + 32));
+    filt3Reg = _mm_loadu_si128((__m128i const *)(filt_h4 + 32 * 2));
+
+    for (i = output_height; i > 0; i -= 1) {
+        srcReg32b1 = _mm_loadu_si128((const __m128i *)src_ptr);
+
+        // filter the source buffer
+        srcRegFilt32b3 = _mm_shuffle_epi8(srcReg32b1, filt2Reg);
+        srcRegFilt32b2 = _mm_shuffle_epi8(srcReg32b1, filt3Reg);
+
+        // multiply 2 adjacent elements with the filter and add the result
+        srcRegFilt32b3 = _mm_maddubs_epi16(srcRegFilt32b3, secondFilters);
+        srcRegFilt32b2 = _mm_maddubs_epi16(srcRegFilt32b2, thirdFilters);
+
+        srcRegFilt32b1_1 = _mm_adds_epi16(srcRegFilt32b3, srcRegFilt32b2);
+
+        // reading stride of the next 16 bytes
+        // (part of it was being read by earlier read)
+        srcReg32b2 = _mm_loadu_si128((const __m128i *)(src_ptr + 8));
+
+        // filter the source buffer
+        srcRegFilt32b3 = _mm_shuffle_epi8(srcReg32b2, filt2Reg);
+        srcRegFilt32b2 = _mm_shuffle_epi8(srcReg32b2, filt3Reg);
+
+        // multiply 2 adjacent elements with the filter and add the result
+        srcRegFilt32b3 = _mm_maddubs_epi16(srcRegFilt32b3, secondFilters);
+        srcRegFilt32b2 = _mm_maddubs_epi16(srcRegFilt32b2, thirdFilters);
+
+        // add and saturate the results together
+        srcRegFilt32b2_1 = _mm_adds_epi16(srcRegFilt32b3, srcRegFilt32b2);
+
+        // shift by 6 bit each 16 bit
+        srcRegFilt32b1_1 = _mm_adds_epi16(srcRegFilt32b1_1, addFilterReg32);
+        srcRegFilt32b2_1 = _mm_adds_epi16(srcRegFilt32b2_1, addFilterReg32);
+        srcRegFilt32b1_1 = _mm_srai_epi16(srcRegFilt32b1_1, 6);
+        srcRegFilt32b2_1 = _mm_srai_epi16(srcRegFilt32b2_1, 6);
+
+        // shrink to 8 bit each 16 bits, the first lane contain the first
+        // convolve result and the second lane contain the second convolve result
+        srcRegFilt32b1_1 = _mm_packus_epi16(srcRegFilt32b1_1, srcRegFilt32b2_1);
+
+        src_ptr += src_pixels_per_line;
+
+        _mm_storeu_si128((__m128i *)output_ptr, srcRegFilt32b1_1);
+
+        output_ptr += output_pitch;
+    }
+}
+
+static void svt_aom_filter_block1d16_v4_ssse3_neon(const uint8_t *src_ptr, ptrdiff_t src_pitch,
+                                              uint8_t *output_ptr, ptrdiff_t out_pitch,
+                                              uint32_t output_height, const int16_t *filter) {
+    __m128i      filtersReg;
+    __m128i      srcReg2, srcReg3, srcReg4, srcReg5, srcReg6;
+    __m128i      srcReg23_lo, srcReg23_hi, srcReg34_lo, srcReg34_hi;
+    __m128i      srcReg45_lo, srcReg45_hi, srcReg56_lo, srcReg56_hi;
+    __m128i      resReg23_lo, resReg34_lo, resReg45_lo, resReg56_lo;
+    __m128i      resReg23_hi, resReg34_hi, resReg45_hi, resReg56_hi;
+    __m128i      resReg23_45_lo, resReg34_56_lo, resReg23_45_hi, resReg34_56_hi;
+    __m128i      resReg23_45, resReg34_56;
+    __m128i      addFilterReg32, secondFilters, thirdFilters;
+    unsigned int i;
+    ptrdiff_t    src_stride, dst_stride;
+
+    addFilterReg32 = _mm_set1_epi16(32);
+    filtersReg     = _mm_loadu_si128((const __m128i *)filter);
+    // converting the 16 bit (short) to  8 bit (byte) and have the
+    // same data in both lanes of 128 bit register.
+    filtersReg = _mm_srai_epi16(filtersReg, 1);
+    filtersReg = _mm_packs_epi16(filtersReg, filtersReg);
+
+    // duplicate only the second 16 bits (third and forth byte)
+    // across 128 bit register
+    secondFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi16(0x302u));
+    // duplicate only the third 16 bits (fifth and sixth byte)
+    // across 128 bit register
+    thirdFilters = _mm_shuffle_epi8(filtersReg, _mm_set1_epi16(0x504u));
+
+    // multiple the size of the source and destination stride by two
+    src_stride = src_pitch << 1;
+    dst_stride = out_pitch << 1;
+
+    srcReg2     = _mm_loadu_si128((const __m128i *)(src_ptr + src_pitch * 2));
+    srcReg3     = _mm_loadu_si128((const __m128i *)(src_ptr + src_pitch * 3));
+    srcReg23_lo = _mm_unpacklo_epi8(srcReg2, srcReg3);
+    srcReg23_hi = _mm_unpackhi_epi8(srcReg2, srcReg3);
+
+    srcReg4 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pitch * 4));
+
+    // have consecutive loads on the same 256 register
+    srcReg34_lo = _mm_unpacklo_epi8(srcReg3, srcReg4);
+    srcReg34_hi = _mm_unpackhi_epi8(srcReg3, srcReg4);
+
+    for (i = output_height; i > 1; i -= 2) {
+        srcReg5 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pitch * 5));
+
+        srcReg45_lo = _mm_unpacklo_epi8(srcReg4, srcReg5);
+        srcReg45_hi = _mm_unpackhi_epi8(srcReg4, srcReg5);
+
+        srcReg6 = _mm_loadu_si128((const __m128i *)(src_ptr + src_pitch * 6));
+
+        srcReg56_lo = _mm_unpacklo_epi8(srcReg5, srcReg6);
+        srcReg56_hi = _mm_unpackhi_epi8(srcReg5, srcReg6);
+
+        // multiply 2 adjacent elements with the filter and add the result
+        resReg23_lo = _mm_maddubs_epi16(srcReg23_lo, secondFilters);
+        resReg34_lo = _mm_maddubs_epi16(srcReg34_lo, secondFilters);
+        resReg45_lo = _mm_maddubs_epi16(srcReg45_lo, thirdFilters);
+        resReg56_lo = _mm_maddubs_epi16(srcReg56_lo, thirdFilters);
+
+        // add and saturate the results together
+        resReg23_45_lo = _mm_adds_epi16(resReg23_lo, resReg45_lo);
+        resReg34_56_lo = _mm_adds_epi16(resReg34_lo, resReg56_lo);
+
+        // multiply 2 adjacent elements with the filter and add the result
+
+        resReg23_hi = _mm_maddubs_epi16(srcReg23_hi, secondFilters);
+        resReg34_hi = _mm_maddubs_epi16(srcReg34_hi, secondFilters);
+        resReg45_hi = _mm_maddubs_epi16(srcReg45_hi, thirdFilters);
+        resReg56_hi = _mm_maddubs_epi16(srcReg56_hi, thirdFilters);
+
+        // add and saturate the results together
+        resReg23_45_hi = _mm_adds_epi16(resReg23_hi, resReg45_hi);
+        resReg34_56_hi = _mm_adds_epi16(resReg34_hi, resReg56_hi);
+
+        // shift by 6 bit each 16 bit
+        resReg23_45_lo = _mm_adds_epi16(resReg23_45_lo, addFilterReg32);
+        resReg34_56_lo = _mm_adds_epi16(resReg34_56_lo, addFilterReg32);
+        resReg23_45_hi = _mm_adds_epi16(resReg23_45_hi, addFilterReg32);
+        resReg34_56_hi = _mm_adds_epi16(resReg34_56_hi, addFilterReg32);
+        resReg23_45_lo = _mm_srai_epi16(resReg23_45_lo, 6);
+        resReg34_56_lo = _mm_srai_epi16(resReg34_56_lo, 6);
+        resReg23_45_hi = _mm_srai_epi16(resReg23_45_hi, 6);
+        resReg34_56_hi = _mm_srai_epi16(resReg34_56_hi, 6);
+
+        // shrink to 8 bit each 16 bits, the first lane contain the first
+        // convolve result and the second lane contain the second convolve
+        // result
+        resReg23_45 = _mm_packus_epi16(resReg23_45_lo, resReg23_45_hi);
+        resReg34_56 = _mm_packus_epi16(resReg34_56_lo, resReg34_56_hi);
+
+        src_ptr += src_stride;
+
+        _mm_storeu_si128((__m128i *)output_ptr, (resReg23_45));
+        _mm_storeu_si128((__m128i *)(output_ptr + out_pitch), (resReg34_56));
+
+        output_ptr += dst_stride;
+
+        // save part of the registers for next strides
+        srcReg23_lo = srcReg45_lo;
+        srcReg34_lo = srcReg56_lo;
+        srcReg23_hi = srcReg45_hi;
+        srcReg34_hi = srcReg56_hi;
+        srcReg4     = srcReg6;
+    }
+}
+
+FUN_CONV_1D(horiz, x_step_q4, filter_x, h, src, , ssse3_neon);
+FUN_CONV_1D(vert, y_step_q4, filter_y, v, src - src_stride * 3, , ssse3_neon);
diff --git a/Source/Lib/Common/ASM_NEON/convolve.c b/Source/Lib/Common/ASM_NEON/convolve.c
new file mode 100644
index 0000000..9dde56c
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/convolve.c
@@ -0,0 +1,1288 @@
+/*
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <assert.h>
+#include <string.h>
+
+#include "EbInterPrediction.h"
+#include "EbSuperRes.h"
+#include "EbDefinitions.h"
+
+
+void av1_convolve_horiz_rs_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                             int dst_stride, int w, int h,
+                             const int16_t *x_filters, int x0_qn,
+                             int x_step_qn) {
+  src -= UPSCALE_NORMATIVE_TAPS / 2 - 1;
+  for (int y = 0; y < h; ++y) {
+    int x_qn = x0_qn;
+    for (int x = 0; x < w; ++x) {
+      const uint8_t *const src_x = &src[x_qn >> RS_SCALE_SUBPEL_BITS];
+      const int x_filter_idx =
+          (x_qn & RS_SCALE_SUBPEL_MASK) >> RS_SCALE_EXTRA_BITS;
+      assert(x_filter_idx <= RS_SUBPEL_MASK);
+      const int16_t *const x_filter =
+          &x_filters[x_filter_idx * UPSCALE_NORMATIVE_TAPS];
+      int sum = 0;
+      for (int k = 0; k < UPSCALE_NORMATIVE_TAPS; ++k)
+        sum += src_x[k] * x_filter[k];
+      dst[x] = clip_pixel(ROUND_POWER_OF_TWO(sum, FILTER_BITS));
+      x_qn += x_step_qn;
+    }
+    src += src_stride;
+    dst += dst_stride;
+  }
+}
+
+void av1_highbd_convolve_horiz_rs_c(const uint16_t *src, int src_stride,
+                                    uint16_t *dst, int dst_stride, int w, int h,
+                                    const int16_t *x_filters, int x0_qn,
+                                    int x_step_qn, int bd) {
+  src -= UPSCALE_NORMATIVE_TAPS / 2 - 1;
+  for (int y = 0; y < h; ++y) {
+    int x_qn = x0_qn;
+    for (int x = 0; x < w; ++x) {
+      const uint16_t *const src_x = &src[x_qn >> RS_SCALE_SUBPEL_BITS];
+      const int x_filter_idx =
+          (x_qn & RS_SCALE_SUBPEL_MASK) >> RS_SCALE_EXTRA_BITS;
+      assert(x_filter_idx <= RS_SUBPEL_MASK);
+      const int16_t *const x_filter =
+          &x_filters[x_filter_idx * UPSCALE_NORMATIVE_TAPS];
+      int sum = 0;
+      for (int k = 0; k < UPSCALE_NORMATIVE_TAPS; ++k)
+        sum += src_x[k] * x_filter[k];
+      dst[x] = clip_pixel_highbd(ROUND_POWER_OF_TWO(sum, FILTER_BITS), bd);
+      x_qn += x_step_qn;
+    }
+    src += src_stride;
+    dst += dst_stride;
+  }
+}
+
+void av1_convolve_2d_sobel_y_c(const uint8_t *src, int src_stride, double *dst,
+                               int dst_stride, int w, int h, int dir,
+                               double norm) {
+  int16_t im_block[(MAX_SB_SIZE + MAX_FILTER_TAP - 1) * MAX_SB_SIZE];
+  DECLARE_ALIGNED(256, static const int16_t, sobel_a[3]) = { 1, 0, -1 };
+  DECLARE_ALIGNED(256, static const int16_t, sobel_b[3]) = { 1, 2, 1 };
+  const int taps = 3;
+  int im_h = h + taps - 1;
+  int im_stride = w;
+  const int fo_vert = 1;
+  const int fo_horiz = 1;
+
+  // horizontal filter
+  const uint8_t *src_horiz = src - fo_vert * src_stride;
+  const int16_t *x_filter = dir ? sobel_a : sobel_b;
+  for (int y = 0; y < im_h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int16_t sum = 0;
+      for (int k = 0; k < taps; ++k) {
+        sum += x_filter[k] * src_horiz[y * src_stride + x - fo_horiz + k];
+      }
+      im_block[y * im_stride + x] = sum;
+    }
+  }
+
+  // vertical filter
+  int16_t *src_vert = im_block + fo_vert * im_stride;
+  const int16_t *y_filter = dir ? sobel_b : sobel_a;
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int16_t sum = 0;
+      for (int k = 0; k < taps; ++k) {
+        sum += y_filter[k] * src_vert[(y - fo_vert + k) * im_stride + x];
+      }
+      dst[y * dst_stride + x] = sum * norm;
+    }
+  }
+}
+
+void av1_convolve_2d_sr_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                          int dst_stride, int w, int h,
+                          const InterpFilterParams *filter_params_x,
+                          const InterpFilterParams *filter_params_y,
+                          const int subpel_x_qn, const int subpel_y_qn,
+                          ConvolveParams *conv_params) {
+  int16_t im_block[(MAX_SB_SIZE + MAX_FILTER_TAP - 1) * MAX_SB_SIZE];
+  int im_h = h + filter_params_y->taps - 1;
+  int im_stride = w;
+  assert(w <= MAX_SB_SIZE && h <= MAX_SB_SIZE);
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int bd = 8;
+  const int bits =
+      FILTER_BITS * 2 - conv_params->round_0 - conv_params->round_1;
+
+  // horizontal filter
+  const uint8_t *src_horiz = src - fo_vert * src_stride;
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_x, subpel_x_qn & SUBPEL_MASK);
+  for (int y = 0; y < im_h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t sum = (1 << (bd + FILTER_BITS - 1));
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        sum += x_filter[k] * src_horiz[y * src_stride + x - fo_horiz + k];
+      }
+      assert(0 <= sum && sum < (1 << (bd + FILTER_BITS + 1)));
+      im_block[y * im_stride + x] =
+          (int16_t)ROUND_POWER_OF_TWO(sum, conv_params->round_0);
+    }
+  }
+
+  // vertical filter
+  int16_t *src_vert = im_block + fo_vert * im_stride;
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_y, subpel_y_qn & SUBPEL_MASK);
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t sum = 1 << offset_bits;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        sum += y_filter[k] * src_vert[(y - fo_vert + k) * im_stride + x];
+      }
+      assert(0 <= sum && sum < (1 << (offset_bits + 2)));
+      int16_t res = ROUND_POWER_OF_TWO(sum, conv_params->round_1) -
+                    ((1 << (offset_bits - conv_params->round_1)) +
+                     (1 << (offset_bits - conv_params->round_1 - 1)));
+      dst[y * dst_stride + x] = clip_pixel(ROUND_POWER_OF_TWO(res, bits));
+    }
+  }
+}
+
+void av1_convolve_y_sr_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                         int dst_stride, int w, int h,
+                         const InterpFilterParams *filter_params_y,
+                         const int subpel_y_qn) {
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+
+  // vertical filter
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_y, subpel_y_qn & SUBPEL_MASK);
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t res = 0;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        res += y_filter[k] * src[(y - fo_vert + k) * src_stride + x];
+      }
+      dst[y * dst_stride + x] =
+          clip_pixel(ROUND_POWER_OF_TWO(res, FILTER_BITS));
+    }
+  }
+}
+
+void av1_convolve_x_sr_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                         int dst_stride, int w, int h,
+                         const InterpFilterParams *filter_params_x,
+                         const int subpel_x_qn, ConvolveParams *conv_params) {
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int bits = FILTER_BITS - conv_params->round_0;
+
+  assert(bits >= 0);
+  assert((FILTER_BITS - conv_params->round_1) >= 0 ||
+         ((conv_params->round_0 + conv_params->round_1) == 2 * FILTER_BITS));
+
+  // horizontal filter
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_x, subpel_x_qn & SUBPEL_MASK);
+
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t res = 0;
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        res += x_filter[k] * src[y * src_stride + x - fo_horiz + k];
+      }
+      res = ROUND_POWER_OF_TWO(res, conv_params->round_0);
+      dst[y * dst_stride + x] = clip_pixel(ROUND_POWER_OF_TWO(res, bits));
+    }
+  }
+}
+
+void av1_dist_wtd_convolve_2d_c(const uint8_t *src, int src_stride,
+                                uint8_t *dst, int dst_stride, int w, int h,
+                                const InterpFilterParams *filter_params_x,
+                                const InterpFilterParams *filter_params_y,
+                                const int subpel_x_qn, const int subpel_y_qn,
+                                ConvolveParams *conv_params) {
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  int dst16_stride = conv_params->dst_stride;
+  int16_t im_block[(MAX_SB_SIZE + MAX_FILTER_TAP - 1) * MAX_SB_SIZE];
+  int im_h = h + filter_params_y->taps - 1;
+  int im_stride = w;
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int bd = 8;
+  const int round_bits =
+      2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+
+  // horizontal filter
+  const uint8_t *src_horiz = src - fo_vert * src_stride;
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_x, subpel_x_qn & SUBPEL_MASK);
+  for (int y = 0; y < im_h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t sum = (1 << (bd + FILTER_BITS - 1));
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        sum += x_filter[k] * src_horiz[y * src_stride + x - fo_horiz + k];
+      }
+      assert(0 <= sum && sum < (1 << (bd + FILTER_BITS + 1)));
+      im_block[y * im_stride + x] =
+          (int16_t)ROUND_POWER_OF_TWO(sum, conv_params->round_0);
+    }
+  }
+
+  // vertical filter
+  int16_t *src_vert = im_block + fo_vert * im_stride;
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_y, subpel_y_qn & SUBPEL_MASK);
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t sum = 1 << offset_bits;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        sum += y_filter[k] * src_vert[(y - fo_vert + k) * im_stride + x];
+      }
+      assert(0 <= sum && sum < (1 << (offset_bits + 2)));
+      CONV_BUF_TYPE res = ROUND_POWER_OF_TWO(sum, conv_params->round_1);
+      if (conv_params->do_average) {
+        int32_t tmp = dst16[y * dst16_stride + x];
+        if (conv_params->use_dist_wtd_comp_avg) {
+          tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+          tmp = tmp >> DIST_PRECISION_BITS;
+        } else {
+          tmp += res;
+          tmp = tmp >> 1;
+        }
+        tmp -= (1 << (offset_bits - conv_params->round_1)) +
+               (1 << (offset_bits - conv_params->round_1 - 1));
+        dst[y * dst_stride + x] =
+            clip_pixel(ROUND_POWER_OF_TWO(tmp, round_bits));
+      } else {
+        dst16[y * dst16_stride + x] = res;
+      }
+    }
+  }
+}
+
+void av1_dist_wtd_convolve_y_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                               int dst_stride, int w, int h,
+                               const InterpFilterParams *filter_params_y,
+                               const int subpel_y_qn,
+                               ConvolveParams *conv_params) {
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  int dst16_stride = conv_params->dst_stride;
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  const int bits = FILTER_BITS - conv_params->round_0;
+  const int bd = 8;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  const int round_offset = (1 << (offset_bits - conv_params->round_1)) +
+                           (1 << (offset_bits - conv_params->round_1 - 1));
+  const int round_bits =
+      2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+
+  // vertical filter
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_y, subpel_y_qn & SUBPEL_MASK);
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t res = 0;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        res += y_filter[k] * src[(y - fo_vert + k) * src_stride + x];
+      }
+      res *= (1 << bits);
+      res = ROUND_POWER_OF_TWO(res, conv_params->round_1) + round_offset;
+
+      if (conv_params->do_average) {
+        int32_t tmp = dst16[y * dst16_stride + x];
+        if (conv_params->use_dist_wtd_comp_avg) {
+          tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+          tmp = tmp >> DIST_PRECISION_BITS;
+        } else {
+          tmp += res;
+          tmp = tmp >> 1;
+        }
+        tmp -= round_offset;
+        dst[y * dst_stride + x] =
+            clip_pixel(ROUND_POWER_OF_TWO(tmp, round_bits));
+      } else {
+        dst16[y * dst16_stride + x] = res;
+      }
+    }
+  }
+}
+
+void av1_dist_wtd_convolve_x_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                               int dst_stride, int w, int h,
+                               const InterpFilterParams *filter_params_x,
+                               const int subpel_x_qn,
+                               ConvolveParams *conv_params) {
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  int dst16_stride = conv_params->dst_stride;
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int bits = FILTER_BITS - conv_params->round_1;
+  const int bd = 8;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  const int round_offset = (1 << (offset_bits - conv_params->round_1)) +
+                           (1 << (offset_bits - conv_params->round_1 - 1));
+  const int round_bits =
+      2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+
+  // horizontal filter
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_x, subpel_x_qn & SUBPEL_MASK);
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t res = 0;
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        res += x_filter[k] * src[y * src_stride + x - fo_horiz + k];
+      }
+      res = (1 << bits) * ROUND_POWER_OF_TWO(res, conv_params->round_0);
+      res += round_offset;
+
+      if (conv_params->do_average) {
+        int32_t tmp = dst16[y * dst16_stride + x];
+        if (conv_params->use_dist_wtd_comp_avg) {
+          tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+          tmp = tmp >> DIST_PRECISION_BITS;
+        } else {
+          tmp += res;
+          tmp = tmp >> 1;
+        }
+        tmp -= round_offset;
+        dst[y * dst_stride + x] =
+            clip_pixel(ROUND_POWER_OF_TWO(tmp, round_bits));
+      } else {
+        dst16[y * dst16_stride + x] = res;
+      }
+    }
+  }
+}
+
+void av1_dist_wtd_convolve_2d_copy_c(const uint8_t *src, int src_stride,
+                                     uint8_t *dst, int dst_stride, int w, int h,
+                                     ConvolveParams *conv_params) {
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  int dst16_stride = conv_params->dst_stride;
+  const int bits =
+      FILTER_BITS * 2 - conv_params->round_1 - conv_params->round_0;
+  const int bd = 8;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  const int round_offset = (1 << (offset_bits - conv_params->round_1)) +
+                           (1 << (offset_bits - conv_params->round_1 - 1));
+
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      CONV_BUF_TYPE res = src[y * src_stride + x] << bits;
+      res += round_offset;
+
+      if (conv_params->do_average) {
+        int32_t tmp = dst16[y * dst16_stride + x];
+        if (conv_params->use_dist_wtd_comp_avg) {
+          tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+          tmp = tmp >> DIST_PRECISION_BITS;
+        } else {
+          tmp += res;
+          tmp = tmp >> 1;
+        }
+        tmp -= round_offset;
+        dst[y * dst_stride + x] = clip_pixel(ROUND_POWER_OF_TWO(tmp, bits));
+      } else {
+        dst16[y * dst16_stride + x] = res;
+      }
+    }
+  }
+}
+
+void av1_convolve_2d_scale_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                             int dst_stride, int w, int h,
+                             const InterpFilterParams *filter_params_x,
+                             const InterpFilterParams *filter_params_y,
+                             const int subpel_x_qn, const int x_step_qn,
+                             const int subpel_y_qn, const int y_step_qn,
+                             ConvolveParams *conv_params) {
+  int16_t im_block[(2 * MAX_SB_SIZE + MAX_FILTER_TAP) * MAX_SB_SIZE];
+  int im_h = (((h - 1) * y_step_qn + subpel_y_qn) >> SCALE_SUBPEL_BITS) +
+             filter_params_y->taps;
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  const int dst16_stride = conv_params->dst_stride;
+  const int bits =
+      FILTER_BITS * 2 - conv_params->round_0 - conv_params->round_1;
+  assert(bits >= 0);
+  int im_stride = w;
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int bd = 8;
+
+  // horizontal filter
+  const uint8_t *src_horiz = src - fo_vert * src_stride;
+  for (int y = 0; y < im_h; ++y) {
+    int x_qn = subpel_x_qn;
+    for (int x = 0; x < w; ++x, x_qn += x_step_qn) {
+      const uint8_t *const src_x = &src_horiz[(x_qn >> SCALE_SUBPEL_BITS)];
+      const int x_filter_idx = (x_qn & SCALE_SUBPEL_MASK) >> SCALE_EXTRA_BITS;
+      assert(x_filter_idx < SUBPEL_SHIFTS);
+      const int16_t *x_filter =
+          av1_get_interp_filter_subpel_kernel(*filter_params_x, x_filter_idx);
+      int32_t sum = (1 << (bd + FILTER_BITS - 1));
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        sum += x_filter[k] * src_x[k - fo_horiz];
+      }
+      assert(0 <= sum && sum < (1 << (bd + FILTER_BITS + 1)));
+      im_block[y * im_stride + x] =
+          (int16_t)ROUND_POWER_OF_TWO(sum, conv_params->round_0);
+    }
+    src_horiz += src_stride;
+  }
+
+  // vertical filter
+  int16_t *src_vert = im_block + fo_vert * im_stride;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  for (int x = 0; x < w; ++x) {
+    int y_qn = subpel_y_qn;
+    for (int y = 0; y < h; ++y, y_qn += y_step_qn) {
+      const int16_t *src_y = &src_vert[(y_qn >> SCALE_SUBPEL_BITS) * im_stride];
+      const int y_filter_idx = (y_qn & SCALE_SUBPEL_MASK) >> SCALE_EXTRA_BITS;
+      assert(y_filter_idx < SUBPEL_SHIFTS);
+      const int16_t *y_filter =
+          av1_get_interp_filter_subpel_kernel(*filter_params_y, y_filter_idx);
+      int32_t sum = 1 << offset_bits;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        sum += y_filter[k] * src_y[(k - fo_vert) * im_stride];
+      }
+      assert(0 <= sum && sum < (1 << (offset_bits + 2)));
+      CONV_BUF_TYPE res = ROUND_POWER_OF_TWO(sum, conv_params->round_1);
+      if (conv_params->is_compound) {
+        if (conv_params->do_average) {
+          int32_t tmp = dst16[y * dst16_stride + x];
+          if (conv_params->use_dist_wtd_comp_avg) {
+            tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+            tmp = tmp >> DIST_PRECISION_BITS;
+          } else {
+            tmp += res;
+            tmp = tmp >> 1;
+          }
+          /* Subtract round offset and convolve round */
+          tmp = tmp - ((1 << (offset_bits - conv_params->round_1)) +
+                       (1 << (offset_bits - conv_params->round_1 - 1)));
+          dst[y * dst_stride + x] = clip_pixel(ROUND_POWER_OF_TWO(tmp, bits));
+        } else {
+          dst16[y * dst16_stride + x] = res;
+        }
+      } else {
+        /* Subtract round offset and convolve round */
+        int32_t tmp = res - ((1 << (offset_bits - conv_params->round_1)) +
+                             (1 << (offset_bits - conv_params->round_1 - 1)));
+        dst[y * dst_stride + x] = clip_pixel(ROUND_POWER_OF_TWO(tmp, bits));
+      }
+    }
+    src_vert++;
+  }
+}
+
+static void convolve_2d_scale_wrapper(
+    const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w,
+    int h, const InterpFilterParams *filter_params_x,
+    const InterpFilterParams *filter_params_y, const int subpel_x_qn,
+    const int x_step_qn, const int subpel_y_qn, const int y_step_qn,
+    ConvolveParams *conv_params) {
+  if (conv_params->is_compound) {
+    assert(conv_params->dst != NULL);
+  }
+  av1_convolve_2d_scale_c(src, src_stride, dst, dst_stride, w, h, filter_params_x,
+                        filter_params_y, subpel_x_qn, x_step_qn, subpel_y_qn,
+                        y_step_qn, conv_params);
+}
+
+static void convolve_2d_facade_compound(
+    const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w,
+    int h, const InterpFilterParams *filter_params_x,
+    const InterpFilterParams *filter_params_y, const int subpel_x_qn,
+    const int subpel_y_qn, ConvolveParams *conv_params) {
+  const Bool need_x = subpel_x_qn != 0;
+  const Bool need_y = subpel_y_qn != 0;
+  if (!need_x && !need_y) {
+    av1_dist_wtd_convolve_2d_copy_c(src, src_stride, dst, dst_stride, w, h,
+                                  conv_params);
+  } else if (need_x && !need_y) {
+    av1_dist_wtd_convolve_x_c(src, src_stride, dst, dst_stride, w, h,
+                            filter_params_x, subpel_x_qn, conv_params);
+  } else if (!need_x && need_y) {
+    av1_dist_wtd_convolve_y_c(src, src_stride, dst, dst_stride, w, h,
+                            filter_params_y, subpel_y_qn, conv_params);
+  } else {
+    assert(need_y && need_x);
+    av1_dist_wtd_convolve_2d_c(src, src_stride, dst, dst_stride, w, h,
+                             filter_params_x, filter_params_y, subpel_x_qn,
+                             subpel_y_qn, conv_params);
+  }
+}
+void aom_convolve_copy_neon(const uint8_t *src, ptrdiff_t src_stride,
+                            uint8_t *dst, ptrdiff_t dst_stride, int w, int h);
+
+static void convolve_2d_facade_single(
+    const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w,
+    int h, const InterpFilterParams *filter_params_x,
+    const InterpFilterParams *filter_params_y, const int subpel_x_qn,
+    const int subpel_y_qn, ConvolveParams *conv_params) {
+  const Bool need_x = subpel_x_qn != 0;
+  const Bool need_y = subpel_y_qn != 0;
+  if (!need_x && !need_y) {
+    aom_convolve_copy_neon(src, src_stride, dst, dst_stride, w, h);
+  } else if (need_x && !need_y) {
+    av1_convolve_x_sr_c(src, src_stride, dst, dst_stride, w, h, filter_params_x,
+                      subpel_x_qn, conv_params);
+  } else if (!need_x && need_y) {
+    av1_convolve_y_sr_c(src, src_stride, dst, dst_stride, w, h, filter_params_y,
+                      subpel_y_qn);
+  } else {
+    assert(need_x && need_y);
+    av1_convolve_2d_sr_c(src, src_stride, dst, dst_stride, w, h, filter_params_x,
+                       filter_params_y, subpel_x_qn, subpel_y_qn, conv_params);
+  }
+}
+
+void av1_convolve_2d_facade(const uint8_t *src, int src_stride, uint8_t *dst,
+                            int dst_stride, int w, int h,
+                            const InterpFilterParams *interp_filters[2],
+                            const int subpel_x_qn, int x_step_q4,
+                            const int subpel_y_qn, int y_step_q4, int scaled,
+                            ConvolveParams *conv_params) {
+  (void)x_step_q4;
+  (void)y_step_q4;
+  (void)dst;
+  (void)dst_stride;
+
+  const InterpFilterParams *filter_params_x = interp_filters[0];
+  const InterpFilterParams *filter_params_y = interp_filters[1];
+
+  // TODO(jingning, yunqing): Add SIMD support to 2-tap filter case.
+  // Do we have SIMD support to 4-tap case?
+  // 2-tap filter indicates that it is for IntraBC.
+  if (filter_params_x->taps == 2 || filter_params_y->taps == 2) {
+    assert(filter_params_x->taps == 2 && filter_params_y->taps == 2);
+    assert(!scaled);
+    if (subpel_x_qn && subpel_y_qn) {
+      av1_convolve_2d_sr_c(src, src_stride, dst, dst_stride, w, h,
+                           filter_params_x, filter_params_y, subpel_x_qn,
+                           subpel_y_qn, conv_params);
+      return;
+    } else if (subpel_x_qn) {
+      av1_convolve_x_sr_c(src, src_stride, dst, dst_stride, w, h,
+                          filter_params_x, subpel_x_qn, conv_params);
+      return;
+    } else if (subpel_y_qn) {
+      av1_convolve_y_sr_c(src, src_stride, dst, dst_stride, w, h,
+                          filter_params_y, subpel_y_qn);
+      return;
+    }
+  }
+
+  if (scaled) {
+    convolve_2d_scale_wrapper(src, src_stride, dst, dst_stride, w, h,
+                              filter_params_x, filter_params_y, subpel_x_qn,
+                              x_step_q4, subpel_y_qn, y_step_q4, conv_params);
+  } else if (conv_params->is_compound) {
+    convolve_2d_facade_compound(src, src_stride, dst, dst_stride, w, h,
+                                filter_params_x, filter_params_y, subpel_x_qn,
+                                subpel_y_qn, conv_params);
+  } else {
+    convolve_2d_facade_single(src, src_stride, dst, dst_stride, w, h,
+                              filter_params_x, filter_params_y, subpel_x_qn,
+                              subpel_y_qn, conv_params);
+  }
+}
+
+#if CONFIG_AV1_HIGHBITDEPTH
+void av1_highbd_convolve_x_sr_c(const uint16_t *src, int src_stride,
+                                uint16_t *dst, int dst_stride, int w, int h,
+                                const InterpFilterParams *filter_params_x,
+                                const int subpel_x_qn,
+                                ConvolveParams *conv_params, int bd) {
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int bits = FILTER_BITS - conv_params->round_0;
+
+  assert(bits >= 0);
+  assert((FILTER_BITS - conv_params->round_1) >= 0 ||
+         ((conv_params->round_0 + conv_params->round_1) == 2 * FILTER_BITS));
+
+  // horizontal filter
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      filter_params_x, subpel_x_qn & SUBPEL_MASK);
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t res = 0;
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        res += x_filter[k] * src[y * src_stride + x - fo_horiz + k];
+      }
+      res = ROUND_POWER_OF_TWO(res, conv_params->round_0);
+      dst[y * dst_stride + x] =
+          clip_pixel_highbd(ROUND_POWER_OF_TWO(res, bits), bd);
+    }
+  }
+}
+
+void av1_highbd_convolve_y_sr_c(const uint16_t *src, int src_stride,
+                                uint16_t *dst, int dst_stride, int w, int h,
+                                const InterpFilterParams *filter_params_y,
+                                const int subpel_y_qn, int bd) {
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  // vertical filter
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      filter_params_y, subpel_y_qn & SUBPEL_MASK);
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t res = 0;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        res += y_filter[k] * src[(y - fo_vert + k) * src_stride + x];
+      }
+      dst[y * dst_stride + x] =
+          clip_pixel_highbd(ROUND_POWER_OF_TWO(res, FILTER_BITS), bd);
+    }
+  }
+}
+
+void av1_highbd_convolve_2d_sr_c(const uint16_t *src, int src_stride,
+                                 uint16_t *dst, int dst_stride, int w, int h,
+                                 const InterpFilterParams *filter_params_x,
+                                 const InterpFilterParams *filter_params_y,
+                                 const int subpel_x_qn, const int subpel_y_qn,
+                                 ConvolveParams *conv_params, int bd) {
+  int16_t im_block[(MAX_SB_SIZE + MAX_FILTER_TAP - 1) * MAX_SB_SIZE];
+  int im_h = h + filter_params_y->taps - 1;
+  int im_stride = w;
+  assert(w <= MAX_SB_SIZE && h <= MAX_SB_SIZE);
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int bits =
+      FILTER_BITS * 2 - conv_params->round_0 - conv_params->round_1;
+  assert(bits >= 0);
+
+  // horizontal filter
+  const uint16_t *src_horiz = src - fo_vert * src_stride;
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      filter_params_x, subpel_x_qn & SUBPEL_MASK);
+  for (int y = 0; y < im_h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t sum = (1 << (bd + FILTER_BITS - 1));
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        sum += x_filter[k] * src_horiz[y * src_stride + x - fo_horiz + k];
+      }
+      assert(0 <= sum && sum < (1 << (bd + FILTER_BITS + 1)));
+      im_block[y * im_stride + x] =
+          ROUND_POWER_OF_TWO(sum, conv_params->round_0);
+    }
+  }
+
+  // vertical filter
+  int16_t *src_vert = im_block + fo_vert * im_stride;
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      filter_params_y, subpel_y_qn & SUBPEL_MASK);
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t sum = 1 << offset_bits;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        sum += y_filter[k] * src_vert[(y - fo_vert + k) * im_stride + x];
+      }
+      assert(0 <= sum && sum < (1 << (offset_bits + 2)));
+      int32_t res = ROUND_POWER_OF_TWO(sum, conv_params->round_1) -
+                    ((1 << (offset_bits - conv_params->round_1)) +
+                     (1 << (offset_bits - conv_params->round_1 - 1)));
+      dst[y * dst_stride + x] =
+          clip_pixel_highbd(ROUND_POWER_OF_TWO(res, bits), bd);
+    }
+  }
+}
+
+void av1_highbd_dist_wtd_convolve_2d_c(
+    const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w,
+    int h, const InterpFilterParams *filter_params_x,
+    const InterpFilterParams *filter_params_y, const int subpel_x_qn,
+    const int subpel_y_qn, ConvolveParams *conv_params, int bd) {
+  int x, y, k;
+  int16_t im_block[(MAX_SB_SIZE + MAX_FILTER_TAP - 1) * MAX_SB_SIZE];
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  int dst16_stride = conv_params->dst_stride;
+  int im_h = h + filter_params_y->taps - 1;
+  int im_stride = w;
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int round_bits =
+      2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+  assert(round_bits >= 0);
+
+  // horizontal filter
+  const uint16_t *src_horiz = src - fo_vert * src_stride;
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      filter_params_x, subpel_x_qn & SUBPEL_MASK);
+  for (y = 0; y < im_h; ++y) {
+    for (x = 0; x < w; ++x) {
+      int32_t sum = (1 << (bd + FILTER_BITS - 1));
+      for (k = 0; k < filter_params_x->taps; ++k) {
+        sum += x_filter[k] * src_horiz[y * src_stride + x - fo_horiz + k];
+      }
+      assert(0 <= sum && sum < (1 << (bd + FILTER_BITS + 1)));
+      (void)bd;
+      im_block[y * im_stride + x] =
+          (int16_t)ROUND_POWER_OF_TWO(sum, conv_params->round_0);
+    }
+  }
+
+  // vertical filter
+  int16_t *src_vert = im_block + fo_vert * im_stride;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      filter_params_y, subpel_y_qn & SUBPEL_MASK);
+  for (y = 0; y < h; ++y) {
+    for (x = 0; x < w; ++x) {
+      int32_t sum = 1 << offset_bits;
+      for (k = 0; k < filter_params_y->taps; ++k) {
+        sum += y_filter[k] * src_vert[(y - fo_vert + k) * im_stride + x];
+      }
+      assert(0 <= sum && sum < (1 << (offset_bits + 2)));
+      CONV_BUF_TYPE res = ROUND_POWER_OF_TWO(sum, conv_params->round_1);
+      if (conv_params->do_average) {
+        int32_t tmp = dst16[y * dst16_stride + x];
+        if (conv_params->use_dist_wtd_comp_avg) {
+          tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+          tmp = tmp >> DIST_PRECISION_BITS;
+        } else {
+          tmp += res;
+          tmp = tmp >> 1;
+        }
+        tmp -= (1 << (offset_bits - conv_params->round_1)) +
+               (1 << (offset_bits - conv_params->round_1 - 1));
+        dst[y * dst_stride + x] =
+            clip_pixel_highbd(ROUND_POWER_OF_TWO(tmp, round_bits), bd);
+      } else {
+        dst16[y * dst16_stride + x] = res;
+      }
+    }
+  }
+}
+
+void av1_highbd_dist_wtd_convolve_x_c(const uint16_t *src, int src_stride,
+                                      uint16_t *dst, int dst_stride, int w,
+                                      int h,
+                                      const InterpFilterParams *filter_params_x,
+                                      const int subpel_x_qn,
+                                      ConvolveParams *conv_params, int bd) {
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  int dst16_stride = conv_params->dst_stride;
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  const int bits = FILTER_BITS - conv_params->round_1;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  const int round_offset = (1 << (offset_bits - conv_params->round_1)) +
+                           (1 << (offset_bits - conv_params->round_1 - 1));
+  const int round_bits =
+      2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+  assert(round_bits >= 0);
+  assert(bits >= 0);
+  // horizontal filter
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      filter_params_x, subpel_x_qn & SUBPEL_MASK);
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t res = 0;
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        res += x_filter[k] * src[y * src_stride + x - fo_horiz + k];
+      }
+      res = (1 << bits) * ROUND_POWER_OF_TWO(res, conv_params->round_0);
+      res += round_offset;
+
+      if (conv_params->do_average) {
+        int32_t tmp = dst16[y * dst16_stride + x];
+        if (conv_params->use_dist_wtd_comp_avg) {
+          tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+          tmp = tmp >> DIST_PRECISION_BITS;
+        } else {
+          tmp += res;
+          tmp = tmp >> 1;
+        }
+        tmp -= round_offset;
+        dst[y * dst_stride + x] =
+            clip_pixel_highbd(ROUND_POWER_OF_TWO(tmp, round_bits), bd);
+      } else {
+        dst16[y * dst16_stride + x] = res;
+      }
+    }
+  }
+}
+
+void av1_highbd_dist_wtd_convolve_y_c(const uint16_t *src, int src_stride,
+                                      uint16_t *dst, int dst_stride, int w,
+                                      int h,
+                                      const InterpFilterParams *filter_params_y,
+                                      const int subpel_y_qn,
+                                      ConvolveParams *conv_params, int bd) {
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  int dst16_stride = conv_params->dst_stride;
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  const int bits = FILTER_BITS - conv_params->round_0;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  const int round_offset = (1 << (offset_bits - conv_params->round_1)) +
+                           (1 << (offset_bits - conv_params->round_1 - 1));
+  const int round_bits =
+      2 * FILTER_BITS - conv_params->round_0 - conv_params->round_1;
+  assert(round_bits >= 0);
+  assert(bits >= 0);
+  // vertical filter
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      filter_params_y, subpel_y_qn & SUBPEL_MASK);
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      int32_t res = 0;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        res += y_filter[k] * src[(y - fo_vert + k) * src_stride + x];
+      }
+      res *= (1 << bits);
+      res = ROUND_POWER_OF_TWO(res, conv_params->round_1) + round_offset;
+
+      if (conv_params->do_average) {
+        int32_t tmp = dst16[y * dst16_stride + x];
+        if (conv_params->use_dist_wtd_comp_avg) {
+          tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+          tmp = tmp >> DIST_PRECISION_BITS;
+        } else {
+          tmp += res;
+          tmp = tmp >> 1;
+        }
+        tmp -= round_offset;
+        dst[y * dst_stride + x] =
+            clip_pixel_highbd(ROUND_POWER_OF_TWO(tmp, round_bits), bd);
+      } else {
+        dst16[y * dst16_stride + x] = res;
+      }
+    }
+  }
+}
+
+void av1_highbd_dist_wtd_convolve_2d_copy_c(const uint16_t *src, int src_stride,
+                                            uint16_t *dst, int dst_stride,
+                                            int w, int h,
+                                            ConvolveParams *conv_params,
+                                            int bd) {
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  int dst16_stride = conv_params->dst_stride;
+  const int bits =
+      FILTER_BITS * 2 - conv_params->round_1 - conv_params->round_0;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  const int round_offset = (1 << (offset_bits - conv_params->round_1)) +
+                           (1 << (offset_bits - conv_params->round_1 - 1));
+  assert(bits >= 0);
+
+  for (int y = 0; y < h; ++y) {
+    for (int x = 0; x < w; ++x) {
+      CONV_BUF_TYPE res = src[y * src_stride + x] << bits;
+      res += round_offset;
+      if (conv_params->do_average) {
+        int32_t tmp = dst16[y * dst16_stride + x];
+        if (conv_params->use_dist_wtd_comp_avg) {
+          tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+          tmp = tmp >> DIST_PRECISION_BITS;
+        } else {
+          tmp += res;
+          tmp = tmp >> 1;
+        }
+        tmp -= round_offset;
+        dst[y * dst_stride + x] =
+            clip_pixel_highbd(ROUND_POWER_OF_TWO(tmp, bits), bd);
+      } else {
+        dst16[y * dst16_stride + x] = res;
+      }
+    }
+  }
+}
+
+void av1_highbd_convolve_2d_scale_c(const uint16_t *src, int src_stride,
+                                    uint16_t *dst, int dst_stride, int w, int h,
+                                    const InterpFilterParams *filter_params_x,
+                                    const InterpFilterParams *filter_params_y,
+                                    const int subpel_x_qn, const int x_step_qn,
+                                    const int subpel_y_qn, const int y_step_qn,
+                                    ConvolveParams *conv_params, int bd) {
+  int16_t im_block[(2 * MAX_SB_SIZE + MAX_FILTER_TAP) * MAX_SB_SIZE];
+  int im_h = (((h - 1) * y_step_qn + subpel_y_qn) >> SCALE_SUBPEL_BITS) +
+             filter_params_y->taps;
+  int im_stride = w;
+  const int fo_vert = filter_params_y->taps / 2 - 1;
+  const int fo_horiz = filter_params_x->taps / 2 - 1;
+  CONV_BUF_TYPE *dst16 = conv_params->dst;
+  const int dst16_stride = conv_params->dst_stride;
+  const int bits =
+      FILTER_BITS * 2 - conv_params->round_0 - conv_params->round_1;
+  assert(bits >= 0);
+  // horizontal filter
+  const uint16_t *src_horiz = src - fo_vert * src_stride;
+  for (int y = 0; y < im_h; ++y) {
+    int x_qn = subpel_x_qn;
+    for (int x = 0; x < w; ++x, x_qn += x_step_qn) {
+      const uint16_t *const src_x = &src_horiz[(x_qn >> SCALE_SUBPEL_BITS)];
+      const int x_filter_idx = (x_qn & SCALE_SUBPEL_MASK) >> SCALE_EXTRA_BITS;
+      assert(x_filter_idx < SUBPEL_SHIFTS);
+      const int16_t *x_filter =
+          av1_get_interp_filter_subpel_kernel(filter_params_x, x_filter_idx);
+      int32_t sum = (1 << (bd + FILTER_BITS - 1));
+      for (int k = 0; k < filter_params_x->taps; ++k) {
+        sum += x_filter[k] * src_x[k - fo_horiz];
+      }
+      assert(0 <= sum && sum < (1 << (bd + FILTER_BITS + 1)));
+      im_block[y * im_stride + x] =
+          (int16_t)ROUND_POWER_OF_TWO(sum, conv_params->round_0);
+    }
+    src_horiz += src_stride;
+  }
+
+  // vertical filter
+  int16_t *src_vert = im_block + fo_vert * im_stride;
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  for (int x = 0; x < w; ++x) {
+    int y_qn = subpel_y_qn;
+    for (int y = 0; y < h; ++y, y_qn += y_step_qn) {
+      const int16_t *src_y = &src_vert[(y_qn >> SCALE_SUBPEL_BITS) * im_stride];
+      const int y_filter_idx = (y_qn & SCALE_SUBPEL_MASK) >> SCALE_EXTRA_BITS;
+      assert(y_filter_idx < SUBPEL_SHIFTS);
+      const int16_t *y_filter =
+          av1_get_interp_filter_subpel_kernel(filter_params_y, y_filter_idx);
+      int32_t sum = 1 << offset_bits;
+      for (int k = 0; k < filter_params_y->taps; ++k) {
+        sum += y_filter[k] * src_y[(k - fo_vert) * im_stride];
+      }
+      assert(0 <= sum && sum < (1 << (offset_bits + 2)));
+      CONV_BUF_TYPE res = ROUND_POWER_OF_TWO(sum, conv_params->round_1);
+      if (conv_params->is_compound) {
+        if (conv_params->do_average) {
+          int32_t tmp = dst16[y * dst16_stride + x];
+          if (conv_params->use_dist_wtd_comp_avg) {
+            tmp = tmp * conv_params->fwd_offset + res * conv_params->bck_offset;
+            tmp = tmp >> DIST_PRECISION_BITS;
+          } else {
+            tmp += res;
+            tmp = tmp >> 1;
+          }
+          /* Subtract round offset and convolve round */
+          tmp = tmp - ((1 << (offset_bits - conv_params->round_1)) +
+                       (1 << (offset_bits - conv_params->round_1 - 1)));
+          dst[y * dst_stride + x] =
+              clip_pixel_highbd(ROUND_POWER_OF_TWO(tmp, bits), bd);
+        } else {
+          dst16[y * dst16_stride + x] = res;
+        }
+      } else {
+        /* Subtract round offset and convolve round */
+        int32_t tmp = res - ((1 << (offset_bits - conv_params->round_1)) +
+                             (1 << (offset_bits - conv_params->round_1 - 1)));
+        dst[y * dst_stride + x] =
+            clip_pixel_highbd(ROUND_POWER_OF_TWO(tmp, bits), bd);
+      }
+    }
+    src_vert++;
+  }
+}
+
+static void highbd_convolve_2d_facade_compound(
+    const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride,
+    const int w, const int h, const InterpFilterParams *filter_params_x,
+    const InterpFilterParams *filter_params_y, const int subpel_x_qn,
+    const int subpel_y_qn, ConvolveParams *conv_params, int bd) {
+  const bool need_x = subpel_x_qn != 0;
+  const bool need_y = subpel_y_qn != 0;
+  if (!need_x && !need_y) {
+    av1_highbd_dist_wtd_convolve_2d_copy(src, src_stride, dst, dst_stride, w, h,
+                                         conv_params, bd);
+  } else if (need_x && !need_y) {
+    av1_highbd_dist_wtd_convolve_x(src, src_stride, dst, dst_stride, w, h,
+                                   filter_params_x, subpel_x_qn, conv_params,
+                                   bd);
+  } else if (!need_x && need_y) {
+    av1_highbd_dist_wtd_convolve_y(src, src_stride, dst, dst_stride, w, h,
+                                   filter_params_y, subpel_y_qn, conv_params,
+                                   bd);
+  } else {
+    assert(need_x && need_y);
+    av1_highbd_dist_wtd_convolve_2d(src, src_stride, dst, dst_stride, w, h,
+                                    filter_params_x, filter_params_y,
+                                    subpel_x_qn, subpel_y_qn, conv_params, bd);
+  }
+}
+
+static void highbd_convolve_2d_facade_single(
+    const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride,
+    const int w, const int h, const InterpFilterParams *filter_params_x,
+    const InterpFilterParams *filter_params_y, const int subpel_x_qn,
+    const int subpel_y_qn, ConvolveParams *conv_params, int bd) {
+  const bool need_x = subpel_x_qn != 0;
+  const bool need_y = subpel_y_qn != 0;
+
+  if (!need_x && !need_y) {
+    aom_highbd_convolve_copy(src, src_stride, dst, dst_stride, w, h);
+  } else if (need_x && !need_y) {
+    av1_highbd_convolve_x_sr(src, src_stride, dst, dst_stride, w, h,
+                             filter_params_x, subpel_x_qn, conv_params, bd);
+  } else if (!need_x && need_y) {
+    av1_highbd_convolve_y_sr(src, src_stride, dst, dst_stride, w, h,
+                             filter_params_y, subpel_y_qn, bd);
+  } else {
+    assert(need_x && need_y);
+    av1_highbd_convolve_2d_sr(src, src_stride, dst, dst_stride, w, h,
+                              filter_params_x, filter_params_y, subpel_x_qn,
+                              subpel_y_qn, conv_params, bd);
+  }
+}
+
+void av1_highbd_convolve_2d_facade(const uint8_t *src8, int src_stride,
+                                   uint8_t *dst8, int dst_stride, int w, int h,
+                                   const InterpFilterParams *interp_filters[2],
+                                   const int subpel_x_qn, int x_step_q4,
+                                   const int subpel_y_qn, int y_step_q4,
+                                   int scaled, ConvolveParams *conv_params,
+                                   int bd) {
+  (void)x_step_q4;
+  (void)y_step_q4;
+  (void)dst_stride;
+  const uint16_t *src = CONVERT_TO_SHORTPTR(src8);
+
+  const int need_filter_params_x = (subpel_x_qn != 0) | scaled;
+  const int need_filter_params_y = (subpel_y_qn != 0) | scaled;
+  const InterpFilterParams *filter_params_x =
+      need_filter_params_x ? interp_filters[0] : NULL;
+  const InterpFilterParams *filter_params_y =
+      need_filter_params_y ? interp_filters[1] : NULL;
+
+  uint16_t *dst = CONVERT_TO_SHORTPTR(dst8);
+  if (scaled) {
+    if (conv_params->is_compound) {
+      assert(conv_params->dst != NULL);
+    }
+    av1_highbd_convolve_2d_scale(src, src_stride, dst, dst_stride, w, h,
+                                 filter_params_x, filter_params_y, subpel_x_qn,
+                                 x_step_q4, subpel_y_qn, y_step_q4, conv_params,
+                                 bd);
+  } else if (conv_params->is_compound) {
+    highbd_convolve_2d_facade_compound(
+        src, src_stride, dst, dst_stride, w, h, filter_params_x,
+        filter_params_y, subpel_x_qn, subpel_y_qn, conv_params, bd);
+  } else {
+    highbd_convolve_2d_facade_single(src, src_stride, dst, dst_stride, w, h,
+                                     filter_params_x, filter_params_y,
+                                     subpel_x_qn, subpel_y_qn, conv_params, bd);
+  }
+}
+#endif  // CONFIG_AV1_HIGHBITDEPTH
+
+// Note: Fixed size intermediate buffers, place limits on parameters
+// of some functions. 2d filtering proceeds in 2 steps:
+//   (1) Interpolate horizontally into an intermediate buffer, temp.
+//   (2) Interpolate temp vertically to derive the sub-pixel result.
+// Deriving the maximum number of rows in the temp buffer (135):
+// --Smallest scaling factor is x1/2 ==> y_step_q4 = 32 (Normative).
+// --Largest block size is 128x128 pixels.
+// --128 rows in the downscaled frame span a distance of (128 - 1) * 32 in the
+//   original frame (in 1/16th pixel units).
+// --Must round-up because block may be located at sub-pixel position.
+// --Require an additional SUBPEL_TAPS rows for the 8-tap filter tails.
+// --((128 - 1) * 32 + 15) >> 4 + 8 = 263.
+#define WIENER_MAX_EXT_SIZE 263
+
+static INLINE int horz_scalar_product(const uint8_t *a, const int16_t *b) {
+  int sum = 0;
+  for (int k = 0; k < SUBPEL_TAPS; ++k) sum += a[k] * b[k];
+  return sum;
+}
+
+#if CONFIG_AV1_HIGHBITDEPTH
+static INLINE int highbd_horz_scalar_product(const uint16_t *a,
+                                             const int16_t *b) {
+  int sum = 0;
+  for (int k = 0; k < SUBPEL_TAPS; ++k) sum += a[k] * b[k];
+  return sum;
+}
+#endif
+
+static INLINE int highbd_vert_scalar_product(const uint16_t *a,
+                                             ptrdiff_t a_stride,
+                                             const int16_t *b) {
+  int sum = 0;
+  for (int k = 0; k < SUBPEL_TAPS; ++k) sum += a[k * a_stride] * b[k];
+  return sum;
+}
+
+static const InterpKernel *get_filter_base(const int16_t *filter) {
+  // NOTE: This assumes that the filter table is 256-byte aligned.
+  // TODO(agrange) Modify to make independent of table alignment.
+  return (const InterpKernel *)(((intptr_t)filter) & ~((intptr_t)0xFF));
+}
+
+static int get_filter_offset(const int16_t *f, const InterpKernel *base) {
+  return (int)((const InterpKernel *)(intptr_t)f - base);
+}
+
+static void convolve_add_src_horiz_hip(const uint8_t *src, ptrdiff_t src_stride,
+                                       uint16_t *dst, ptrdiff_t dst_stride,
+                                       const InterpKernel *x_filters, int x0_q4,
+                                       int x_step_q4, int w, int h,
+                                       int round0_bits) {
+  const int bd = 8;
+  src -= SUBPEL_TAPS / 2 - 1;
+  for (int y = 0; y < h; ++y) {
+    int x_q4 = x0_q4;
+    for (int x = 0; x < w; ++x) {
+      const uint8_t *const src_x = &src[x_q4 >> SUBPEL_BITS];
+      const int16_t *const x_filter = x_filters[x_q4 & SUBPEL_MASK];
+      const int rounding = ((int)src_x[SUBPEL_TAPS / 2 - 1] << FILTER_BITS) +
+                           (1 << (bd + FILTER_BITS - 1));
+      const int sum = horz_scalar_product(src_x, x_filter) + rounding;
+      dst[x] = (uint16_t)clamp(ROUND_POWER_OF_TWO(sum, round0_bits), 0,
+                               WIENER_CLAMP_LIMIT(round0_bits, bd) - 1);
+      x_q4 += x_step_q4;
+    }
+    src += src_stride;
+    dst += dst_stride;
+  }
+}
+
+static void convolve_add_src_vert_hip(const uint16_t *src, ptrdiff_t src_stride,
+                                      uint8_t *dst, ptrdiff_t dst_stride,
+                                      const InterpKernel *y_filters, int y0_q4,
+                                      int y_step_q4, int w, int h,
+                                      int round1_bits) {
+  const int bd = 8;
+  src -= src_stride * (SUBPEL_TAPS / 2 - 1);
+
+  for (int x = 0; x < w; ++x) {
+    int y_q4 = y0_q4;
+    for (int y = 0; y < h; ++y) {
+      const uint16_t *src_y = &src[(y_q4 >> SUBPEL_BITS) * src_stride];
+      const int16_t *const y_filter = y_filters[y_q4 & SUBPEL_MASK];
+      const int rounding =
+          ((int)src_y[(SUBPEL_TAPS / 2 - 1) * src_stride] << FILTER_BITS) -
+          (1 << (bd + round1_bits - 1));
+      const int sum =
+          highbd_vert_scalar_product(src_y, src_stride, y_filter) + rounding;
+      dst[y * dst_stride] = clip_pixel(ROUND_POWER_OF_TWO(sum, round1_bits));
+      y_q4 += y_step_q4;
+    }
+    ++src;
+    ++dst;
+  }
+}
+
+void av1_wiener_convolve_add_src_c(const uint8_t *src, ptrdiff_t src_stride,
+                                   uint8_t *dst, ptrdiff_t dst_stride,
+                                   const int16_t *filter_x, int x_step_q4,
+                                   const int16_t *filter_y, int y_step_q4,
+                                   int w, int h,
+                                   const ConvolveParams *conv_params) {
+  const InterpKernel *const filters_x = get_filter_base(filter_x);
+  const int x0_q4 = get_filter_offset(filter_x, filters_x);
+
+  const InterpKernel *const filters_y = get_filter_base(filter_y);
+  const int y0_q4 = get_filter_offset(filter_y, filters_y);
+
+  uint16_t temp[WIENER_MAX_EXT_SIZE * MAX_SB_SIZE];
+  const int intermediate_height =
+      (((h - 1) * y_step_q4 + y0_q4) >> SUBPEL_BITS) + SUBPEL_TAPS - 1;
+  memset(temp + (intermediate_height * MAX_SB_SIZE), 0, MAX_SB_SIZE);
+
+  assert(w <= MAX_SB_SIZE);
+  assert(h <= MAX_SB_SIZE);
+  assert(y_step_q4 <= 32);
+  assert(x_step_q4 <= 32);
+
+  convolve_add_src_horiz_hip(src - src_stride * (SUBPEL_TAPS / 2 - 1),
+                             src_stride, temp, MAX_SB_SIZE, filters_x, x0_q4,
+                             x_step_q4, w, intermediate_height,
+                             conv_params->round_0);
+  convolve_add_src_vert_hip(temp + MAX_SB_SIZE * (SUBPEL_TAPS / 2 - 1),
+                            MAX_SB_SIZE, dst, dst_stride, filters_y, y0_q4,
+                            y_step_q4, w, h, conv_params->round_1);
+}
+
+#if CONFIG_AV1_HIGHBITDEPTH
+static void highbd_convolve_add_src_horiz_hip(
+    const uint8_t *src8, ptrdiff_t src_stride, uint16_t *dst,
+    ptrdiff_t dst_stride, const InterpKernel *x_filters, int x0_q4,
+    int x_step_q4, int w, int h, int round0_bits, int bd) {
+  const int extraprec_clamp_limit = WIENER_CLAMP_LIMIT(round0_bits, bd);
+  uint16_t *src = CONVERT_TO_SHORTPTR(src8);
+  src -= SUBPEL_TAPS / 2 - 1;
+  for (int y = 0; y < h; ++y) {
+    int x_q4 = x0_q4;
+    for (int x = 0; x < w; ++x) {
+      const uint16_t *const src_x = &src[x_q4 >> SUBPEL_BITS];
+      const int16_t *const x_filter = x_filters[x_q4 & SUBPEL_MASK];
+      const int rounding = ((int)src_x[SUBPEL_TAPS / 2 - 1] << FILTER_BITS) +
+                           (1 << (bd + FILTER_BITS - 1));
+      const int sum = highbd_horz_scalar_product(src_x, x_filter) + rounding;
+      dst[x] = (uint16_t)clamp(ROUND_POWER_OF_TWO(sum, round0_bits), 0,
+                               extraprec_clamp_limit - 1);
+      x_q4 += x_step_q4;
+    }
+    src += src_stride;
+    dst += dst_stride;
+  }
+}
+
+static void highbd_convolve_add_src_vert_hip(
+    const uint16_t *src, ptrdiff_t src_stride, uint8_t *dst8,
+    ptrdiff_t dst_stride, const InterpKernel *y_filters, int y0_q4,
+    int y_step_q4, int w, int h, int round1_bits, int bd) {
+  uint16_t *dst = CONVERT_TO_SHORTPTR(dst8);
+  src -= src_stride * (SUBPEL_TAPS / 2 - 1);
+  for (int x = 0; x < w; ++x) {
+    int y_q4 = y0_q4;
+    for (int y = 0; y < h; ++y) {
+      const uint16_t *src_y = &src[(y_q4 >> SUBPEL_BITS) * src_stride];
+      const int16_t *const y_filter = y_filters[y_q4 & SUBPEL_MASK];
+      const int rounding =
+          ((int)src_y[(SUBPEL_TAPS / 2 - 1) * src_stride] << FILTER_BITS) -
+          (1 << (bd + round1_bits - 1));
+      const int sum =
+          highbd_vert_scalar_product(src_y, src_stride, y_filter) + rounding;
+      dst[y * dst_stride] =
+          clip_pixel_highbd(ROUND_POWER_OF_TWO(sum, round1_bits), bd);
+      y_q4 += y_step_q4;
+    }
+    ++src;
+    ++dst;
+  }
+}
+
+void av1_highbd_wiener_convolve_add_src_c(
+    const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+    ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4,
+    const int16_t *filter_y, int y_step_q4, int w, int h,
+    const ConvolveParams *conv_params, int bd) {
+  const InterpKernel *const filters_x = get_filter_base(filter_x);
+  const int x0_q4 = get_filter_offset(filter_x, filters_x);
+
+  const InterpKernel *const filters_y = get_filter_base(filter_y);
+  const int y0_q4 = get_filter_offset(filter_y, filters_y);
+
+  uint16_t temp[WIENER_MAX_EXT_SIZE * MAX_SB_SIZE];
+  const int intermediate_height =
+      (((h - 1) * y_step_q4 + y0_q4) >> SUBPEL_BITS) + SUBPEL_TAPS;
+
+  assert(w <= MAX_SB_SIZE);
+  assert(h <= MAX_SB_SIZE);
+  assert(y_step_q4 <= 32);
+  assert(x_step_q4 <= 32);
+  assert(bd + FILTER_BITS - conv_params->round_0 + 2 <= 16);
+
+  highbd_convolve_add_src_horiz_hip(src - src_stride * (SUBPEL_TAPS / 2 - 1),
+                                    src_stride, temp, MAX_SB_SIZE, filters_x,
+                                    x0_q4, x_step_q4, w, intermediate_height,
+                                    conv_params->round_0, bd);
+  highbd_convolve_add_src_vert_hip(
+      temp + MAX_SB_SIZE * (SUBPEL_TAPS / 2 - 1), MAX_SB_SIZE, dst, dst_stride,
+      filters_y, y0_q4, y_step_q4, w, h, conv_params->round_1, bd);
+}
+#endif  // CONFIG_AV1_HIGHBITDEPTH
diff --git a/Source/Lib/Common/ASM_NEON/convolve.h b/Source/Lib/Common/ASM_NEON/convolve.h
new file mode 100644
index 0000000..5caff18
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/convolve.h
@@ -0,0 +1,139 @@
+/*
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#ifndef AOM_AV1_COMMON_CONVOLVE_H_
+#define AOM_AV1_COMMON_CONVOLVE_H_
+
+#include "EbDefinitions.h"
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+#define ROUND0_BITS 3
+#define COMPOUND_ROUND1_BITS 7
+#define WIENER_ROUND0_BITS 3
+
+#define WIENER_CLAMP_LIMIT(r0, bd) (1 << ((bd) + 1 + FILTER_BITS - r0))
+
+typedef void (*aom_convolve_fn_t)(const uint8_t *src, int src_stride,
+                                  uint8_t *dst, int dst_stride, int w, int h,
+                                  const InterpFilterParams *filter_params_x,
+                                  const InterpFilterParams *filter_params_y,
+                                  const int subpel_x_qn, const int subpel_y_qn,
+                                  ConvolveParams *conv_params);
+
+typedef void (*aom_highbd_convolve_fn_t)(
+    const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w,
+    int h, const InterpFilterParams *filter_params_x,
+    const InterpFilterParams *filter_params_y, const int subpel_x_qn,
+    const int subpel_y_qn, ConvolveParams *conv_params, int bd);
+
+struct AV1Common;
+struct scale_factors;
+
+void av1_convolve_2d_facade(const uint8_t *src, int src_stride, uint8_t *dst,
+                            int dst_stride, int w, int h,
+                            const InterpFilterParams *interp_filters[2],
+                            const int subpel_x_qn, int x_step_q4,
+                            const int subpel_y_qn, int y_step_q4, int scaled,
+                            ConvolveParams *conv_params);
+
+static INLINE ConvolveParams get_conv_params_no_round(int cmp_index, int plane,
+                                                      CONV_BUF_TYPE *dst,
+                                                      int dst_stride,
+                                                      int is_compound, int bd) {
+  ConvolveParams conv_params;
+  assert(IMPLIES(cmp_index, is_compound));
+
+  conv_params.is_compound = is_compound;
+  conv_params.use_dist_wtd_comp_avg = 0;
+  conv_params.round_0 = ROUND0_BITS;
+  conv_params.round_1 = is_compound ? COMPOUND_ROUND1_BITS
+                                    : 2 * FILTER_BITS - conv_params.round_0;
+  const int intbufrange = bd + FILTER_BITS - conv_params.round_0 + 2;
+  assert(IMPLIES(bd < 12, intbufrange <= 16));
+  if (intbufrange > 16) {
+    conv_params.round_0 += intbufrange - 16;
+    if (!is_compound) conv_params.round_1 -= intbufrange - 16;
+  }
+  // TODO(yunqing): The following dst should only be valid while
+  // is_compound = 1;
+  conv_params.dst = dst;
+  conv_params.dst_stride = dst_stride;
+  conv_params.plane = plane;
+
+  // By default, set do average to 1 if this is the second single prediction
+  // in a compound mode.
+  conv_params.do_average = cmp_index;
+  return conv_params;
+}
+
+static INLINE ConvolveParams get_conv_params(int do_average, int plane,
+                                             int bd) {
+  return get_conv_params_no_round(do_average, plane, NULL, 0, 0, bd);
+}
+
+static INLINE ConvolveParams get_conv_params_wiener(int bd) {
+  ConvolveParams conv_params;
+  (void)bd;
+  conv_params.do_average = 0;
+  conv_params.is_compound = 0;
+  conv_params.round_0 = WIENER_ROUND0_BITS;
+  conv_params.round_1 = 2 * FILTER_BITS - conv_params.round_0;
+  const int intbufrange = bd + FILTER_BITS - conv_params.round_0 + 2;
+  assert(IMPLIES(bd < 12, intbufrange <= 16));
+  if (intbufrange > 16) {
+    conv_params.round_0 += intbufrange - 16;
+    conv_params.round_1 -= intbufrange - 16;
+  }
+  conv_params.dst = NULL;
+  conv_params.dst_stride = 0;
+  conv_params.plane = 0;
+  return conv_params;
+}
+
+void av1_highbd_convolve_2d_facade(const uint8_t *src8, int src_stride,
+                                   uint8_t *dst, int dst_stride, int w, int h,
+                                   const InterpFilterParams *interp_filters[2],
+                                   const int subpel_x_qn, int x_step_q4,
+                                   const int subpel_y_qn, int y_step_q4,
+                                   int scaled, ConvolveParams *conv_params,
+                                   int bd);
+
+// TODO(sarahparker) This will need to be integerized and optimized
+void av1_convolve_2d_sobel_y_c(const uint8_t *src, int src_stride, double *dst,
+                               int dst_stride, int w, int h, int dir,
+                               double norm);
+
+void av1_convolve_2d_sr_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                          int dst_stride, int w, int h,
+                          const InterpFilterParams *filter_params_x,
+                          const InterpFilterParams *filter_params_y,
+                          const int subpel_x_qn, const int subpel_y_qn,
+                          ConvolveParams *conv_params);
+
+void av1_convolve_x_sr_c(const uint8_t* src, int src_stride, uint8_t* dst,
+                        int dst_stride, int w, int h,
+                        const InterpFilterParams* filter_params_x,
+                        const int subpel_x_qn, ConvolveParams* conv_params);
+
+void av1_convolve_y_sr_c(const uint8_t *src, int src_stride, uint8_t *dst,
+                         int dst_stride, int w, int h,
+                         const InterpFilterParams *filter_params_y,
+                         const int subpel_y_qn);
+#ifdef __cplusplus
+}  // extern "C"
+#endif
+
+#endif  // AOM_AV1_COMMON_CONVOLVE_H_
diff --git a/Source/Lib/Common/ASM_NEON/convolve8_neon.c b/Source/Lib/Common/ASM_NEON/convolve8_neon.c
new file mode 100644
index 0000000..f4682eb
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/convolve8_neon.c
@@ -0,0 +1,137 @@
+/*
+  *  Copyright (c) 2020, Alliance for Open Media. All Rights Reserved.
+  *
+  *  Use of this source code is governed by a BSD-style license
+  *  that can be found in the LICENSE file in the root of the source
+  *  tree. An additional intellectual property rights grant can be found
+  *  in the file PATENTS.  All contributing project authors may
+  *  be found in the AUTHORS file in the root of the source tree.
+  */
+
+#include <assert.h>
+#include "convolve.h"
+#include <arm_neon.h>
+
+#include "convolve8_neon.h"
+#include "EbDefinitions.h"
+#include "common_dsp_rtcd.h"
+
+
+// Note: Fixed size intermediate buffers, place limits on parameters
+// of some functions. 2d filtering proceeds in 2 steps:
+//   (1) Interpolate horizontally into an intermediate buffer, temp.
+//   (2) Interpolate temp vertically to derive the sub-pixel result.
+// Deriving the maximum number of rows in the temp buffer (135):
+// --Smallest scaling factor is x1/2 ==> y_step_q4 = 32 (Normative).
+// --Largest block size is 128x128 pixels.
+// --128 rows in the downscaled frame span a distance of (128 - 1) * 32 in the
+//   original frame (in 1/16th pixel units).
+// --Must round-up because block may be located at sub-pixel position.
+// --Require an additional SUBPEL_TAPS rows for the 8-tap filter tails.
+// --((128 - 1) * 32 + 15) >> 4 + 8 = 263.
+#define WIENER_MAX_EXT_SIZE 263
+
+static INLINE int32_t horz_scalar_product(const uint8_t *a, const int16_t *b) {
+     int32_t sum = 0;
+     int16x8_t i_a = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(a)));
+     int16x8_t i_b = vld1q_s16(b);
+     int32x4_t low = vmull_s16(vget_low_s16(i_a),
+                               vget_low_s16(i_b));
+     int32x4_t high = vmull_s16(vget_high_s16(i_a),
+                               vget_high_s16(i_b));
+     sum = vaddvq_s32( low );
+     sum += vaddvq_s32( high );
+     return sum;
+ }
+
+static INLINE int vert_scalar_product(const uint8_t *a, ptrdiff_t a_stride, const int16_t *b) {
+    uint8_t r_a[8];
+    for (int k = 0; k < SUBPEL_TAPS; ++k) r_a[k] = a[k * a_stride];
+
+    return horz_scalar_product(&r_a[0], b);
+}
+
+static INLINE int32_t highbd_horz_scalar_product(const uint16_t *a, const int16_t *b) {
+    int32_t sum = 0;
+    for (int32_t k = 0; k < SUBPEL_TAPS; ++k) sum += a[k] * b[k];
+    return sum;
+}
+
+static INLINE int32_t highbd_vert_scalar_product(const uint16_t *a, ptrdiff_t a_stride,
+                                                 const int16_t *b) {
+    int32_t sum = 0;
+    for (int32_t k = 0; k < SUBPEL_TAPS; ++k) sum += a[k * a_stride] * b[k];
+    return sum;
+}
+
+static const InterpKernel *get_filter_base(const int16_t *filter) {
+    // NOTE: This assumes that the filter table is 256-byte aligned.
+    return (const InterpKernel *)(((intptr_t)filter) & ~((intptr_t)0xFF));
+}
+
+static int32_t get_filter_offset(const int16_t *f, const InterpKernel *base) {
+    return (int32_t)((const InterpKernel *)(intptr_t)f - base);
+}
+
+static void convolve_horiz(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+                           ptrdiff_t dst_stride, const InterpKernel *x_filters, int x0_q4,
+                           int x_step_q4, int w, int h) {
+    src -= SUBPEL_TAPS / 2 - 1;
+    for (int y = 0; y < h; ++y) {
+        int x_q4 = x0_q4;
+        for (int x = 0; x < w; ++x) {
+            const uint8_t *const src_x    = &src[x_q4 >> SUBPEL_BITS];
+            const int16_t *const x_filter = x_filters[x_q4 & SUBPEL_MASK];
+            const int            sum      = horz_scalar_product(src_x, x_filter);
+            dst[x]                        = clip_pixel(ROUND_POWER_OF_TWO(sum, FILTER_BITS));
+            x_q4 += x_step_q4;
+        }
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void convolve_vert(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+                          ptrdiff_t dst_stride, const InterpKernel *y_filters, int y0_q4,
+                          int y_step_q4, int w, int h) {
+    src -= src_stride * (SUBPEL_TAPS / 2 - 1);
+
+    for (int x = 0; x < w; ++x) {
+        int y_q4 = y0_q4;
+        for (int y = 0; y < h; ++y) {
+            const unsigned char *src_y    = &src[(y_q4 >> SUBPEL_BITS) * src_stride];
+            const int16_t *const y_filter = y_filters[y_q4 & SUBPEL_MASK];
+            const int            sum      = vert_scalar_product(src_y, src_stride, y_filter);
+            dst[y * dst_stride]           = clip_pixel(ROUND_POWER_OF_TWO(sum, FILTER_BITS));
+            y_q4 += y_step_q4;
+        }
+        ++src;
+        ++dst;
+    }
+}
+
+void svt_aom_convolve8_horiz_fallback_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+                               ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4,
+                               const int16_t *filter_y, int y_step_q4, int w, int h) {
+    const InterpKernel *const filters_x = get_filter_base(filter_x);
+    const int                 x0_q4     = get_filter_offset(filter_x, filters_x);
+
+    (void)filter_y;
+    (void)y_step_q4;
+
+    convolve_horiz(src, src_stride, dst, dst_stride, filters_x, x0_q4, x_step_q4, w, h);
+}
+
+void svt_aom_convolve8_vert_fallback_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+                              ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4,
+                              const int16_t *filter_y, int y_step_q4, int w, int h) {
+    const InterpKernel *const filters_y = get_filter_base(filter_y);
+    const int                 y0_q4     = get_filter_offset(filter_y, filters_y);
+
+    (void)filter_x;
+    (void)x_step_q4;
+
+    convolve_vert(src, src_stride, dst, dst_stride, filters_y, y0_q4, y_step_q4, w, h);
+}
+static INLINE const int16_t *av1_get_interp_filter_subpel_kernel(
+    const InterpFilterParams filter_params, const int32_t subpel);
diff --git a/Source/Lib/Common/ASM_NEON/convolve8_neon.h b/Source/Lib/Common/ASM_NEON/convolve8_neon.h
new file mode 100644
index 0000000..16623f9
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/convolve8_neon.h
@@ -0,0 +1,31 @@
+/*
+  *  Copyright (c) 2020, Alliance for Open Media. All Rights Reserved.
+  *
+  *  Use of this source code is governed by a BSD-style license
+  *  that can be found in the LICENSE file in the root of the source
+  *  tree. An additional intellectual property rights grant can be found
+  *  in the file PATENTS.  All contributing project authors may
+  *  be found in the AUTHORS file in the root of the source tree.
+  */
+
+#ifndef CONVOLVE_NEON_H
+#define CONVOLVE_NEON_H
+
+#include "EbDefinitions.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+void svt_aom_convolve8_horiz_fallback_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+                               ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4,
+                               const int16_t *filter_y, int y_step_q4, int w, int h);
+
+void svt_aom_convolve8_vert_fallback_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+                              ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4,
+                              const int16_t *filter_y, int y_step_q4, int w, int h);
+
+#ifdef __cplusplus
+}
+#endif
+#endif //CONVOLVE_NEON_H
diff --git a/Source/Lib/Common/ASM_NEON/convolve_neon.c b/Source/Lib/Common/ASM_NEON/convolve_neon.c
new file mode 100644
index 0000000..9f6626f
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/convolve_neon.c
@@ -0,0 +1,1858 @@
+
+/*
+ *
+ * Copyright (c) 2018, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <assert.h>
+#include "convolve.h"
+#include "filter.h"
+#include "mem_neon.h"
+#include <arm_neon.h>
+
+#include "convolve_neon.h"
+#include "transpose_neon.h"
+#include "EbDefinitions.h"
+#include "common_dsp_rtcd.h"
+
+static INLINE int16x4_t convolve8_4x4(const int16x4_t s0, const int16x4_t s1,
+                                      const int16x4_t s2, const int16x4_t s3,
+                                      const int16x4_t s4, const int16x4_t s5,
+                                      const int16x4_t s6, const int16x4_t s7,
+                                      const int16_t *filter) {
+  int16x4_t sum;
+
+  sum = vmul_n_s16(s0, filter[0]);
+  sum = vmla_n_s16(sum, s1, filter[1]);
+  sum = vmla_n_s16(sum, s2, filter[2]);
+  sum = vmla_n_s16(sum, s5, filter[5]);
+  sum = vmla_n_s16(sum, s6, filter[6]);
+  sum = vmla_n_s16(sum, s7, filter[7]);
+  /* filter[3] can take a max value of 128. So the max value of the result :
+   * 128*255 + sum > 16 bits
+   */
+  sum = vqadd_s16(sum, vmul_n_s16(s3, filter[3]));
+  sum = vqadd_s16(sum, vmul_n_s16(s4, filter[4]));
+
+  return sum;
+}
+
+
+static INLINE uint8x8_t convolve8_horiz_8x8(
+    const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
+    const int16x8_t s3, const int16x8_t s4, const int16x8_t s5,
+    const int16x8_t s6, const int16x8_t s7, const int16_t *filter,
+    const int16x8_t shift_round_0, const int16x8_t shift_by_bits) {
+  int16x8_t sum;
+
+  sum = vmulq_n_s16(s0, filter[0]);
+  sum = vmlaq_n_s16(sum, s1, filter[1]);
+  sum = vmlaq_n_s16(sum, s2, filter[2]);
+  sum = vmlaq_n_s16(sum, s5, filter[5]);
+  sum = vmlaq_n_s16(sum, s6, filter[6]);
+  sum = vmlaq_n_s16(sum, s7, filter[7]);
+  /* filter[3] can take a max value of 128. So the max value of the result :
+   * 128*255 + sum > 16 bits
+   */
+  sum = vqaddq_s16(sum, vmulq_n_s16(s3, filter[3]));
+  sum = vqaddq_s16(sum, vmulq_n_s16(s4, filter[4]));
+
+  sum = vqrshlq_s16(sum, shift_round_0);
+  sum = vqrshlq_s16(sum, shift_by_bits);
+
+  return vqmovun_s16(sum);
+}
+
+#if !defined(__aarch64__)
+static INLINE uint8x8_t convolve8_horiz_4x1(
+    const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
+    const int16x4_t s3, const int16x4_t s4, const int16x4_t s5,
+    const int16x4_t s6, const int16x4_t s7, const int16_t *filter,
+    const int16x4_t shift_round_0, const int16x4_t shift_by_bits) {
+  int16x4_t sum;
+
+  sum = vmul_n_s16(s0, filter[0]);
+  sum = vmla_n_s16(sum, s1, filter[1]);
+  sum = vmla_n_s16(sum, s2, filter[2]);
+  sum = vmla_n_s16(sum, s5, filter[5]);
+  sum = vmla_n_s16(sum, s6, filter[6]);
+  sum = vmla_n_s16(sum, s7, filter[7]);
+  /* filter[3] can take a max value of 128. So the max value of the result :
+   * 128*255 + sum > 16 bits
+   */
+  sum = vqadd_s16(sum, vmul_n_s16(s3, filter[3]));
+  sum = vqadd_s16(sum, vmul_n_s16(s4, filter[4]));
+
+  sum = vqrshl_s16(sum, shift_round_0);
+  sum = vqrshl_s16(sum, shift_by_bits);
+
+  return vqmovun_s16(vcombine_s16(sum, sum));
+}
+#endif  // !defined(__arch64__)
+
+static INLINE uint8x8_t convolve8_vert_8x4(
+    const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
+    const int16x8_t s3, const int16x8_t s4, const int16x8_t s5,
+    const int16x8_t s6, const int16x8_t s7, const int16_t *filter) {
+  int16x8_t sum;
+
+  sum = vmulq_n_s16(s0, filter[0]);
+  sum = vmlaq_n_s16(sum, s1, filter[1]);
+  sum = vmlaq_n_s16(sum, s2, filter[2]);
+  sum = vmlaq_n_s16(sum, s5, filter[5]);
+  sum = vmlaq_n_s16(sum, s6, filter[6]);
+  sum = vmlaq_n_s16(sum, s7, filter[7]);
+  /* filter[3] can take a max value of 128. So the max value of the result :
+   * 128*255 + sum > 16 bits
+   */
+  sum = vqaddq_s16(sum, vmulq_n_s16(s3, filter[3]));
+  sum = vqaddq_s16(sum, vmulq_n_s16(s4, filter[4]));
+
+  return vqrshrun_n_s16(sum, FILTER_BITS);
+}
+
+static INLINE uint16x4_t convolve8_vert_4x4_s32(
+    const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
+    const int16x4_t s3, const int16x4_t s4, const int16x4_t s5,
+    const int16x4_t s6, const int16x4_t s7, const int16_t *y_filter,
+    const int32x4_t round_shift_vec, const int32x4_t offset_const,
+    const int32x4_t sub_const_vec) {
+  int32x4_t sum0;
+  uint16x4_t res;
+  const int32x4_t zero = vdupq_n_s32(0);
+
+  sum0 = vmull_n_s16(s0, y_filter[0]);
+  sum0 = vmlal_n_s16(sum0, s1, y_filter[1]);
+  sum0 = vmlal_n_s16(sum0, s2, y_filter[2]);
+  sum0 = vmlal_n_s16(sum0, s3, y_filter[3]);
+  sum0 = vmlal_n_s16(sum0, s4, y_filter[4]);
+  sum0 = vmlal_n_s16(sum0, s5, y_filter[5]);
+  sum0 = vmlal_n_s16(sum0, s6, y_filter[6]);
+  sum0 = vmlal_n_s16(sum0, s7, y_filter[7]);
+
+  sum0 = vaddq_s32(sum0, offset_const);
+  sum0 = vqrshlq_s32(sum0, round_shift_vec);
+  sum0 = vsubq_s32(sum0, sub_const_vec);
+  sum0 = vmaxq_s32(sum0, zero);
+
+  res = vmovn_u32(vreinterpretq_u32_s32(sum0));
+
+  return res;
+}
+
+static INLINE uint8x8_t convolve8_vert_8x4_s32(
+    const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
+    const int16x8_t s3, const int16x8_t s4, const int16x8_t s5,
+    const int16x8_t s6, const int16x8_t s7, const int16_t *y_filter,
+    const int32x4_t round_shift_vec, const int32x4_t offset_const,
+    const int32x4_t sub_const_vec, const int16x8_t vec_round_bits) {
+  int32x4_t sum0, sum1;
+  uint16x8_t res;
+  const int32x4_t zero = vdupq_n_s32(0);
+
+  sum0 = vmull_n_s16(vget_low_s16(s0), y_filter[0]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(s1), y_filter[1]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(s2), y_filter[2]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(s3), y_filter[3]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(s4), y_filter[4]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(s5), y_filter[5]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(s6), y_filter[6]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(s7), y_filter[7]);
+
+  sum1 = vmull_n_s16(vget_high_s16(s0), y_filter[0]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(s1), y_filter[1]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(s2), y_filter[2]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(s3), y_filter[3]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(s4), y_filter[4]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(s5), y_filter[5]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(s6), y_filter[6]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(s7), y_filter[7]);
+
+  sum0 = vaddq_s32(sum0, offset_const);
+  sum1 = vaddq_s32(sum1, offset_const);
+  sum0 = vqrshlq_s32(sum0, round_shift_vec);
+  sum1 = vqrshlq_s32(sum1, round_shift_vec);
+  sum0 = vsubq_s32(sum0, sub_const_vec);
+  sum1 = vsubq_s32(sum1, sub_const_vec);
+  sum0 = vmaxq_s32(sum0, zero);
+  sum1 = vmaxq_s32(sum1, zero);
+  res = vcombine_u16(vqmovn_u32(vreinterpretq_u32_s32(sum0)),
+                     vqmovn_u32(vreinterpretq_u32_s32(sum1)));
+
+  res = vqrshlq_u16(res, vec_round_bits);
+
+  return vqmovn_u16(res);
+}
+
+void svt_av1_convolve_x_sr_neon(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride,
+                                int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y,
+                                const int32_t subpel_x_qn, const int32_t subpel_y_qn, ConvolveParams *conv_params) {
+  if (filter_params_x->taps > 8) {
+    av1_convolve_x_sr_c(src, src_stride, dst, dst_stride, w, h, filter_params_x,
+                        subpel_x_qn, conv_params);
+    return;
+  }
+  const uint8_t horiz_offset = filter_params_x->taps / 2 - 1;
+  const int8_t bits = FILTER_BITS - conv_params->round_0;
+
+  uint8x8_t t0;
+#if defined(__aarch64__)
+  uint8x8_t t1, t2, t3;
+#endif
+
+  assert(bits >= 0);
+  assert((FILTER_BITS - conv_params->round_1) >= 0 ||
+         ((conv_params->round_0 + conv_params->round_1) == 2 * FILTER_BITS));
+
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_x, subpel_x_qn & SUBPEL_MASK);
+
+  const int16x8_t shift_round_0 = vdupq_n_s16(-conv_params->round_0);
+  const int16x8_t shift_by_bits = vdupq_n_s16(-bits);
+
+  src -= horiz_offset;
+#if defined(__aarch64__)
+  if (h == 4) {
+    uint8x8_t d01, d23;
+    int16x4_t s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, d0, d1, d2, d3;
+    int16x8_t d01_temp, d23_temp;
+
+    __builtin_prefetch(src + 0 * src_stride);
+    __builtin_prefetch(src + 1 * src_stride);
+    __builtin_prefetch(src + 2 * src_stride);
+    __builtin_prefetch(src + 3 * src_stride);
+
+    load_u8_8x4(src, src_stride, &t0, &t1, &t2, &t3);
+    transpose_u8_8x4(&t0, &t1, &t2, &t3);
+
+    s0 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+    s1 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+    s2 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+    s3 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t3)));
+    s4 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+    s5 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+    s6 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+    __builtin_prefetch(dst + 0 * dst_stride);
+    __builtin_prefetch(dst + 1 * dst_stride);
+    __builtin_prefetch(dst + 2 * dst_stride);
+    __builtin_prefetch(dst + 3 * dst_stride);
+    src += 7;
+
+    do {
+      load_u8_8x4(src, src_stride, &t0, &t1, &t2, &t3);
+      transpose_u8_8x4(&t0, &t1, &t2, &t3);
+
+      s7 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+      s8 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+      s9 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+      s10 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t3)));
+
+      d0 = convolve8_4x4(s0, s1, s2, s3, s4, s5, s6, s7, x_filter);
+
+      d1 = convolve8_4x4(s1, s2, s3, s4, s5, s6, s7, s8, x_filter);
+
+      d2 = convolve8_4x4(s2, s3, s4, s5, s6, s7, s8, s9, x_filter);
+
+      d3 = convolve8_4x4(s3, s4, s5, s6, s7, s8, s9, s10, x_filter);
+
+      d01_temp = vqrshlq_s16(vcombine_s16(d0, d1), shift_round_0);
+      d23_temp = vqrshlq_s16(vcombine_s16(d2, d3), shift_round_0);
+
+      d01_temp = vqrshlq_s16(d01_temp, shift_by_bits);
+      d23_temp = vqrshlq_s16(d23_temp, shift_by_bits);
+
+      d01 = vqmovun_s16(d01_temp);
+      d23 = vqmovun_s16(d23_temp);
+
+      transpose_u8_4x4(&d01, &d23);
+
+      if (w != 2) {
+        vst1_lane_u32((uint32_t *)(dst + 0 * dst_stride),  // 00 01 02 03
+                      vreinterpret_u32_u8(d01), 0);
+        vst1_lane_u32((uint32_t *)(dst + 1 * dst_stride),  // 10 11 12 13
+                      vreinterpret_u32_u8(d23), 0);
+        vst1_lane_u32((uint32_t *)(dst + 2 * dst_stride),  // 20 21 22 23
+                      vreinterpret_u32_u8(d01), 1);
+        vst1_lane_u32((uint32_t *)(dst + 3 * dst_stride),  // 30 31 32 33
+                      vreinterpret_u32_u8(d23), 1);
+      } else {
+        vst1_lane_u16((uint16_t *)(dst + 0 * dst_stride),  // 00 01
+                      vreinterpret_u16_u8(d01), 0);
+        vst1_lane_u16((uint16_t *)(dst + 1 * dst_stride),  // 10 11
+                      vreinterpret_u16_u8(d23), 0);
+        vst1_lane_u16((uint16_t *)(dst + 2 * dst_stride),  // 20 21
+                      vreinterpret_u16_u8(d01), 2);
+        vst1_lane_u16((uint16_t *)(dst + 3 * dst_stride),  // 30 31
+                      vreinterpret_u16_u8(d23), 2);
+      }
+
+      s0 = s4;
+      s1 = s5;
+      s2 = s6;
+      s3 = s7;
+      s4 = s8;
+      s5 = s9;
+      s6 = s10;
+      src += 4;
+      dst += 4;
+      w -= 4;
+    } while (w > 0);
+  } else {
+#endif
+    int width;
+    const uint8_t *s;
+    int16x8_t s0, s1, s2, s3, s4, s5, s6, s7;
+
+#if defined(__aarch64__)
+    int16x8_t s8, s9, s10;
+    uint8x8_t t4, t5, t6, t7;
+#endif
+
+    if (w <= 4) {
+#if defined(__aarch64__)
+      do {
+        load_u8_8x8(src, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+        transpose_u8_8x8(&t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+        s0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+        s1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+        s2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+        s3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+        s4 = vreinterpretq_s16_u16(vmovl_u8(t4));
+        s5 = vreinterpretq_s16_u16(vmovl_u8(t5));
+        s6 = vreinterpretq_s16_u16(vmovl_u8(t6));
+
+        load_u8_8x8(src + 7, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6,
+                    &t7);
+        src += 8 * src_stride;
+        __builtin_prefetch(dst + 0 * dst_stride);
+        __builtin_prefetch(dst + 1 * dst_stride);
+        __builtin_prefetch(dst + 2 * dst_stride);
+        __builtin_prefetch(dst + 3 * dst_stride);
+        __builtin_prefetch(dst + 4 * dst_stride);
+        __builtin_prefetch(dst + 5 * dst_stride);
+        __builtin_prefetch(dst + 6 * dst_stride);
+        __builtin_prefetch(dst + 7 * dst_stride);
+
+        transpose_u8_4x8(&t0, &t1, &t2, &t3, t4, t5, t6, t7);
+
+        s7 = vreinterpretq_s16_u16(vmovl_u8(t0));
+        s8 = vreinterpretq_s16_u16(vmovl_u8(t1));
+        s9 = vreinterpretq_s16_u16(vmovl_u8(t2));
+        s10 = vreinterpretq_s16_u16(vmovl_u8(t3));
+
+        __builtin_prefetch(src + 0 * src_stride);
+        __builtin_prefetch(src + 1 * src_stride);
+        __builtin_prefetch(src + 2 * src_stride);
+        __builtin_prefetch(src + 3 * src_stride);
+        __builtin_prefetch(src + 4 * src_stride);
+        __builtin_prefetch(src + 5 * src_stride);
+        __builtin_prefetch(src + 6 * src_stride);
+        __builtin_prefetch(src + 7 * src_stride);
+        t0 = convolve8_horiz_8x8(s0, s1, s2, s3, s4, s5, s6, s7, x_filter,
+                                 shift_round_0, shift_by_bits);
+        t1 = convolve8_horiz_8x8(s1, s2, s3, s4, s5, s6, s7, s8, x_filter,
+                                 shift_round_0, shift_by_bits);
+        t2 = convolve8_horiz_8x8(s2, s3, s4, s5, s6, s7, s8, s9, x_filter,
+                                 shift_round_0, shift_by_bits);
+        t3 = convolve8_horiz_8x8(s3, s4, s5, s6, s7, s8, s9, s10, x_filter,
+                                 shift_round_0, shift_by_bits);
+
+        transpose_u8_8x4(&t0, &t1, &t2, &t3);
+
+        if ((w == 4) && (h > 4)) {
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t0),
+                        0);  // 00 01 02 03
+          dst += dst_stride;
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t1),
+                        0);  // 10 11 12 13
+          dst += dst_stride;
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t2),
+                        0);  // 20 21 22 23
+          dst += dst_stride;
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t3),
+                        0);  // 30 31 32 33
+          dst += dst_stride;
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t0),
+                        1);  // 40 41 42 43
+          dst += dst_stride;
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t1),
+                        1);  // 50 51 52 53
+          dst += dst_stride;
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t2),
+                        1);  // 60 61 62 63
+          dst += dst_stride;
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t3),
+                        1);  // 70 71 72 73
+          dst += dst_stride;
+        } else if ((w == 4) && (h == 2)) {
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t0),
+                        0);  // 00 01 02 03
+          dst += dst_stride;
+          vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t1),
+                        0);  // 10 11 12 13
+          dst += dst_stride;
+        } else if ((w == 2) && (h > 4)) {
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t0),
+                        0);  // 00 01
+          dst += dst_stride;
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t1),
+                        0);  // 10 11
+          dst += dst_stride;
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t2),
+                        0);  // 20 21
+          dst += dst_stride;
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t3),
+                        0);  // 30 31
+          dst += dst_stride;
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t0),
+                        2);  // 40 41
+          dst += dst_stride;
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t1),
+                        2);  // 50 51
+          dst += dst_stride;
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t2),
+                        2);  // 60 61
+          dst += dst_stride;
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t3),
+                        2);  // 70 71
+          dst += dst_stride;
+        } else if ((w == 2) && (h == 2)) {
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t0),
+                        0);  // 00 01
+          dst += dst_stride;
+          vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t1),
+                        0);  // 10 11
+          dst += dst_stride;
+        }
+        h -= 8;
+      } while (h > 0);
+#else
+    int16x8_t tt0;
+    int16x4_t x0, x1, x2, x3, x4, x5, x6, x7;
+    const int16x4_t shift_round_0_low = vget_low_s16(shift_round_0);
+    const int16x4_t shift_by_bits_low = vget_low_s16(shift_by_bits);
+    do {
+      t0 = vld1_u8(src);  // a0 a1 a2 a3 a4 a5 a6 a7
+      tt0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+      x0 = vget_low_s16(tt0);   // a0 a1 a2 a3
+      x4 = vget_high_s16(tt0);  // a4 a5 a6 a7
+
+      t0 = vld1_u8(src + 8);  // a8 a9 a10 a11 a12 a13 a14 a15
+      tt0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+      x7 = vget_low_s16(tt0);  // a8 a9 a10 a11
+
+      x1 = vext_s16(x0, x4, 1);  // a1 a2 a3 a4
+      x2 = vext_s16(x0, x4, 2);  // a2 a3 a4 a5
+      x3 = vext_s16(x0, x4, 3);  // a3 a4 a5 a6
+      x5 = vext_s16(x4, x7, 1);  // a5 a6 a7 a8
+      x6 = vext_s16(x4, x7, 2);  // a6 a7 a8 a9
+      x7 = vext_s16(x4, x7, 3);  // a7 a8 a9 a10
+
+      src += src_stride;
+
+      t0 = convolve8_horiz_4x1(x0, x1, x2, x3, x4, x5, x6, x7, x_filter,
+                               shift_round_0_low, shift_by_bits_low);
+
+      if (w == 4) {
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(t0),
+                      0);  // 00 01 02 03
+        dst += dst_stride;
+      } else if (w == 2) {
+        vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(t0), 0);  // 00 01
+        dst += dst_stride;
+      }
+      h -= 1;
+    } while (h > 0);
+#endif
+    } else {
+      uint8_t *d;
+      int16x8_t s11;
+#if defined(__aarch64__)
+      int16x8_t s12, s13, s14;
+      do {
+        __builtin_prefetch(src + 0 * src_stride);
+        __builtin_prefetch(src + 1 * src_stride);
+        __builtin_prefetch(src + 2 * src_stride);
+        __builtin_prefetch(src + 3 * src_stride);
+        __builtin_prefetch(src + 4 * src_stride);
+        __builtin_prefetch(src + 5 * src_stride);
+        __builtin_prefetch(src + 6 * src_stride);
+        __builtin_prefetch(src + 7 * src_stride);
+        load_u8_8x8(src, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+        transpose_u8_8x8(&t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+        s0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+        s1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+        s2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+        s3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+        s4 = vreinterpretq_s16_u16(vmovl_u8(t4));
+        s5 = vreinterpretq_s16_u16(vmovl_u8(t5));
+        s6 = vreinterpretq_s16_u16(vmovl_u8(t6));
+
+        width = w;
+        s = src + 7;
+        d = dst;
+        __builtin_prefetch(dst + 0 * dst_stride);
+        __builtin_prefetch(dst + 1 * dst_stride);
+        __builtin_prefetch(dst + 2 * dst_stride);
+        __builtin_prefetch(dst + 3 * dst_stride);
+        __builtin_prefetch(dst + 4 * dst_stride);
+        __builtin_prefetch(dst + 5 * dst_stride);
+        __builtin_prefetch(dst + 6 * dst_stride);
+        __builtin_prefetch(dst + 7 * dst_stride);
+
+        do {
+          load_u8_8x8(s, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+          transpose_u8_8x8(&t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+          s7 = vreinterpretq_s16_u16(vmovl_u8(t0));
+          s8 = vreinterpretq_s16_u16(vmovl_u8(t1));
+          s9 = vreinterpretq_s16_u16(vmovl_u8(t2));
+          s10 = vreinterpretq_s16_u16(vmovl_u8(t3));
+          s11 = vreinterpretq_s16_u16(vmovl_u8(t4));
+          s12 = vreinterpretq_s16_u16(vmovl_u8(t5));
+          s13 = vreinterpretq_s16_u16(vmovl_u8(t6));
+          s14 = vreinterpretq_s16_u16(vmovl_u8(t7));
+
+          t0 = convolve8_horiz_8x8(s0, s1, s2, s3, s4, s5, s6, s7, x_filter,
+                                   shift_round_0, shift_by_bits);
+
+          t1 = convolve8_horiz_8x8(s1, s2, s3, s4, s5, s6, s7, s8, x_filter,
+                                   shift_round_0, shift_by_bits);
+
+          t2 = convolve8_horiz_8x8(s2, s3, s4, s5, s6, s7, s8, s9, x_filter,
+                                   shift_round_0, shift_by_bits);
+
+          t3 = convolve8_horiz_8x8(s3, s4, s5, s6, s7, s8, s9, s10, x_filter,
+                                   shift_round_0, shift_by_bits);
+
+          t4 = convolve8_horiz_8x8(s4, s5, s6, s7, s8, s9, s10, s11, x_filter,
+                                   shift_round_0, shift_by_bits);
+
+          t5 = convolve8_horiz_8x8(s5, s6, s7, s8, s9, s10, s11, s12, x_filter,
+                                   shift_round_0, shift_by_bits);
+
+          t6 = convolve8_horiz_8x8(s6, s7, s8, s9, s10, s11, s12, s13, x_filter,
+                                   shift_round_0, shift_by_bits);
+
+          t7 = convolve8_horiz_8x8(s7, s8, s9, s10, s11, s12, s13, s14,
+                                   x_filter, shift_round_0, shift_by_bits);
+
+          transpose_u8_8x8(&t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+          if (h != 2) {
+            store_u8_8x8(d, dst_stride, t0, t1, t2, t3, t4, t5, t6, t7);
+          } else {
+            store_row2_u8_8x8(d, dst_stride, t0, t1);
+          }
+          s0 = s8;
+          s1 = s9;
+          s2 = s10;
+          s3 = s11;
+          s4 = s12;
+          s5 = s13;
+          s6 = s14;
+          s += 8;
+          d += 8;
+          width -= 8;
+        } while (width > 0);
+        src += 8 * src_stride;
+        dst += 8 * dst_stride;
+        h -= 8;
+      } while (h > 0);
+#else
+    do {
+      t0 = vld1_u8(src);  // a0 a1 a2 a3 a4 a5 a6 a7
+      s0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+
+      width = w;
+      s = src + 8;
+      d = dst;
+      __builtin_prefetch(dst);
+
+      do {
+        t0 = vld1_u8(s);  // a8 a9 a10 a11 a12 a13 a14 a15
+        s7 = vreinterpretq_s16_u16(vmovl_u8(t0));
+        s11 = s0;
+        s0 = s7;
+
+        s1 = vextq_s16(s11, s7, 1);  // a1 a2 a3 a4 a5 a6 a7 a8
+        s2 = vextq_s16(s11, s7, 2);  // a2 a3 a4 a5 a6 a7 a8 a9
+        s3 = vextq_s16(s11, s7, 3);  // a3 a4 a5 a6 a7 a8 a9 a10
+        s4 = vextq_s16(s11, s7, 4);  // a4 a5 a6 a7 a8 a9 a10 a11
+        s5 = vextq_s16(s11, s7, 5);  // a5 a6 a7 a8 a9 a10 a11 a12
+        s6 = vextq_s16(s11, s7, 6);  // a6 a7 a8 a9 a10 a11 a12 a13
+        s7 = vextq_s16(s11, s7, 7);  // a7 a8 a9 a10 a11 a12 a13 a14
+
+        t0 = convolve8_horiz_8x8(s11, s1, s2, s3, s4, s5, s6, s7, x_filter,
+                                 shift_round_0, shift_by_bits);
+        vst1_u8(d, t0);
+
+        s += 8;
+        d += 8;
+        width -= 8;
+      } while (width > 0);
+      src += src_stride;
+      dst += dst_stride;
+      h -= 1;
+    } while (h > 0);
+#endif
+    }
+#if defined(__aarch64__)
+  }
+#endif
+}
+
+void svt_av1_convolve_y_sr_neon(const uint8_t *src, int32_t src_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                int32_t w, int32_t h,
+                                InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y,
+                                const int32_t subpel_x_qn, const int32_t subpel_y_qn, ConvolveParams *conv_params){
+  if (filter_params_y->taps > 8) {
+    av1_convolve_y_sr_c(src, src_stride, dst, dst_stride, w, h, filter_params_y,
+                        subpel_y_qn);
+    return;
+  }
+  const int vert_offset = filter_params_y->taps / 2 - 1;
+
+  src -= vert_offset * src_stride;
+
+  const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_y, subpel_y_qn & SUBPEL_MASK);
+
+  if (w <= 4) {
+    uint8x8_t d01;
+    int16x4_t s0, s1, s2, s3, s4, s5, s6, s7, d0;
+#if defined(__aarch64__)
+    uint8x8_t d23;
+    int16x4_t s8, s9, s10, d1, d2, d3;
+#endif
+    s0 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+    src += src_stride;
+    s1 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+    src += src_stride;
+    s2 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+    src += src_stride;
+    s3 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+    src += src_stride;
+    s4 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+    src += src_stride;
+    s5 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+    src += src_stride;
+    s6 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+    src += src_stride;
+
+    do {
+      s7 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+      src += src_stride;
+#if defined(__aarch64__)
+      s8 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+      src += src_stride;
+      s9 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+      src += src_stride;
+      s10 = vreinterpret_s16_u16(vget_low_u16(vmovl_u8(vld1_u8(src))));
+      src += src_stride;
+
+      __builtin_prefetch(dst + 0 * dst_stride);
+      __builtin_prefetch(dst + 1 * dst_stride);
+      __builtin_prefetch(dst + 2 * dst_stride);
+      __builtin_prefetch(dst + 3 * dst_stride);
+      __builtin_prefetch(src + 0 * src_stride);
+      __builtin_prefetch(src + 1 * src_stride);
+      __builtin_prefetch(src + 2 * src_stride);
+      __builtin_prefetch(src + 3 * src_stride);
+      d0 = convolve8_4x4(s0, s1, s2, s3, s4, s5, s6, s7, y_filter);
+      d1 = convolve8_4x4(s1, s2, s3, s4, s5, s6, s7, s8, y_filter);
+      d2 = convolve8_4x4(s2, s3, s4, s5, s6, s7, s8, s9, y_filter);
+      d3 = convolve8_4x4(s3, s4, s5, s6, s7, s8, s9, s10, y_filter);
+
+      d01 = vqrshrun_n_s16(vcombine_s16(d0, d1), FILTER_BITS);
+      d23 = vqrshrun_n_s16(vcombine_s16(d2, d3), FILTER_BITS);
+      if ((w == 4) && (h != 2)) {
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(d01),
+                      0);  // 00 01 02 03
+        dst += dst_stride;
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(d01),
+                      1);  // 10 11 12 13
+        dst += dst_stride;
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(d23),
+                      0);  // 20 21 22 23
+        dst += dst_stride;
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(d23),
+                      1);  // 30 31 32 33
+        dst += dst_stride;
+      } else if ((w == 4) && (h == 2)) {
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(d01),
+                      0);  // 00 01 02 03
+        dst += dst_stride;
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(d01),
+                      1);  // 10 11 12 13
+        dst += dst_stride;
+      } else if ((w == 2) && (h != 2)) {
+        vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(d01), 0);  // 00 01
+        dst += dst_stride;
+        vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(d01), 2);  // 10 11
+        dst += dst_stride;
+        vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(d23), 0);  // 20 21
+        dst += dst_stride;
+        vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(d23), 2);  // 30 31
+        dst += dst_stride;
+      } else if ((w == 2) && (h == 2)) {
+        vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(d01), 0);  // 00 01
+        dst += dst_stride;
+        vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(d01), 2);  // 10 11
+        dst += dst_stride;
+      }
+      s0 = s4;
+      s1 = s5;
+      s2 = s6;
+      s3 = s7;
+      s4 = s8;
+      s5 = s9;
+      s6 = s10;
+      h -= 4;
+#else
+      __builtin_prefetch(dst + 0 * dst_stride);
+      __builtin_prefetch(src + 0 * src_stride);
+
+      d0 = convolve8_4x4(s0, s1, s2, s3, s4, s5, s6, s7, y_filter);
+
+      d01 = vqrshrun_n_s16(vcombine_s16(d0, d0), FILTER_BITS);
+
+      if (w == 4) {
+        vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(d01), 0);
+        dst += dst_stride;
+      } else if (w == 2) {
+        vst1_lane_u16((uint16_t *)dst, vreinterpret_u16_u8(d01), 0);
+        dst += dst_stride;
+      }
+      s0 = s1;
+      s1 = s2;
+      s2 = s3;
+      s3 = s4;
+      s4 = s5;
+      s5 = s6;
+      s6 = s7;
+      h -= 1;
+#endif
+    } while (h > 0);
+  } else {
+    int height;
+    const uint8_t *s;
+    uint8_t *d;
+    uint8x8_t t0;
+    int16x8_t s0, s1, s2, s3, s4, s5, s6, s7;
+#if defined(__aarch64__)
+    uint8x8_t t1, t2, t3;
+    int16x8_t s8, s9, s10;
+#endif
+    do {
+      __builtin_prefetch(src + 0 * src_stride);
+      __builtin_prefetch(src + 1 * src_stride);
+      __builtin_prefetch(src + 2 * src_stride);
+      __builtin_prefetch(src + 3 * src_stride);
+      __builtin_prefetch(src + 4 * src_stride);
+      __builtin_prefetch(src + 5 * src_stride);
+      __builtin_prefetch(src + 6 * src_stride);
+      s = src;
+      s0 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+      s += src_stride;
+      s1 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+      s += src_stride;
+      s2 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+      s += src_stride;
+      s3 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+      s += src_stride;
+      s4 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+      s += src_stride;
+      s5 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+      s += src_stride;
+      s6 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+      s += src_stride;
+      d = dst;
+      height = h;
+
+      do {
+        s7 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+        s += src_stride;
+#if defined(__aarch64__)
+        s8 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+        s += src_stride;
+        s9 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+        s += src_stride;
+        s10 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(s)));
+        s += src_stride;
+
+        __builtin_prefetch(d + 0 * dst_stride);
+        __builtin_prefetch(d + 1 * dst_stride);
+        __builtin_prefetch(d + 2 * dst_stride);
+        __builtin_prefetch(d + 3 * dst_stride);
+        __builtin_prefetch(s + 0 * src_stride);
+        __builtin_prefetch(s + 1 * src_stride);
+        __builtin_prefetch(s + 2 * src_stride);
+        __builtin_prefetch(s + 3 * src_stride);
+        t0 = convolve8_vert_8x4(s0, s1, s2, s3, s4, s5, s6, s7, y_filter);
+        t1 = convolve8_vert_8x4(s1, s2, s3, s4, s5, s6, s7, s8, y_filter);
+        t2 = convolve8_vert_8x4(s2, s3, s4, s5, s6, s7, s8, s9, y_filter);
+        t3 = convolve8_vert_8x4(s3, s4, s5, s6, s7, s8, s9, s10, y_filter);
+        if (h != 2) {
+          vst1_u8(d, t0);
+          d += dst_stride;
+          vst1_u8(d, t1);
+          d += dst_stride;
+          vst1_u8(d, t2);
+          d += dst_stride;
+          vst1_u8(d, t3);
+          d += dst_stride;
+        } else {
+          vst1_u8(d, t0);
+          d += dst_stride;
+          vst1_u8(d, t1);
+          d += dst_stride;
+        }
+        s0 = s4;
+        s1 = s5;
+        s2 = s6;
+        s3 = s7;
+        s4 = s8;
+        s5 = s9;
+        s6 = s10;
+        height -= 4;
+#else
+        __builtin_prefetch(d);
+        __builtin_prefetch(s);
+
+        t0 = convolve8_vert_8x4(s0, s1, s2, s3, s4, s5, s6, s7, y_filter);
+
+        vst1_u8(d, t0);
+        d += dst_stride;
+
+        s0 = s1;
+        s1 = s2;
+        s2 = s3;
+        s3 = s4;
+        s4 = s5;
+        s5 = s6;
+        s6 = s7;
+        height -= 1;
+#endif
+      } while (height > 0);
+      src += 8;
+      dst += 8;
+      w -= 8;
+    } while (w > 0);
+  }
+}
+
+// // Horizontal filtering for convolve_2d_sr for width multiple of 8
+// // Processes one row at a time
+static INLINE void horiz_filter_w8_single_row(
+    const uint8_t *src_ptr, int src_stride, int16_t *dst_ptr,
+    const int dst_stride, int width, int height, const int16_t *x_filter,
+    const int16x8_t horiz_const, const int16x8_t shift_round_0) {
+  int16x8_t s0, s1, s2, s3, s4, s5, s6, s7;
+  do {
+    uint8x8_t t0 = vld1_u8(src_ptr);
+    s0 = vreinterpretq_s16_u16(vmovl_u8(t0));  // a0 a1 a2 a3 a4 a5 a6 a7
+
+    int width_tmp = width;
+    const uint8_t *s = src_ptr + 8;
+    int16_t *dst_tmp = dst_ptr;
+
+    __builtin_prefetch(dst_ptr);
+
+    do {
+      t0 = vld1_u8(s);  // a8 a9 a10 a11 a12 a13 a14 a15
+      s7 = vreinterpretq_s16_u16(vmovl_u8(t0));
+      int16x8_t sum = s0;
+      s0 = s7;
+
+      s1 = vextq_s16(sum, s7, 1);  // a1 a2 a3 a4 a5 a6 a7 a8
+      s2 = vextq_s16(sum, s7, 2);  // a2 a3 a4 a5 a6 a7 a8 a9
+      s3 = vextq_s16(sum, s7, 3);  // a3 a4 a5 a6 a7 a8 a9 a10
+      s4 = vextq_s16(sum, s7, 4);  // a4 a5 a6 a7 a8 a9 a10 a11
+      s5 = vextq_s16(sum, s7, 5);  // a5 a6 a7 a8 a9 a10 a11 a12
+      s6 = vextq_s16(sum, s7, 6);  // a6 a7 a8 a9 a10 a11 a12 a13
+      s7 = vextq_s16(sum, s7, 7);  // a7 a8 a9 a10 a11 a12 a13 a14
+
+      int16x8_t res0 = convolve8_8x8_s16(sum, s1, s2, s3, s4, s5, s6, s7,
+                                         x_filter, horiz_const, shift_round_0);
+
+      vst1q_s16(dst_tmp, res0);
+
+      s += 8;
+      dst_tmp += 8;
+      width_tmp -= 8;
+    } while (width_tmp > 0);
+    src_ptr += src_stride;
+    dst_ptr += dst_stride;
+    height--;
+  } while (height > 0);
+}
+
+// Horizontal filtering for convolve_2d_sr for width <= 4
+// Processes one row at a time
+static INLINE void horiz_filter_w4_single_row(
+    const uint8_t *src_ptr, int src_stride, int16_t *dst_ptr,
+    const int dst_stride, int width, int height, const int16_t *x_filter,
+    const int16x4_t horiz_const, const int16x4_t shift_round_0) {
+  int16x4_t s0, s1, s2, s3, s4, s5, s6, s7;
+  do {
+    const uint8_t *s = src_ptr;
+
+    __builtin_prefetch(s);
+
+    uint8x8_t t0 = vld1_u8(s);  // a0 a1 a2 a3 a4 a5 a6 a7
+    int16x8_t tt0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+    s0 = vget_low_s16(tt0);
+    s4 = vget_high_s16(tt0);
+
+    __builtin_prefetch(dst_ptr);
+    s += 8;
+
+    t0 = vld1_u8(s);  // a8 a9 a10 a11 a12 a13 a14 a15
+    s7 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+
+    s1 = vext_s16(s0, s4, 1);  // a1 a2 a3 a4
+    s2 = vext_s16(s0, s4, 2);  // a2 a3 a4 a5
+    s3 = vext_s16(s0, s4, 3);  // a3 a4 a5 a6
+    s5 = vext_s16(s4, s7, 1);  // a5 a6 a7 a8
+    s6 = vext_s16(s4, s7, 2);  // a6 a7 a8 a9
+    s7 = vext_s16(s4, s7, 3);  // a7 a8 a9 a10
+
+    int16x4_t d0 = convolve8_4x4_s16(s0, s1, s2, s3, s4, s5, s6, s7, x_filter,
+                                     horiz_const, shift_round_0);
+
+    if (width == 4) {
+      vst1_s16(dst_ptr, d0);
+      dst_ptr += dst_stride;
+    } else if (width == 2) {
+      vst1_lane_u32((uint32_t *)dst_ptr, vreinterpret_u32_s16(d0), 0);
+      dst_ptr += dst_stride;
+    }
+
+    src_ptr += src_stride;
+    height--;
+  } while (height > 0);
+}
+void svt_av1_convolve_2d_sr_neon(const uint8_t *src, int32_t src_stride, uint8_t *dst,
+                                int32_t dst_stride, int32_t w, int32_t h,
+                                InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y,
+                                const int32_t subpel_x_qn, const int32_t subpel_y_q4,
+                                ConvolveParams *conv_params){
+
+  if (filter_params_x->taps > 8) {
+    av1_convolve_2d_sr_c(src, src_stride, dst, dst_stride, w, h,
+                         filter_params_x, filter_params_y, subpel_x_qn,
+                         subpel_y_q4, conv_params);
+    return;
+  }
+  int im_dst_stride;
+  int width, height;
+#if defined(__aarch64__)
+  uint8x8_t t0;
+  uint8x8_t t1, t2, t3, t4, t5, t6, t7;
+  const uint8_t *s;
+#endif
+
+  DECLARE_ALIGNED(16, int16_t,
+                  im_block[(MAX_SB_SIZE + HORIZ_EXTRA_ROWS) * MAX_SB_SIZE]);
+
+  const int bd = 8;
+  const int im_h = h + filter_params_y->taps - 1;
+  const int im_stride = MAX_SB_SIZE;
+  const int vert_offset = filter_params_y->taps / 2 - 1;
+  const int horiz_offset = filter_params_x->taps / 2 - 1;
+
+  const uint8_t *src_ptr = src - vert_offset * src_stride - horiz_offset;
+
+  int16_t *dst_ptr;
+
+  dst_ptr = im_block;
+  im_dst_stride = im_stride;
+  height = im_h;
+  width = w;
+
+  const int16_t round_bits =
+      FILTER_BITS * 2 - conv_params->round_0 - conv_params->round_1;
+  const int16x8_t vec_round_bits = vdupq_n_s16(-round_bits);
+  const int offset_bits = bd + 2 * FILTER_BITS - conv_params->round_0;
+  const int16_t *x_filter = av1_get_interp_filter_subpel_kernel(
+      *filter_params_x, subpel_x_qn & SUBPEL_MASK);
+
+  int16_t x_filter_tmp[8];
+  int16x8_t filter_x_coef = vld1q_s16(x_filter);
+
+  // filter coeffs are even, so downshifting by 1 to reduce intermediate
+  // precision requirements.
+  filter_x_coef = vshrq_n_s16(filter_x_coef, 1);
+  vst1q_s16(&x_filter_tmp[0], filter_x_coef);
+
+  assert(conv_params->round_0 > 0);
+
+  if (w <= 4) {
+    const int16x4_t horiz_const = vdup_n_s16((1 << (bd + FILTER_BITS - 2)));
+    const int16x4_t shift_round_0 = vdup_n_s16(-(conv_params->round_0 - 1));
+
+#if defined(__aarch64__)
+    int16x4_t s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, d0, d1, d2, d3;
+    do {
+      assert(height >= 4);
+      s = src_ptr;
+      __builtin_prefetch(s + 0 * src_stride);
+      __builtin_prefetch(s + 1 * src_stride);
+      __builtin_prefetch(s + 2 * src_stride);
+      __builtin_prefetch(s + 3 * src_stride);
+
+      load_u8_8x4(s, src_stride, &t0, &t1, &t2, &t3);
+      transpose_u8_8x4(&t0, &t1, &t2, &t3);
+
+      s0 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+      s1 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+      s2 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+      s3 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t3)));
+      s4 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+      s5 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+      s6 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+
+      __builtin_prefetch(dst_ptr + 0 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 1 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 2 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 3 * im_dst_stride);
+      s += 7;
+
+      load_u8_8x4(s, src_stride, &t0, &t1, &t2, &t3);
+      transpose_u8_8x4(&t0, &t1, &t2, &t3);
+
+      s7 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+      s8 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+      s9 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+      s10 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t3)));
+
+      d0 = convolve8_4x4_s16(s0, s1, s2, s3, s4, s5, s6, s7, x_filter_tmp,
+                             horiz_const, shift_round_0);
+      d1 = convolve8_4x4_s16(s1, s2, s3, s4, s5, s6, s7, s8, x_filter_tmp,
+                             horiz_const, shift_round_0);
+      d2 = convolve8_4x4_s16(s2, s3, s4, s5, s6, s7, s8, s9, x_filter_tmp,
+                             horiz_const, shift_round_0);
+      d3 = convolve8_4x4_s16(s3, s4, s5, s6, s7, s8, s9, s10, x_filter_tmp,
+                             horiz_const, shift_round_0);
+
+      transpose_s16_4x4d(&d0, &d1, &d2, &d3);
+      if (w == 4) {
+        vst1_s16((dst_ptr + 0 * im_dst_stride), d0);
+        vst1_s16((dst_ptr + 1 * im_dst_stride), d1);
+        vst1_s16((dst_ptr + 2 * im_dst_stride), d2);
+        vst1_s16((dst_ptr + 3 * im_dst_stride), d3);
+      } else if (w == 2) {
+        vst1_lane_u32((uint32_t *)(dst_ptr + 0 * im_dst_stride),
+                      vreinterpret_u32_s16(d0), 0);
+        vst1_lane_u32((uint32_t *)(dst_ptr + 1 * im_dst_stride),
+                      vreinterpret_u32_s16(d1), 0);
+        vst1_lane_u32((uint32_t *)(dst_ptr + 2 * im_dst_stride),
+                      vreinterpret_u32_s16(d2), 0);
+        vst1_lane_u32((uint32_t *)(dst_ptr + 3 * im_dst_stride),
+                      vreinterpret_u32_s16(d3), 0);
+      }
+      src_ptr += 4 * src_stride;
+      dst_ptr += 4 * im_dst_stride;
+      height -= 4;
+    } while (height >= 4);
+
+    if (height) {
+      assert(height < 4);
+      horiz_filter_w4_single_row(src_ptr, src_stride, dst_ptr, im_dst_stride, w,
+                                 height, x_filter_tmp, horiz_const,
+                                 shift_round_0);
+    }
+#else
+    horiz_filter_w4_single_row(src_ptr, src_stride, dst_ptr, im_dst_stride, w,
+                               height, x_filter_tmp, horiz_const,
+                               shift_round_0);
+#endif
+
+  } else {
+    const int16x8_t horiz_const = vdupq_n_s16((1 << (bd + FILTER_BITS - 2)));
+    const int16x8_t shift_round_0 = vdupq_n_s16(-(conv_params->round_0 - 1));
+
+#if defined(__aarch64__)
+    int16_t *d_tmp;
+    int16x8_t s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14;
+    int16x8_t res0, res1, res2, res3, res4, res5, res6, res7;
+    do {
+      assert(height >= 8);
+      __builtin_prefetch(src_ptr + 0 * src_stride);
+      __builtin_prefetch(src_ptr + 1 * src_stride);
+      __builtin_prefetch(src_ptr + 2 * src_stride);
+      __builtin_prefetch(src_ptr + 3 * src_stride);
+      __builtin_prefetch(src_ptr + 4 * src_stride);
+      __builtin_prefetch(src_ptr + 5 * src_stride);
+      __builtin_prefetch(src_ptr + 6 * src_stride);
+      __builtin_prefetch(src_ptr + 7 * src_stride);
+
+      load_u8_8x8(src_ptr, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+
+      transpose_u8_8x8(&t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+
+      s0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+      s1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+      s2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+      s3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+      s4 = vreinterpretq_s16_u16(vmovl_u8(t4));
+      s5 = vreinterpretq_s16_u16(vmovl_u8(t5));
+      s6 = vreinterpretq_s16_u16(vmovl_u8(t6));
+
+      width = w;
+      s = src_ptr + 7;
+      d_tmp = dst_ptr;
+
+      __builtin_prefetch(dst_ptr + 0 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 1 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 2 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 3 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 4 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 5 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 6 * im_dst_stride);
+      __builtin_prefetch(dst_ptr + 7 * im_dst_stride);
+
+      do {
+        load_u8_8x8(s, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+
+        transpose_u8_8x8(&t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+
+        s7 = vreinterpretq_s16_u16(vmovl_u8(t0));
+        s8 = vreinterpretq_s16_u16(vmovl_u8(t1));
+        s9 = vreinterpretq_s16_u16(vmovl_u8(t2));
+        s10 = vreinterpretq_s16_u16(vmovl_u8(t3));
+        s11 = vreinterpretq_s16_u16(vmovl_u8(t4));
+        s12 = vreinterpretq_s16_u16(vmovl_u8(t5));
+        s13 = vreinterpretq_s16_u16(vmovl_u8(t6));
+        s14 = vreinterpretq_s16_u16(vmovl_u8(t7));
+
+        res0 = convolve8_8x8_s16(s0, s1, s2, s3, s4, s5, s6, s7, x_filter_tmp,
+                                 horiz_const, shift_round_0);
+        res1 = convolve8_8x8_s16(s1, s2, s3, s4, s5, s6, s7, s8, x_filter_tmp,
+                                 horiz_const, shift_round_0);
+        res2 = convolve8_8x8_s16(s2, s3, s4, s5, s6, s7, s8, s9, x_filter_tmp,
+                                 horiz_const, shift_round_0);
+        res3 = convolve8_8x8_s16(s3, s4, s5, s6, s7, s8, s9, s10, x_filter_tmp,
+                                 horiz_const, shift_round_0);
+        res4 = convolve8_8x8_s16(s4, s5, s6, s7, s8, s9, s10, s11, x_filter_tmp,
+                                 horiz_const, shift_round_0);
+        res5 = convolve8_8x8_s16(s5, s6, s7, s8, s9, s10, s11, s12,
+                                 x_filter_tmp, horiz_const, shift_round_0);
+        res6 = convolve8_8x8_s16(s6, s7, s8, s9, s10, s11, s12, s13,
+                                 x_filter_tmp, horiz_const, shift_round_0);
+        res7 = convolve8_8x8_s16(s7, s8, s9, s10, s11, s12, s13, s14,
+                                 x_filter_tmp, horiz_const, shift_round_0);
+
+        transpose_s16_8x8(&res0, &res1, &res2, &res3, &res4, &res5, &res6,
+                          &res7);
+
+        store_s16_8x8(d_tmp, im_dst_stride, res0, res1, res2, res3, res4, res5,
+                      res6, res7);
+
+        s0 = s8;
+        s1 = s9;
+        s2 = s10;
+        s3 = s11;
+        s4 = s12;
+        s5 = s13;
+        s6 = s14;
+        s += 8;
+        d_tmp += 8;
+        width -= 8;
+      } while (width > 0);
+      src_ptr += 8 * src_stride;
+      dst_ptr += 8 * im_dst_stride;
+      height -= 8;
+    } while (height >= 8);
+
+    if (height >= 4) {
+      assert(height < 8);
+      int16x4_t reg0, reg1, reg2, reg3, reg4, reg5, reg6, reg7, reg8, reg9,
+          reg10, reg11, reg12, reg13, reg14;
+      int16x4_t d0, d1, d2, d3, d4, d5, d6, d7;
+      int16x8_t out0, out1, out2, out3;
+
+      __builtin_prefetch(src_ptr + 0 * src_stride);
+      __builtin_prefetch(src_ptr + 1 * src_stride);
+      __builtin_prefetch(src_ptr + 2 * src_stride);
+      __builtin_prefetch(src_ptr + 3 * src_stride);
+
+      load_u8_8x4(src_ptr, src_stride, &t0, &t1, &t2, &t3);
+      transpose_u8_8x4(&t0, &t1, &t2, &t3);
+
+      reg0 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+      reg1 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+      reg2 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+      reg3 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t3)));
+      reg4 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+      reg5 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+      reg6 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+
+      __builtin_prefetch(dst_ptr + 0 * dst_stride);
+      __builtin_prefetch(dst_ptr + 1 * dst_stride);
+      __builtin_prefetch(dst_ptr + 2 * dst_stride);
+      __builtin_prefetch(dst_ptr + 3 * dst_stride);
+
+      s = src_ptr + 7;
+      d_tmp = dst_ptr;
+      width = w;
+
+      do {
+        load_u8_8x4(s, src_stride, &t0, &t1, &t2, &t3);
+        transpose_u8_8x4(&t0, &t1, &t2, &t3);
+
+        reg7 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+        reg8 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+        reg9 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+        reg10 = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(t3)));
+        reg11 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t0)));
+        reg12 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t1)));
+        reg13 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t2)));
+        reg14 = vget_high_s16(vreinterpretq_s16_u16(vmovl_u8(t3)));
+
+        d0 = convolve8_4x4(reg0, reg1, reg2, reg3, reg4, reg5, reg6, reg7,
+                           x_filter_tmp);
+
+        d1 = convolve8_4x4(reg1, reg2, reg3, reg4, reg5, reg6, reg7, reg8,
+                           x_filter_tmp);
+
+        d2 = convolve8_4x4(reg2, reg3, reg4, reg5, reg6, reg7, reg8, reg9,
+                           x_filter_tmp);
+
+        d3 = convolve8_4x4(reg3, reg4, reg5, reg6, reg7, reg8, reg9, reg10,
+                           x_filter_tmp);
+
+        d4 = convolve8_4x4(reg4, reg5, reg6, reg7, reg8, reg9, reg10, reg11,
+                           x_filter_tmp);
+
+        d5 = convolve8_4x4(reg5, reg6, reg7, reg8, reg9, reg10, reg11, reg12,
+                           x_filter_tmp);
+
+        d6 = convolve8_4x4(reg6, reg7, reg8, reg9, reg10, reg11, reg12, reg13,
+                           x_filter_tmp);
+
+        d7 = convolve8_4x4(reg7, reg8, reg9, reg10, reg11, reg12, reg13, reg14,
+                           x_filter_tmp);
+
+        transpose_s16_4x8(&d0, &d1, &d2, &d3, &d4, &d5, &d6, &d7, &out0, &out1,
+                          &out2, &out3);
+
+        out0 = vaddq_s16(out0, horiz_const);
+        out0 = vqrshlq_s16(out0, shift_round_0);
+
+        out1 = vaddq_s16(out1, horiz_const);
+        out1 = vqrshlq_s16(out1, shift_round_0);
+
+        out2 = vaddq_s16(out2, horiz_const);
+        out2 = vqrshlq_s16(out2, shift_round_0);
+
+        out3 = vaddq_s16(out3, horiz_const);
+        out3 = vqrshlq_s16(out3, shift_round_0);
+
+        store_s16_8x4(d_tmp, im_dst_stride, out0, out1, out2, out3);
+
+        reg0 = reg8;
+        reg1 = reg9;
+        reg2 = reg10;
+        reg3 = reg11;
+        reg4 = reg12;
+        reg5 = reg13;
+        reg6 = reg14;
+        s += 8;
+        d_tmp += 8;
+        width -= 8;
+      } while (width > 0);
+      src_ptr += 4 * src_stride;
+      dst_ptr += 4 * im_dst_stride;
+      height -= 4;
+    }
+
+    if (height) {
+      assert(height < 4);
+      horiz_filter_w8_single_row(src_ptr, src_stride, dst_ptr, im_stride, w,
+                                 height, x_filter_tmp, horiz_const,
+                                 shift_round_0);
+    }
+#else
+
+    horiz_filter_w8_single_row(src_ptr, src_stride, dst_ptr, im_stride, w,
+                               height, x_filter_tmp, horiz_const,
+                               shift_round_0);
+#endif
+  }
+
+  // vertical
+  {
+    uint8_t *dst_u8_ptr, *d_u8;
+    int16_t *v_src_ptr, *v_s;
+
+    const int32_t sub_const = (1 << (offset_bits - conv_params->round_1)) +
+                              (1 << (offset_bits - conv_params->round_1 - 1));
+    const int16_t *y_filter = av1_get_interp_filter_subpel_kernel(
+        *filter_params_y, subpel_y_q4 & SUBPEL_MASK);
+
+    const int32x4_t round_shift_vec = vdupq_n_s32(-(conv_params->round_1));
+    const int32x4_t offset_const = vdupq_n_s32(1 << offset_bits);
+    const int32x4_t sub_const_vec = vdupq_n_s32(sub_const);
+
+    src_stride = im_stride;
+    v_src_ptr = im_block;
+    dst_u8_ptr = dst;
+
+    height = h;
+    width = w;
+
+    if (width <= 4) {
+      int16x4_t s0, s1, s2, s3, s4, s5, s6, s7;
+      uint16x4_t d0;
+      uint16x8_t dd0;
+      uint8x8_t d01;
+
+#if defined(__aarch64__)
+      int16x4_t s8, s9, s10;
+      uint16x4_t d1, d2, d3;
+      uint16x8_t dd1;
+      uint8x8_t d23;
+#endif
+
+      d_u8 = dst_u8_ptr;
+      v_s = v_src_ptr;
+
+      __builtin_prefetch(v_s + 0 * im_stride);
+      __builtin_prefetch(v_s + 1 * im_stride);
+      __builtin_prefetch(v_s + 2 * im_stride);
+      __builtin_prefetch(v_s + 3 * im_stride);
+      __builtin_prefetch(v_s + 4 * im_stride);
+      __builtin_prefetch(v_s + 5 * im_stride);
+      __builtin_prefetch(v_s + 6 * im_stride);
+      __builtin_prefetch(v_s + 7 * im_stride);
+
+      load_s16_4x8(v_s, im_stride, &s0, &s1, &s2, &s3, &s4, &s5, &s6, &s7);
+      v_s += (7 * im_stride);
+
+      do {
+#if defined(__aarch64__)
+        load_s16_4x4(v_s, im_stride, &s7, &s8, &s9, &s10);
+        v_s += (im_stride << 2);
+
+        __builtin_prefetch(d_u8 + 0 * dst_stride);
+        __builtin_prefetch(d_u8 + 1 * dst_stride);
+        __builtin_prefetch(d_u8 + 2 * dst_stride);
+        __builtin_prefetch(d_u8 + 3 * dst_stride);
+
+        d0 = convolve8_vert_4x4_s32(s0, s1, s2, s3, s4, s5, s6, s7, y_filter,
+                                    round_shift_vec, offset_const,
+                                    sub_const_vec);
+        d1 = convolve8_vert_4x4_s32(s1, s2, s3, s4, s5, s6, s7, s8, y_filter,
+                                    round_shift_vec, offset_const,
+                                    sub_const_vec);
+        d2 = convolve8_vert_4x4_s32(s2, s3, s4, s5, s6, s7, s8, s9, y_filter,
+                                    round_shift_vec, offset_const,
+                                    sub_const_vec);
+        d3 = convolve8_vert_4x4_s32(s3, s4, s5, s6, s7, s8, s9, s10, y_filter,
+                                    round_shift_vec, offset_const,
+                                    sub_const_vec);
+
+        dd0 = vqrshlq_u16(vcombine_u16(d0, d1), vec_round_bits);
+        dd1 = vqrshlq_u16(vcombine_u16(d2, d3), vec_round_bits);
+
+        d01 = vqmovn_u16(dd0);
+        d23 = vqmovn_u16(dd1);
+
+        if ((w == 4) && (h != 2)) {
+          vst1_lane_u32((uint32_t *)d_u8, vreinterpret_u32_u8(d01),
+                        0);  // 00 01 02 03
+          d_u8 += dst_stride;
+          vst1_lane_u32((uint32_t *)d_u8, vreinterpret_u32_u8(d01),
+                        1);  // 10 11 12 13
+          d_u8 += dst_stride;
+          vst1_lane_u32((uint32_t *)d_u8, vreinterpret_u32_u8(d23),
+                        0);  // 20 21 22 23
+          d_u8 += dst_stride;
+          vst1_lane_u32((uint32_t *)d_u8, vreinterpret_u32_u8(d23),
+                        1);  // 30 31 32 33
+          d_u8 += dst_stride;
+        } else if ((w == 2) && (h != 2)) {
+          vst1_lane_u16((uint16_t *)d_u8, vreinterpret_u16_u8(d01),
+                        0);  // 00 01
+          d_u8 += dst_stride;
+          vst1_lane_u16((uint16_t *)d_u8, vreinterpret_u16_u8(d01),
+                        2);  // 10 11
+          d_u8 += dst_stride;
+          vst1_lane_u16((uint16_t *)d_u8, vreinterpret_u16_u8(d23),
+                        0);  // 20 21
+          d_u8 += dst_stride;
+          vst1_lane_u16((uint16_t *)d_u8, vreinterpret_u16_u8(d23),
+                        2);  // 30 31
+          d_u8 += dst_stride;
+        } else if ((w == 4) && (h == 2)) {
+          vst1_lane_u32((uint32_t *)d_u8, vreinterpret_u32_u8(d01),
+                        0);  // 00 01 02 03
+          d_u8 += dst_stride;
+          vst1_lane_u32((uint32_t *)d_u8, vreinterpret_u32_u8(d01),
+                        1);  // 10 11 12 13
+          d_u8 += dst_stride;
+        } else if ((w == 2) && (h == 2)) {
+          vst1_lane_u16((uint16_t *)d_u8, vreinterpret_u16_u8(d01),
+                        0);  // 00 01
+          d_u8 += dst_stride;
+          vst1_lane_u16((uint16_t *)d_u8, vreinterpret_u16_u8(d01),
+                        2);  // 10 11
+          d_u8 += dst_stride;
+        }
+
+        s0 = s4;
+        s1 = s5;
+        s2 = s6;
+        s3 = s7;
+        s4 = s8;
+        s5 = s9;
+        s6 = s10;
+        height -= 4;
+#else
+        s7 = vld1_s16(v_s);
+        v_s += im_stride;
+
+        __builtin_prefetch(d_u8 + 0 * dst_stride);
+
+        d0 = convolve8_vert_4x4_s32(s0, s1, s2, s3, s4, s5, s6, s7, y_filter,
+                                    round_shift_vec, offset_const,
+                                    sub_const_vec);
+
+        dd0 = vqrshlq_u16(vcombine_u16(d0, d0), vec_round_bits);
+        d01 = vqmovn_u16(dd0);
+
+        if (w == 4) {
+          vst1_lane_u32((uint32_t *)d_u8, vreinterpret_u32_u8(d01),
+                        0);  // 00 01 02 03
+          d_u8 += dst_stride;
+
+        } else if (w == 2) {
+          vst1_lane_u16((uint16_t *)d_u8, vreinterpret_u16_u8(d01),
+                        0);  // 00 01
+          d_u8 += dst_stride;
+        }
+
+        s0 = s1;
+        s1 = s2;
+        s2 = s3;
+        s3 = s4;
+        s4 = s5;
+        s5 = s6;
+        s6 = s7;
+        height -= 1;
+#endif
+      } while (height > 0);
+    } else {
+      // if width is a multiple of 8 & height is a multiple of 4
+      int16x8_t s0, s1, s2, s3, s4, s5, s6, s7;
+      uint8x8_t res0;
+#if defined(__aarch64__)
+      int16x8_t s8, s9, s10;
+      uint8x8_t res1, res2, res3;
+#endif
+
+      do {
+        __builtin_prefetch(v_src_ptr + 0 * im_stride);
+        __builtin_prefetch(v_src_ptr + 1 * im_stride);
+        __builtin_prefetch(v_src_ptr + 2 * im_stride);
+        __builtin_prefetch(v_src_ptr + 3 * im_stride);
+        __builtin_prefetch(v_src_ptr + 4 * im_stride);
+        __builtin_prefetch(v_src_ptr + 5 * im_stride);
+        __builtin_prefetch(v_src_ptr + 6 * im_stride);
+        __builtin_prefetch(v_src_ptr + 7 * im_stride);
+
+        v_s = v_src_ptr;
+        load_s16_8x8(v_s, im_stride, &s0, &s1, &s2, &s3, &s4, &s5, &s6, &s7);
+        v_s += (7 * im_stride);
+
+        d_u8 = dst_u8_ptr;
+        height = h;
+
+        do {
+#if defined(__aarch64__)
+          load_s16_8x4(v_s, im_stride, &s7, &s8, &s9, &s10);
+          v_s += (im_stride << 2);
+
+          __builtin_prefetch(d_u8 + 4 * dst_stride);
+          __builtin_prefetch(d_u8 + 5 * dst_stride);
+          __builtin_prefetch(d_u8 + 6 * dst_stride);
+          __builtin_prefetch(d_u8 + 7 * dst_stride);
+
+          res0 = convolve8_vert_8x4_s32(s0, s1, s2, s3, s4, s5, s6, s7,
+                                        y_filter, round_shift_vec, offset_const,
+                                        sub_const_vec, vec_round_bits);
+          res1 = convolve8_vert_8x4_s32(s1, s2, s3, s4, s5, s6, s7, s8,
+                                        y_filter, round_shift_vec, offset_const,
+                                        sub_const_vec, vec_round_bits);
+          res2 = convolve8_vert_8x4_s32(s2, s3, s4, s5, s6, s7, s8, s9,
+                                        y_filter, round_shift_vec, offset_const,
+                                        sub_const_vec, vec_round_bits);
+          res3 = convolve8_vert_8x4_s32(s3, s4, s5, s6, s7, s8, s9, s10,
+                                        y_filter, round_shift_vec, offset_const,
+                                        sub_const_vec, vec_round_bits);
+
+          if (h != 2) {
+            vst1_u8(d_u8, res0);
+            d_u8 += dst_stride;
+            vst1_u8(d_u8, res1);
+            d_u8 += dst_stride;
+            vst1_u8(d_u8, res2);
+            d_u8 += dst_stride;
+            vst1_u8(d_u8, res3);
+            d_u8 += dst_stride;
+          } else {
+            vst1_u8(d_u8, res0);
+            d_u8 += dst_stride;
+            vst1_u8(d_u8, res1);
+            d_u8 += dst_stride;
+          }
+          s0 = s4;
+          s1 = s5;
+          s2 = s6;
+          s3 = s7;
+          s4 = s8;
+          s5 = s9;
+          s6 = s10;
+          height -= 4;
+#else
+          s7 = vld1q_s16(v_s);
+          v_s += im_stride;
+
+          __builtin_prefetch(d_u8 + 0 * dst_stride);
+
+          res0 = convolve8_vert_8x4_s32(s0, s1, s2, s3, s4, s5, s6, s7,
+                                        y_filter, round_shift_vec, offset_const,
+                                        sub_const_vec, vec_round_bits);
+
+          vst1_u8(d_u8, res0);
+          d_u8 += dst_stride;
+
+          s0 = s1;
+          s1 = s2;
+          s2 = s3;
+          s3 = s4;
+          s4 = s5;
+          s5 = s6;
+          s6 = s7;
+          height -= 1;
+#endif
+        } while (height > 0);
+        v_src_ptr += 8;
+        dst_u8_ptr += 8;
+        w -= 8;
+      } while (w > 0);
+    }
+  }
+}
+
+static INLINE void scaledconvolve_horiz_w4(
+    const uint8_t *src, const ptrdiff_t src_stride, uint8_t *dst,
+    const ptrdiff_t dst_stride, const InterpKernel *const x_filters,
+    const int x0_q4, const int x_step_q4, const int w, const int h) {
+  DECLARE_ALIGNED(16, uint8_t, temp[4 * 4]);
+  int x, y, z;
+
+  src -= SUBPEL_TAPS / 2 - 1;
+
+  y = h;
+  do {
+    int x_q4 = x0_q4;
+    x = 0;
+    do {
+      // process 4 src_x steps
+      for (z = 0; z < 4; ++z) {
+        const uint8_t *const src_x = &src[x_q4 >> SUBPEL_BITS];
+        if (x_q4 & SUBPEL_MASK) {
+          const int16x8_t filters = vld1q_s16(x_filters[x_q4 & SUBPEL_MASK]);
+          const int16x4_t filter3 = vdup_lane_s16(vget_low_s16(filters), 3);
+          const int16x4_t filter4 = vdup_lane_s16(vget_high_s16(filters), 0);
+          uint8x8_t s[8], d;
+          int16x8_t ss[4];
+          int16x4_t t[8], tt;
+
+          load_u8_8x4(src_x, src_stride, &s[0], &s[1], &s[2], &s[3]);
+          transpose_u8_8x4(&s[0], &s[1], &s[2], &s[3]);
+
+          ss[0] = vreinterpretq_s16_u16(vmovl_u8(s[0]));
+          ss[1] = vreinterpretq_s16_u16(vmovl_u8(s[1]));
+          ss[2] = vreinterpretq_s16_u16(vmovl_u8(s[2]));
+          ss[3] = vreinterpretq_s16_u16(vmovl_u8(s[3]));
+          t[0] = vget_low_s16(ss[0]);
+          t[1] = vget_low_s16(ss[1]);
+          t[2] = vget_low_s16(ss[2]);
+          t[3] = vget_low_s16(ss[3]);
+          t[4] = vget_high_s16(ss[0]);
+          t[5] = vget_high_s16(ss[1]);
+          t[6] = vget_high_s16(ss[2]);
+          t[7] = vget_high_s16(ss[3]);
+
+          tt = convolve8_4(t[0], t[1], t[2], t[3], t[4], t[5], t[6], t[7],
+                           filters, filter3, filter4);
+          d = vqrshrun_n_s16(vcombine_s16(tt, tt), 7);
+          vst1_lane_u32((uint32_t *)&temp[4 * z], vreinterpret_u32_u8(d), 0);
+        } else {
+          int i;
+          for (i = 0; i < 4; ++i) {
+            temp[z * 4 + i] = src_x[i * src_stride + 3];
+          }
+        }
+        x_q4 += x_step_q4;
+      }
+
+      // transpose the 4x4 filters values back to dst
+      {
+        const uint8x8x4_t d4 = vld4_u8(temp);
+        vst1_lane_u32((uint32_t *)&dst[x + 0 * dst_stride],
+                      vreinterpret_u32_u8(d4.val[0]), 0);
+        vst1_lane_u32((uint32_t *)&dst[x + 1 * dst_stride],
+                      vreinterpret_u32_u8(d4.val[1]), 0);
+        vst1_lane_u32((uint32_t *)&dst[x + 2 * dst_stride],
+                      vreinterpret_u32_u8(d4.val[2]), 0);
+        vst1_lane_u32((uint32_t *)&dst[x + 3 * dst_stride],
+                      vreinterpret_u32_u8(d4.val[3]), 0);
+      }
+      x += 4;
+    } while (x < w);
+
+    src += src_stride * 4;
+    dst += dst_stride * 4;
+    y -= 4;
+  } while (y > 0);
+}
+
+static INLINE void scaledconvolve_horiz_w8(
+    const uint8_t *src, const ptrdiff_t src_stride, uint8_t *dst,
+    const ptrdiff_t dst_stride, const InterpKernel *const x_filters,
+    const int x0_q4, const int x_step_q4, const int w, const int h) {
+  DECLARE_ALIGNED(16, uint8_t, temp[8 * 8]);
+  int x, y, z;
+  src -= SUBPEL_TAPS / 2 - 1;
+
+  // This function processes 8x8 areas. The intermediate height is not always
+  // a multiple of 8, so force it to be a multiple of 8 here.
+  y = (h + 7) & ~7;
+
+  do {
+    int x_q4 = x0_q4;
+    x = 0;
+    do {
+      uint8x8_t d[8];
+      // process 8 src_x steps
+      for (z = 0; z < 8; ++z) {
+        const uint8_t *const src_x = &src[x_q4 >> SUBPEL_BITS];
+
+        if (x_q4 & SUBPEL_MASK) {
+          const int16x8_t filters = vld1q_s16(x_filters[x_q4 & SUBPEL_MASK]);
+          uint8x8_t s[8];
+          load_u8_8x8(src_x, src_stride, &s[0], &s[1], &s[2], &s[3], &s[4],
+                      &s[5], &s[6], &s[7]);
+          transpose_u8_8x8(&s[0], &s[1], &s[2], &s[3], &s[4], &s[5], &s[6],
+                           &s[7]);
+          d[0] = scale_filter_8(s, filters);
+          vst1_u8(&temp[8 * z], d[0]);
+        } else {
+          int i;
+          for (i = 0; i < 8; ++i) {
+            temp[z * 8 + i] = src_x[i * src_stride + 3];
+          }
+        }
+        x_q4 += x_step_q4;
+      }
+
+      // transpose the 8x8 filters values back to dst
+      load_u8_8x8(temp, 8, &d[0], &d[1], &d[2], &d[3], &d[4], &d[5], &d[6],
+                  &d[7]);
+      transpose_u8_8x8(&d[0], &d[1], &d[2], &d[3], &d[4], &d[5], &d[6], &d[7]);
+      vst1_u8(&dst[x + 0 * dst_stride], d[0]);
+      vst1_u8(&dst[x + 1 * dst_stride], d[1]);
+      vst1_u8(&dst[x + 2 * dst_stride], d[2]);
+      vst1_u8(&dst[x + 3 * dst_stride], d[3]);
+      vst1_u8(&dst[x + 4 * dst_stride], d[4]);
+      vst1_u8(&dst[x + 5 * dst_stride], d[5]);
+      vst1_u8(&dst[x + 6 * dst_stride], d[6]);
+      vst1_u8(&dst[x + 7 * dst_stride], d[7]);
+      x += 8;
+    } while (x < w);
+
+    src += src_stride * 8;
+    dst += dst_stride * 8;
+  } while (y -= 8);
+}
+
+static INLINE void scaledconvolve_vert_w4(
+    const uint8_t *src, const ptrdiff_t src_stride, uint8_t *dst,
+    const ptrdiff_t dst_stride, const InterpKernel *const y_filters,
+    const int y0_q4, const int y_step_q4, const int w, const int h) {
+  int y;
+  int y_q4 = y0_q4;
+
+  src -= src_stride * (SUBPEL_TAPS / 2 - 1);
+  y = h;
+  do {
+    const unsigned char *src_y = &src[(y_q4 >> SUBPEL_BITS) * src_stride];
+
+    if (y_q4 & SUBPEL_MASK) {
+      const int16x8_t filters = vld1q_s16(y_filters[y_q4 & SUBPEL_MASK]);
+      const int16x4_t filter3 = vdup_lane_s16(vget_low_s16(filters), 3);
+      const int16x4_t filter4 = vdup_lane_s16(vget_high_s16(filters), 0);
+      uint8x8_t s[8], d;
+      int16x4_t t[8], tt;
+
+      load_u8_8x8(src_y, src_stride, &s[0], &s[1], &s[2], &s[3], &s[4], &s[5],
+                  &s[6], &s[7]);
+      t[0] = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(s[0])));
+      t[1] = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(s[1])));
+      t[2] = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(s[2])));
+      t[3] = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(s[3])));
+      t[4] = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(s[4])));
+      t[5] = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(s[5])));
+      t[6] = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(s[6])));
+      t[7] = vget_low_s16(vreinterpretq_s16_u16(vmovl_u8(s[7])));
+
+      tt = convolve8_4(t[0], t[1], t[2], t[3], t[4], t[5], t[6], t[7], filters,
+                       filter3, filter4);
+      d = vqrshrun_n_s16(vcombine_s16(tt, tt), 7);
+      vst1_lane_u32((uint32_t *)dst, vreinterpret_u32_u8(d), 0);
+    } else {
+      memcpy(dst, &src_y[3 * src_stride], w);
+    }
+
+    dst += dst_stride;
+    y_q4 += y_step_q4;
+  } while (--y);
+}
+
+static INLINE void scaledconvolve_vert_w8(
+    const uint8_t *src, const ptrdiff_t src_stride, uint8_t *dst,
+    const ptrdiff_t dst_stride, const InterpKernel *const y_filters,
+    const int y0_q4, const int y_step_q4, const int w, const int h) {
+  int y;
+  int y_q4 = y0_q4;
+
+  src -= src_stride * (SUBPEL_TAPS / 2 - 1);
+  y = h;
+  do {
+    const unsigned char *src_y = &src[(y_q4 >> SUBPEL_BITS) * src_stride];
+    if (y_q4 & SUBPEL_MASK) {
+      const int16x8_t filters = vld1q_s16(y_filters[y_q4 & SUBPEL_MASK]);
+      uint8x8_t s[8], d;
+      load_u8_8x8(src_y, src_stride, &s[0], &s[1], &s[2], &s[3], &s[4], &s[5],
+                  &s[6], &s[7]);
+      d = scale_filter_8(s, filters);
+      vst1_u8(dst, d);
+    } else {
+      memcpy(dst, &src_y[3 * src_stride], w);
+    }
+    dst += dst_stride;
+    y_q4 += y_step_q4;
+  } while (--y);
+}
+
+static INLINE void scaledconvolve_vert_w16(
+    const uint8_t *src, const ptrdiff_t src_stride, uint8_t *dst,
+    const ptrdiff_t dst_stride, const InterpKernel *const y_filters,
+    const int y0_q4, const int y_step_q4, const int w, const int h) {
+  int x, y;
+  int y_q4 = y0_q4;
+
+  src -= src_stride * (SUBPEL_TAPS / 2 - 1);
+  y = h;
+  do {
+    const unsigned char *src_y = &src[(y_q4 >> SUBPEL_BITS) * src_stride];
+    if (y_q4 & SUBPEL_MASK) {
+      x = 0;
+      do {
+        const int16x8_t filters = vld1q_s16(y_filters[y_q4 & SUBPEL_MASK]);
+        uint8x16_t ss[8];
+        uint8x8_t s[8], d[2];
+        load_u8_16x8(src_y, src_stride, &ss[0], &ss[1], &ss[2], &ss[3], &ss[4],
+                     &ss[5], &ss[6], &ss[7]);
+        s[0] = vget_low_u8(ss[0]);
+        s[1] = vget_low_u8(ss[1]);
+        s[2] = vget_low_u8(ss[2]);
+        s[3] = vget_low_u8(ss[3]);
+        s[4] = vget_low_u8(ss[4]);
+        s[5] = vget_low_u8(ss[5]);
+        s[6] = vget_low_u8(ss[6]);
+        s[7] = vget_low_u8(ss[7]);
+        d[0] = scale_filter_8(s, filters);
+
+        s[0] = vget_high_u8(ss[0]);
+        s[1] = vget_high_u8(ss[1]);
+        s[2] = vget_high_u8(ss[2]);
+        s[3] = vget_high_u8(ss[3]);
+        s[4] = vget_high_u8(ss[4]);
+        s[5] = vget_high_u8(ss[5]);
+        s[6] = vget_high_u8(ss[6]);
+        s[7] = vget_high_u8(ss[7]);
+        d[1] = scale_filter_8(s, filters);
+        vst1q_u8(&dst[x], vcombine_u8(d[0], d[1]));
+        src_y += 16;
+        x += 16;
+      } while (x < w);
+    } else {
+      memcpy(dst, &src_y[3 * src_stride], w);
+    }
+    dst += dst_stride;
+    y_q4 += y_step_q4;
+  } while (--y);
+}
+
+void aom_scaled_2d_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst,
+                        ptrdiff_t dst_stride, const InterpKernel *filter,
+                        int x0_q4, int x_step_q4, int y0_q4, int y_step_q4,
+                        int w, int h) {
+  // Note: Fixed size intermediate buffer, temp, places limits on parameters.
+  // 2d filtering proceeds in 2 steps:
+  //   (1) Interpolate horizontally into an intermediate buffer, temp.
+  //   (2) Interpolate temp vertically to derive the sub-pixel result.
+  // Deriving the maximum number of rows in the temp buffer (135):
+  // --Smallest scaling factor is x1/2 ==> y_step_q4 = 32 (Normative).
+  // --Largest block size is 64x64 pixels.
+  // --64 rows in the downscaled frame span a distance of (64 - 1) * 32 in the
+  //   original frame (in 1/16th pixel units).
+  // --Must round-up because block may be located at sub-pixel position.
+  // --Require an additional SUBPEL_TAPS rows for the 8-tap filter tails.
+  // --((64 - 1) * 32 + 15) >> 4 + 8 = 135.
+  // --Require an additional 8 rows for the horiz_w8 transpose tail.
+  // When calling in frame scaling function, the smallest scaling factor is x1/4
+  // ==> y_step_q4 = 64. Since w and h are at most 16, the temp buffer is still
+  // big enough.
+  DECLARE_ALIGNED(16, uint8_t, temp[(135 + 8) * 64]);
+  const int intermediate_height =
+      (((h - 1) * y_step_q4 + y0_q4) >> SUBPEL_BITS) + SUBPEL_TAPS;
+
+  assert(w <= 64);
+  assert(h <= 64);
+  assert(y_step_q4 <= 32 || (y_step_q4 <= 64 && h <= 32));
+  assert(x_step_q4 <= 64);
+
+  if (w >= 8) {
+    scaledconvolve_horiz_w8(src - src_stride * (SUBPEL_TAPS / 2 - 1),
+                            src_stride, temp, 64, filter, x0_q4, x_step_q4, w,
+                            intermediate_height);
+  } else {
+    scaledconvolve_horiz_w4(src - src_stride * (SUBPEL_TAPS / 2 - 1),
+                            src_stride, temp, 64, filter, x0_q4, x_step_q4, w,
+                            intermediate_height);
+  }
+
+  if (w >= 16) {
+    scaledconvolve_vert_w16(temp + 64 * (SUBPEL_TAPS / 2 - 1), 64, dst,
+                            dst_stride, filter, y0_q4, y_step_q4, w, h);
+  } else if (w == 8) {
+    scaledconvolve_vert_w8(temp + 64 * (SUBPEL_TAPS / 2 - 1), 64, dst,
+                           dst_stride, filter, y0_q4, y_step_q4, w, h);
+  } else {
+    scaledconvolve_vert_w4(temp + 64 * (SUBPEL_TAPS / 2 - 1), 64, dst,
+                           dst_stride, filter, y0_q4, y_step_q4, w, h);
+  }
+}
diff --git a/Source/Lib/Common/ASM_NEON/convolve_neon.h b/Source/Lib/Common/ASM_NEON/convolve_neon.h
new file mode 100644
index 0000000..cc373f0
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/convolve_neon.h
@@ -0,0 +1,298 @@
+/*
+ *  Copyright (c) 2018, Alliance for Open Media. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#ifndef AOM_AV1_COMMON_ARM_CONVOLVE_NEON_H_
+#define AOM_AV1_COMMON_ARM_CONVOLVE_NEON_H_
+
+#include <arm_neon.h>
+
+void svt_av1_wiener_convolve_add_src_neon(const uint8_t *const src, const ptrdiff_t src_stride,
+                                       uint8_t *const dst, const ptrdiff_t dst_stride,
+                                       const int16_t *const filter_x, const int16_t *const filter_y,
+                                       const int32_t w, const int32_t h,
+                                       const ConvolveParams *const conv_params);
+
+
+#define HORIZ_EXTRA_ROWS ((SUBPEL_TAPS + 7) & ~0x07)
+
+static INLINE int16x4_t convolve8_4(const int16x4_t s0, const int16x4_t s1,
+                                    const int16x4_t s2, const int16x4_t s3,
+                                    const int16x4_t s4, const int16x4_t s5,
+                                    const int16x4_t s6, const int16x4_t s7,
+                                    const int16x8_t filters,
+                                    const int16x4_t filter3,
+                                    const int16x4_t filter4) {
+  const int16x4_t filters_lo = vget_low_s16(filters);
+  const int16x4_t filters_hi = vget_high_s16(filters);
+  int16x4_t sum;
+
+  sum = vmul_lane_s16(s0, filters_lo, 0);
+  sum = vmla_lane_s16(sum, s1, filters_lo, 1);
+  sum = vmla_lane_s16(sum, s2, filters_lo, 2);
+  sum = vmla_lane_s16(sum, s5, filters_hi, 1);
+  sum = vmla_lane_s16(sum, s6, filters_hi, 2);
+  sum = vmla_lane_s16(sum, s7, filters_hi, 3);
+  sum = vqadd_s16(sum, vmul_s16(s3, filter3));
+  sum = vqadd_s16(sum, vmul_s16(s4, filter4));
+  return sum;
+}
+
+static INLINE uint8x8_t convolve8_8(const int16x8_t s0, const int16x8_t s1,
+                                    const int16x8_t s2, const int16x8_t s3,
+                                    const int16x8_t s4, const int16x8_t s5,
+                                    const int16x8_t s6, const int16x8_t s7,
+                                    const int16x8_t filters,
+                                    const int16x8_t filter3,
+                                    const int16x8_t filter4) {
+  const int16x4_t filters_lo = vget_low_s16(filters);
+  const int16x4_t filters_hi = vget_high_s16(filters);
+  int16x8_t sum;
+
+  sum = vmulq_lane_s16(s0, filters_lo, 0);
+  sum = vmlaq_lane_s16(sum, s1, filters_lo, 1);
+  sum = vmlaq_lane_s16(sum, s2, filters_lo, 2);
+  sum = vmlaq_lane_s16(sum, s5, filters_hi, 1);
+  sum = vmlaq_lane_s16(sum, s6, filters_hi, 2);
+  sum = vmlaq_lane_s16(sum, s7, filters_hi, 3);
+  sum = vqaddq_s16(sum, vmulq_s16(s3, filter3));
+  sum = vqaddq_s16(sum, vmulq_s16(s4, filter4));
+  return vqrshrun_n_s16(sum, 7);
+}
+
+static INLINE uint8x8_t scale_filter_8(const uint8x8_t *const s,
+                                       const int16x8_t filters) {
+  const int16x8_t filter3 = vdupq_lane_s16(vget_low_s16(filters), 3);
+  const int16x8_t filter4 = vdupq_lane_s16(vget_high_s16(filters), 0);
+  int16x8_t ss[8];
+
+  ss[0] = vreinterpretq_s16_u16(vmovl_u8(s[0]));
+  ss[1] = vreinterpretq_s16_u16(vmovl_u8(s[1]));
+  ss[2] = vreinterpretq_s16_u16(vmovl_u8(s[2]));
+  ss[3] = vreinterpretq_s16_u16(vmovl_u8(s[3]));
+  ss[4] = vreinterpretq_s16_u16(vmovl_u8(s[4]));
+  ss[5] = vreinterpretq_s16_u16(vmovl_u8(s[5]));
+  ss[6] = vreinterpretq_s16_u16(vmovl_u8(s[6]));
+  ss[7] = vreinterpretq_s16_u16(vmovl_u8(s[7]));
+
+  return convolve8_8(ss[0], ss[1], ss[2], ss[3], ss[4], ss[5], ss[6], ss[7],
+                     filters, filter3, filter4);
+}
+
+static INLINE uint8x8_t wiener_convolve8_vert_4x8(
+    const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
+    const int16x8_t s3, const int16x8_t s4, const int16x8_t s5,
+    const int16x8_t s6, int16_t *filter_y, const int bd,
+    const int round1_bits) {
+  int16x8_t ss0, ss1, ss2;
+  int32x4_t sum0, sum1;
+  uint16x4_t tmp0, tmp1;
+  uint16x8_t tmp;
+  uint8x8_t res;
+
+  const int32_t round_const = (1 << (bd + round1_bits - 1));
+  const int32x4_t round_bits = vdupq_n_s32(-round1_bits);
+  const int32x4_t zero = vdupq_n_s32(0);
+  const int32x4_t round_vec = vdupq_n_s32(round_const);
+
+  ss0 = vaddq_s16(s0, s6);
+  ss1 = vaddq_s16(s1, s5);
+  ss2 = vaddq_s16(s2, s4);
+
+  sum0 = vmull_n_s16(vget_low_s16(ss0), filter_y[0]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(ss1), filter_y[1]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(ss2), filter_y[2]);
+  sum0 = vmlal_n_s16(sum0, vget_low_s16(s3), filter_y[3]);
+
+  sum1 = vmull_n_s16(vget_high_s16(ss0), filter_y[0]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(ss1), filter_y[1]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(ss2), filter_y[2]);
+  sum1 = vmlal_n_s16(sum1, vget_high_s16(s3), filter_y[3]);
+
+  sum0 = vsubq_s32(sum0, round_vec);
+  sum1 = vsubq_s32(sum1, round_vec);
+
+  /* right shift & rounding */
+  sum0 = vrshlq_s32(sum0, round_bits);
+  sum1 = vrshlq_s32(sum1, round_bits);
+
+  sum0 = vmaxq_s32(sum0, zero);
+  sum1 = vmaxq_s32(sum1, zero);
+
+  /* from int32x4_t to uint8x8_t */
+  tmp0 = vqmovn_u32(vreinterpretq_u32_s32(sum0));
+  tmp1 = vqmovn_u32(vreinterpretq_u32_s32(sum1));
+  tmp = vcombine_u16(tmp0, tmp1);
+  res = vqmovn_u16(tmp);
+
+  return res;
+}
+
+static INLINE uint16x8_t wiener_convolve8_horiz_8x8(
+    const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
+    const int16x8_t s3, int16_t *filter_x, const int bd,
+    const int round0_bits) {
+  int16x8_t sum;
+  uint16x8_t res;
+  int32x4_t sum_0, sum_1;
+  int32x4_t s3_0, s3_1;
+  const int32_t round_const_0 = (1 << (bd + FILTER_BITS - 1));
+  const int32_t round_const_1 = (1 << (bd + 1 + FILTER_BITS - round0_bits)) - 1;
+
+  /* for the purpose of right shift by { conv_params->round_0 } */
+  const int32x4_t round_bits = vdupq_n_s32(-round0_bits);
+
+  const int32x4_t round_vec_0 = vdupq_n_s32(round_const_0);
+  const int32x4_t round_vec_1 = vdupq_n_s32(round_const_1);
+
+  sum = vmulq_n_s16(s0, filter_x[0]);
+  sum = vmlaq_n_s16(sum, s1, filter_x[1]);
+  sum = vmlaq_n_s16(sum, s2, filter_x[2]);
+
+  /* sum from 16x8 to 2 32x4 registers */
+  sum_0 = vmovl_s16(vget_low_s16(sum));
+  sum_1 = vmovl_s16(vget_high_s16(sum));
+
+  /* s[3]*128 -- and filter coef max can be 128
+   *  then max value possible = 128*128*255 exceeding 16 bit
+   */
+
+  s3_0 = vmull_n_s16(vget_low_s16(s3), filter_x[3]);
+  s3_1 = vmull_n_s16(vget_high_s16(s3), filter_x[3]);
+  sum_0 = vaddq_s32(sum_0, s3_0);
+  sum_1 = vaddq_s32(sum_1, s3_1);
+
+  /* Add the constant value */
+  sum_0 = vaddq_s32(sum_0, round_vec_0);
+  sum_1 = vaddq_s32(sum_1, round_vec_0);
+
+  /* right shift & rounding & saturating */
+  sum_0 = vqrshlq_s32(sum_0, round_bits);
+  sum_1 = vqrshlq_s32(sum_1, round_bits);
+
+  /* Clipping to max value */
+  sum_0 = vminq_s32(sum_0, round_vec_1);
+  sum_1 = vminq_s32(sum_1, round_vec_1);
+
+  res = vcombine_u16(vqmovun_s32(sum_0), vqmovun_s32(sum_1));
+  return res;
+}
+
+static INLINE uint16x4_t wiener_convolve8_horiz_4x8(
+    const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
+    const int16x4_t s3, const int16x4_t s4, const int16x4_t s5,
+    const int16x4_t s6, int16_t *filter_x, const int bd,
+    const int round0_bits) {
+  uint16x4_t res;
+  int32x4_t sum_0, s3_0;
+  int16x4_t sum, temp0, temp1, temp2;
+
+  const int32_t round_const_0 = (1 << (bd + FILTER_BITS - 1));
+  const int32_t round_const_1 = (1 << (bd + 1 + FILTER_BITS - round0_bits)) - 1;
+  const int32x4_t round_bits = vdupq_n_s32(-round0_bits);
+  const int32x4_t zero = vdupq_n_s32(0);
+  const int32x4_t round_vec_0 = vdupq_n_s32(round_const_0);
+  const int32x4_t round_vec_1 = vdupq_n_s32(round_const_1);
+
+  temp0 = vadd_s16(s0, s6);
+  temp1 = vadd_s16(s1, s5);
+  temp2 = vadd_s16(s2, s4);
+
+  sum = vmul_n_s16(temp0, filter_x[0]);
+  sum = vmla_n_s16(sum, temp1, filter_x[1]);
+  sum = vmla_n_s16(sum, temp2, filter_x[2]);
+  sum_0 = vmovl_s16(sum);
+
+  /* s[3]*128 -- and filter coff max can be 128.
+   * then max value possible = 128*128*255 Therefore, 32 bits are required to
+   * hold the result.
+   */
+  s3_0 = vmull_n_s16(s3, filter_x[3]);
+  sum_0 = vaddq_s32(sum_0, s3_0);
+
+  sum_0 = vaddq_s32(sum_0, round_vec_0);
+  sum_0 = vrshlq_s32(sum_0, round_bits);
+
+  sum_0 = vmaxq_s32(sum_0, zero);
+  sum_0 = vminq_s32(sum_0, round_vec_1);
+  res = vqmovun_s32(sum_0);
+  return res;
+}
+
+static INLINE int16x8_t
+convolve8_8x8_s16(const int16x8_t s0, const int16x8_t s1, const int16x8_t s2,
+                  const int16x8_t s3, const int16x8_t s4, const int16x8_t s5,
+                  const int16x8_t s6, const int16x8_t s7, const int16_t *filter,
+                  const int16x8_t horiz_const, const int16x8_t shift_round_0) {
+  int16x8_t sum;
+  int16x8_t res;
+
+  sum = horiz_const;
+  sum = vmlaq_n_s16(sum, s0, filter[0]);
+  sum = vmlaq_n_s16(sum, s1, filter[1]);
+  sum = vmlaq_n_s16(sum, s2, filter[2]);
+  sum = vmlaq_n_s16(sum, s3, filter[3]);
+  sum = vmlaq_n_s16(sum, s4, filter[4]);
+  sum = vmlaq_n_s16(sum, s5, filter[5]);
+  sum = vmlaq_n_s16(sum, s6, filter[6]);
+  sum = vmlaq_n_s16(sum, s7, filter[7]);
+
+  res = vqrshlq_s16(sum, shift_round_0);
+
+  return res;
+}
+
+static INLINE int16x4_t
+convolve8_4x4_s16(const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
+                  const int16x4_t s3, const int16x4_t s4, const int16x4_t s5,
+                  const int16x4_t s6, const int16x4_t s7, const int16_t *filter,
+                  const int16x4_t horiz_const, const int16x4_t shift_round_0) {
+  int16x4_t sum;
+  sum = horiz_const;
+  sum = vmla_n_s16(sum, s0, filter[0]);
+  sum = vmla_n_s16(sum, s1, filter[1]);
+  sum = vmla_n_s16(sum, s2, filter[2]);
+  sum = vmla_n_s16(sum, s3, filter[3]);
+  sum = vmla_n_s16(sum, s4, filter[4]);
+  sum = vmla_n_s16(sum, s5, filter[5]);
+  sum = vmla_n_s16(sum, s6, filter[6]);
+  sum = vmla_n_s16(sum, s7, filter[7]);
+
+  sum = vqrshl_s16(sum, shift_round_0);
+
+  return sum;
+}
+
+static INLINE uint16x4_t convolve8_4x4_s32(
+    const int16x4_t s0, const int16x4_t s1, const int16x4_t s2,
+    const int16x4_t s3, const int16x4_t s4, const int16x4_t s5,
+    const int16x4_t s6, const int16x4_t s7, const int16_t *y_filter,
+    const int32x4_t round_shift_vec, const int32x4_t offset_const) {
+  int32x4_t sum0;
+  uint16x4_t res;
+  const int32x4_t zero = vdupq_n_s32(0);
+
+  sum0 = vmull_n_s16(s0, y_filter[0]);
+  sum0 = vmlal_n_s16(sum0, s1, y_filter[1]);
+  sum0 = vmlal_n_s16(sum0, s2, y_filter[2]);
+  sum0 = vmlal_n_s16(sum0, s3, y_filter[3]);
+  sum0 = vmlal_n_s16(sum0, s4, y_filter[4]);
+  sum0 = vmlal_n_s16(sum0, s5, y_filter[5]);
+  sum0 = vmlal_n_s16(sum0, s6, y_filter[6]);
+  sum0 = vmlal_n_s16(sum0, s7, y_filter[7]);
+
+  sum0 = vaddq_s32(sum0, offset_const);
+  sum0 = vqrshlq_s32(sum0, round_shift_vec);
+  sum0 = vmaxq_s32(sum0, zero);
+  res = vmovn_u32(vreinterpretq_u32_s32(sum0));
+
+  return res;
+}
+
+#endif  // AOM_AV1_COMMON_ARM_CONVOLVE_NEON_H_
diff --git a/Source/Lib/Common/ASM_NEON/mem_neon.h b/Source/Lib/Common/ASM_NEON/mem_neon.h
new file mode 100644
index 0000000..6a7afd6
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/mem_neon.h
@@ -0,0 +1,546 @@
+/*
+ *  Copyright (c) 2018, Alliance for Open Media. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#ifndef AOM_AOM_DSP_ARM_MEM_NEON_H_
+#define AOM_AOM_DSP_ARM_MEM_NEON_H_
+
+#include <arm_neon.h>
+#include <string.h>
+// #include "aom_dsp_common.h"
+
+static INLINE void store_row2_u8_8x8(uint8_t *s, int p, const uint8x8_t s0,
+                                     const uint8x8_t s1) {
+  vst1_u8(s, s0);
+  s += p;
+  vst1_u8(s, s1);
+  s += p;
+}
+
+/* These intrinsics require immediate values, so we must use #defines
+   to enforce that. */
+#define load_u8_4x1(s, s0, lane)                                           \
+  do {                                                                     \
+    *(s0) = vreinterpret_u8_u32(                                           \
+        vld1_lane_u32((uint32_t *)(s), vreinterpret_u32_u8(*(s0)), lane)); \
+  } while (0)
+
+static INLINE void load_u8_8x8(const uint8_t *s, ptrdiff_t p,
+                               uint8x8_t *const s0, uint8x8_t *const s1,
+                               uint8x8_t *const s2, uint8x8_t *const s3,
+                               uint8x8_t *const s4, uint8x8_t *const s5,
+                               uint8x8_t *const s6, uint8x8_t *const s7) {
+  *s0 = vld1_u8(s);
+  s += p;
+  *s1 = vld1_u8(s);
+  s += p;
+  *s2 = vld1_u8(s);
+  s += p;
+  *s3 = vld1_u8(s);
+  s += p;
+  *s4 = vld1_u8(s);
+  s += p;
+  *s5 = vld1_u8(s);
+  s += p;
+  *s6 = vld1_u8(s);
+  s += p;
+  *s7 = vld1_u8(s);
+}
+
+static INLINE void load_u8_8x16(const uint8_t *s, ptrdiff_t p,
+                                uint8x16_t *const s0, uint8x16_t *const s1,
+                                uint8x16_t *const s2, uint8x16_t *const s3) {
+  *s0 = vld1q_u8(s);
+  s += p;
+  *s1 = vld1q_u8(s);
+  s += p;
+  *s2 = vld1q_u8(s);
+  s += p;
+  *s3 = vld1q_u8(s);
+}
+
+static INLINE void load_u8_8x4(const uint8_t *s, const ptrdiff_t p,
+                               uint8x8_t *const s0, uint8x8_t *const s1,
+                               uint8x8_t *const s2, uint8x8_t *const s3) {
+  *s0 = vld1_u8(s);
+  s += p;
+  *s1 = vld1_u8(s);
+  s += p;
+  *s2 = vld1_u8(s);
+  s += p;
+  *s3 = vld1_u8(s);
+}
+
+static INLINE void load_u16_4x4(const uint16_t *s, const ptrdiff_t p,
+                                uint16x4_t *const s0, uint16x4_t *const s1,
+                                uint16x4_t *const s2, uint16x4_t *const s3) {
+  *s0 = vld1_u16(s);
+  s += p;
+  *s1 = vld1_u16(s);
+  s += p;
+  *s2 = vld1_u16(s);
+  s += p;
+  *s3 = vld1_u16(s);
+  s += p;
+}
+
+static INLINE void load_u16_8x4(const uint16_t *s, const ptrdiff_t p,
+                                uint16x8_t *const s0, uint16x8_t *const s1,
+                                uint16x8_t *const s2, uint16x8_t *const s3) {
+  *s0 = vld1q_u16(s);
+  s += p;
+  *s1 = vld1q_u16(s);
+  s += p;
+  *s2 = vld1q_u16(s);
+  s += p;
+  *s3 = vld1q_u16(s);
+  s += p;
+}
+
+static INLINE void load_s16_4x8(const int16_t *s, ptrdiff_t p,
+                                int16x4_t *const s0, int16x4_t *const s1,
+                                int16x4_t *const s2, int16x4_t *const s3,
+                                int16x4_t *const s4, int16x4_t *const s5,
+                                int16x4_t *const s6, int16x4_t *const s7) {
+  *s0 = vld1_s16(s);
+  s += p;
+  *s1 = vld1_s16(s);
+  s += p;
+  *s2 = vld1_s16(s);
+  s += p;
+  *s3 = vld1_s16(s);
+  s += p;
+  *s4 = vld1_s16(s);
+  s += p;
+  *s5 = vld1_s16(s);
+  s += p;
+  *s6 = vld1_s16(s);
+  s += p;
+  *s7 = vld1_s16(s);
+}
+
+static INLINE void load_s16_4x4(const int16_t *s, ptrdiff_t p,
+                                int16x4_t *const s0, int16x4_t *const s1,
+                                int16x4_t *const s2, int16x4_t *const s3) {
+  *s0 = vld1_s16(s);
+  s += p;
+  *s1 = vld1_s16(s);
+  s += p;
+  *s2 = vld1_s16(s);
+  s += p;
+  *s3 = vld1_s16(s);
+}
+
+/* These intrinsics require immediate values, so we must use #defines
+   to enforce that. */
+#define store_u8_4x1(s, s0, lane)                                  \
+  do {                                                             \
+    vst1_lane_u32((uint32_t *)(s), vreinterpret_u32_u8(s0), lane); \
+  } while (0)
+
+static INLINE void store_u8_8x8(uint8_t *s, ptrdiff_t p, const uint8x8_t s0,
+                                const uint8x8_t s1, const uint8x8_t s2,
+                                const uint8x8_t s3, const uint8x8_t s4,
+                                const uint8x8_t s5, const uint8x8_t s6,
+                                const uint8x8_t s7) {
+  vst1_u8(s, s0);
+  s += p;
+  vst1_u8(s, s1);
+  s += p;
+  vst1_u8(s, s2);
+  s += p;
+  vst1_u8(s, s3);
+  s += p;
+  vst1_u8(s, s4);
+  s += p;
+  vst1_u8(s, s5);
+  s += p;
+  vst1_u8(s, s6);
+  s += p;
+  vst1_u8(s, s7);
+}
+
+static INLINE void store_u8_8x4(uint8_t *s, ptrdiff_t p, const uint8x8_t s0,
+                                const uint8x8_t s1, const uint8x8_t s2,
+                                const uint8x8_t s3) {
+  vst1_u8(s, s0);
+  s += p;
+  vst1_u8(s, s1);
+  s += p;
+  vst1_u8(s, s2);
+  s += p;
+  vst1_u8(s, s3);
+}
+
+static INLINE void store_u8_8x16(uint8_t *s, ptrdiff_t p, const uint8x16_t s0,
+                                 const uint8x16_t s1, const uint8x16_t s2,
+                                 const uint8x16_t s3) {
+  vst1q_u8(s, s0);
+  s += p;
+  vst1q_u8(s, s1);
+  s += p;
+  vst1q_u8(s, s2);
+  s += p;
+  vst1q_u8(s, s3);
+}
+
+static INLINE void store_u16_8x8(uint16_t *s, ptrdiff_t dst_stride,
+                                 const uint16x8_t s0, const uint16x8_t s1,
+                                 const uint16x8_t s2, const uint16x8_t s3,
+                                 const uint16x8_t s4, const uint16x8_t s5,
+                                 const uint16x8_t s6, const uint16x8_t s7) {
+  vst1q_u16(s, s0);
+  s += dst_stride;
+  vst1q_u16(s, s1);
+  s += dst_stride;
+  vst1q_u16(s, s2);
+  s += dst_stride;
+  vst1q_u16(s, s3);
+  s += dst_stride;
+  vst1q_u16(s, s4);
+  s += dst_stride;
+  vst1q_u16(s, s5);
+  s += dst_stride;
+  vst1q_u16(s, s6);
+  s += dst_stride;
+  vst1q_u16(s, s7);
+}
+
+static INLINE void store_u16_4x4(uint16_t *s, ptrdiff_t dst_stride,
+                                 const uint16x4_t s0, const uint16x4_t s1,
+                                 const uint16x4_t s2, const uint16x4_t s3) {
+  vst1_u16(s, s0);
+  s += dst_stride;
+  vst1_u16(s, s1);
+  s += dst_stride;
+  vst1_u16(s, s2);
+  s += dst_stride;
+  vst1_u16(s, s3);
+}
+
+static INLINE void store_u16_8x4(uint16_t *s, ptrdiff_t dst_stride,
+                                 const uint16x8_t s0, const uint16x8_t s1,
+                                 const uint16x8_t s2, const uint16x8_t s3) {
+  vst1q_u16(s, s0);
+  s += dst_stride;
+  vst1q_u16(s, s1);
+  s += dst_stride;
+  vst1q_u16(s, s2);
+  s += dst_stride;
+  vst1q_u16(s, s3);
+}
+
+static INLINE void store_s16_8x8(int16_t *s, ptrdiff_t dst_stride,
+                                 const int16x8_t s0, const int16x8_t s1,
+                                 const int16x8_t s2, const int16x8_t s3,
+                                 const int16x8_t s4, const int16x8_t s5,
+                                 const int16x8_t s6, const int16x8_t s7) {
+  vst1q_s16(s, s0);
+  s += dst_stride;
+  vst1q_s16(s, s1);
+  s += dst_stride;
+  vst1q_s16(s, s2);
+  s += dst_stride;
+  vst1q_s16(s, s3);
+  s += dst_stride;
+  vst1q_s16(s, s4);
+  s += dst_stride;
+  vst1q_s16(s, s5);
+  s += dst_stride;
+  vst1q_s16(s, s6);
+  s += dst_stride;
+  vst1q_s16(s, s7);
+}
+
+static INLINE void store_s16_4x4(int16_t *s, ptrdiff_t dst_stride,
+                                 const int16x4_t s0, const int16x4_t s1,
+                                 const int16x4_t s2, const int16x4_t s3) {
+  vst1_s16(s, s0);
+  s += dst_stride;
+  vst1_s16(s, s1);
+  s += dst_stride;
+  vst1_s16(s, s2);
+  s += dst_stride;
+  vst1_s16(s, s3);
+}
+
+static INLINE void store_s16_8x4(int16_t *s, ptrdiff_t dst_stride,
+                                 const int16x8_t s0, const int16x8_t s1,
+                                 const int16x8_t s2, const int16x8_t s3) {
+  vst1q_s16(s, s0);
+  s += dst_stride;
+  vst1q_s16(s, s1);
+  s += dst_stride;
+  vst1q_s16(s, s2);
+  s += dst_stride;
+  vst1q_s16(s, s3);
+}
+
+static INLINE void load_s16_8x8(const int16_t *s, ptrdiff_t p,
+                                int16x8_t *const s0, int16x8_t *const s1,
+                                int16x8_t *const s2, int16x8_t *const s3,
+                                int16x8_t *const s4, int16x8_t *const s5,
+                                int16x8_t *const s6, int16x8_t *const s7) {
+  *s0 = vld1q_s16(s);
+  s += p;
+  *s1 = vld1q_s16(s);
+  s += p;
+  *s2 = vld1q_s16(s);
+  s += p;
+  *s3 = vld1q_s16(s);
+  s += p;
+  *s4 = vld1q_s16(s);
+  s += p;
+  *s5 = vld1q_s16(s);
+  s += p;
+  *s6 = vld1q_s16(s);
+  s += p;
+  *s7 = vld1q_s16(s);
+}
+
+static INLINE void load_s16_8x4(const int16_t *s, ptrdiff_t p,
+                                int16x8_t *const s0, int16x8_t *const s1,
+                                int16x8_t *const s2, int16x8_t *const s3) {
+  *s0 = vld1q_s16(s);
+  s += p;
+  *s1 = vld1q_s16(s);
+  s += p;
+  *s2 = vld1q_s16(s);
+  s += p;
+  *s3 = vld1q_s16(s);
+}
+
+// Load 4 sets of 4 bytes when alignment is not guaranteed.
+static INLINE uint8x16_t load_unaligned_u8q(const uint8_t *buf, int stride) {
+  uint32_t a;
+  uint32x4_t a_u32 = vdupq_n_u32(0);
+  if (stride == 4) return vld1q_u8(buf);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  a_u32 = vsetq_lane_u32(a, a_u32, 0);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  a_u32 = vsetq_lane_u32(a, a_u32, 1);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  a_u32 = vsetq_lane_u32(a, a_u32, 2);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  a_u32 = vsetq_lane_u32(a, a_u32, 3);
+  return vreinterpretq_u8_u32(a_u32);
+}
+
+static INLINE void load_unaligned_u8_4x8(const uint8_t *buf, int stride,
+                                         uint32x2_t *tu0, uint32x2_t *tu1,
+                                         uint32x2_t *tu2, uint32x2_t *tu3) {
+  uint32_t a;
+
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu0 = vset_lane_u32(a, *tu0, 0);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu0 = vset_lane_u32(a, *tu0, 1);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu1 = vset_lane_u32(a, *tu1, 0);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu1 = vset_lane_u32(a, *tu1, 1);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu2 = vset_lane_u32(a, *tu2, 0);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu2 = vset_lane_u32(a, *tu2, 1);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu3 = vset_lane_u32(a, *tu3, 0);
+  memcpy(&a, buf, 4);
+  *tu3 = vset_lane_u32(a, *tu3, 1);
+}
+
+static INLINE void load_unaligned_u8_4x4(const uint8_t *buf, int stride,
+                                         uint32x2_t *tu0, uint32x2_t *tu1) {
+  uint32_t a;
+
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu0 = vset_lane_u32(a, *tu0, 0);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu0 = vset_lane_u32(a, *tu0, 1);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu1 = vset_lane_u32(a, *tu1, 0);
+  memcpy(&a, buf, 4);
+  *tu1 = vset_lane_u32(a, *tu1, 1);
+}
+
+static INLINE void load_unaligned_u8_4x1(const uint8_t *buf, int stride,
+                                         uint32x2_t *tu0) {
+  uint32_t a;
+
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu0 = vset_lane_u32(a, *tu0, 0);
+}
+
+static INLINE void load_unaligned_u8_4x2(const uint8_t *buf, int stride,
+                                         uint32x2_t *tu0) {
+  uint32_t a;
+
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu0 = vset_lane_u32(a, *tu0, 0);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  *tu0 = vset_lane_u32(a, *tu0, 1);
+}
+
+/* These intrinsics require immediate values, so we must use #defines
+   to enforce that. */
+#define store_unaligned_u8_4x1(dst, src, lane)         \
+  do {                                                 \
+    uint32_t a;                                        \
+    a = vget_lane_u32(vreinterpret_u32_u8(src), lane); \
+    memcpy(dst, &a, 4);                                \
+  } while (0)
+
+#define store_unaligned_u8_2x1(dst, src, lane)         \
+  do {                                                 \
+    uint16_t a;                                        \
+    a = vget_lane_u16(vreinterpret_u16_u8(src), lane); \
+    memcpy(dst, &a, 2);                                \
+  } while (0)
+
+static INLINE void load_unaligned_u8_2x2(const uint8_t *buf, int stride,
+                                         uint16x4_t *tu0) {
+  uint16_t a;
+
+  memcpy(&a, buf, 2);
+  buf += stride;
+  *tu0 = vset_lane_u16(a, *tu0, 0);
+  memcpy(&a, buf, 2);
+  buf += stride;
+  *tu0 = vset_lane_u16(a, *tu0, 1);
+}
+
+static INLINE void load_u8_16x8(const uint8_t *s, ptrdiff_t p,
+                                uint8x16_t *const s0, uint8x16_t *const s1,
+                                uint8x16_t *const s2, uint8x16_t *const s3,
+                                uint8x16_t *const s4, uint8x16_t *const s5,
+                                uint8x16_t *const s6, uint8x16_t *const s7) {
+  *s0 = vld1q_u8(s);
+  s += p;
+  *s1 = vld1q_u8(s);
+  s += p;
+  *s2 = vld1q_u8(s);
+  s += p;
+  *s3 = vld1q_u8(s);
+  s += p;
+  *s4 = vld1q_u8(s);
+  s += p;
+  *s5 = vld1q_u8(s);
+  s += p;
+  *s6 = vld1q_u8(s);
+  s += p;
+  *s7 = vld1q_u8(s);
+}
+
+static INLINE void load_u8_16x4(const uint8_t *s, ptrdiff_t p,
+                                uint8x16_t *const s0, uint8x16_t *const s1,
+                                uint8x16_t *const s2, uint8x16_t *const s3) {
+  *s0 = vld1q_u8(s);
+  s += p;
+  *s1 = vld1q_u8(s);
+  s += p;
+  *s2 = vld1q_u8(s);
+  s += p;
+  *s3 = vld1q_u8(s);
+}
+
+static INLINE void load_unaligned_u16_4x4(const uint16_t *buf, uint32_t stride,
+                                          uint64x2_t *tu0, uint64x2_t *tu1) {
+  uint64_t a;
+
+  memcpy(&a, buf, 8);
+  buf += stride;
+  *tu0 = vsetq_lane_u64(a, *tu0, 0);
+  memcpy(&a, buf, 8);
+  buf += stride;
+  *tu0 = vsetq_lane_u64(a, *tu0, 1);
+  memcpy(&a, buf, 8);
+  buf += stride;
+  *tu1 = vsetq_lane_u64(a, *tu1, 0);
+  memcpy(&a, buf, 8);
+  *tu1 = vsetq_lane_u64(a, *tu1, 1);
+}
+
+static INLINE void load_s32_4x4(int32_t *s, int32_t p, int32x4_t *s1,
+                                int32x4_t *s2, int32x4_t *s3, int32x4_t *s4) {
+  *s1 = vld1q_s32(s);
+  s += p;
+  *s2 = vld1q_s32(s);
+  s += p;
+  *s3 = vld1q_s32(s);
+  s += p;
+  *s4 = vld1q_s32(s);
+}
+
+static INLINE void store_s32_4x4(int32_t *s, int32_t p, int32x4_t s1,
+                                 int32x4_t s2, int32x4_t s3, int32x4_t s4) {
+  vst1q_s32(s, s1);
+  s += p;
+  vst1q_s32(s, s2);
+  s += p;
+  vst1q_s32(s, s3);
+  s += p;
+  vst1q_s32(s, s4);
+}
+
+static INLINE void load_u32_4x4(uint32_t *s, int32_t p, uint32x4_t *s1,
+                                uint32x4_t *s2, uint32x4_t *s3,
+                                uint32x4_t *s4) {
+  *s1 = vld1q_u32(s);
+  s += p;
+  *s2 = vld1q_u32(s);
+  s += p;
+  *s3 = vld1q_u32(s);
+  s += p;
+  *s4 = vld1q_u32(s);
+}
+
+static INLINE void store_u32_4x4(uint32_t *s, int32_t p, uint32x4_t s1,
+                                 uint32x4_t s2, uint32x4_t s3, uint32x4_t s4) {
+  vst1q_u32(s, s1);
+  s += p;
+  vst1q_u32(s, s2);
+  s += p;
+  vst1q_u32(s, s3);
+  s += p;
+  vst1q_u32(s, s4);
+}
+
+// static INLINE int16x8_t load_tran_low_to_s16q(const tran_low_t *buf) {
+//   const int32x4_t v0 = vld1q_s32(buf);
+//   const int32x4_t v1 = vld1q_s32(buf + 4);
+//   const int16x4_t s0 = vmovn_s32(v0);
+//   const int16x4_t s1 = vmovn_s32(v1);
+//   return vcombine_s16(s0, s1);
+// }
+
+// static INLINE void store_s16q_to_tran_low(tran_low_t *buf, const int16x8_t a) {
+//   const int32x4_t v0 = vmovl_s16(vget_low_s16(a));
+//   const int32x4_t v1 = vmovl_s16(vget_high_s16(a));
+//   vst1q_s32(buf, v0);
+//   vst1q_s32(buf + 4, v1);
+// }
+
+#endif  // AOM_AOM_DSP_ARM_MEM_NEON_H_
diff --git a/Source/Lib/Common/ASM_NEON/transpose_neon.h b/Source/Lib/Common/ASM_NEON/transpose_neon.h
new file mode 100644
index 0000000..2650a20
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/transpose_neon.h
@@ -0,0 +1,755 @@
+/*
+ *  Copyright (c) 2018, Alliance for Open Media. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#ifndef AOM_AOM_DSP_ARM_TRANSPOSE_NEON_H_
+#define AOM_AOM_DSP_ARM_TRANSPOSE_NEON_H_
+
+#include <arm_neon.h>
+
+// Swap high and low halves.
+static INLINE uint16x8_t transpose64_u16q(const uint16x8_t a) {
+  return vextq_u16(a, a, 4);
+}
+
+static INLINE void transpose_u8_8x8(uint8x8_t *a0, uint8x8_t *a1, uint8x8_t *a2,
+                                    uint8x8_t *a3, uint8x8_t *a4, uint8x8_t *a5,
+                                    uint8x8_t *a6, uint8x8_t *a7) {
+  // Swap 8 bit elements. Goes from:
+  // a0: 00 01 02 03 04 05 06 07
+  // a1: 10 11 12 13 14 15 16 17
+  // a2: 20 21 22 23 24 25 26 27
+  // a3: 30 31 32 33 34 35 36 37
+  // a4: 40 41 42 43 44 45 46 47
+  // a5: 50 51 52 53 54 55 56 57
+  // a6: 60 61 62 63 64 65 66 67
+  // a7: 70 71 72 73 74 75 76 77
+  // to:
+  // b0.val[0]: 00 10 02 12 04 14 06 16  40 50 42 52 44 54 46 56
+  // b0.val[1]: 01 11 03 13 05 15 07 17  41 51 43 53 45 55 47 57
+  // b1.val[0]: 20 30 22 32 24 34 26 36  60 70 62 72 64 74 66 76
+  // b1.val[1]: 21 31 23 33 25 35 27 37  61 71 63 73 65 75 67 77
+
+  const uint8x16x2_t b0 =
+      vtrnq_u8(vcombine_u8(*a0, *a4), vcombine_u8(*a1, *a5));
+  const uint8x16x2_t b1 =
+      vtrnq_u8(vcombine_u8(*a2, *a6), vcombine_u8(*a3, *a7));
+
+  // Swap 16 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30 04 14 24 34  40 50 60 70 44 54 64 74
+  // c0.val[1]: 02 12 22 32 06 16 26 36  42 52 62 72 46 56 66 76
+  // c1.val[0]: 01 11 21 31 05 15 25 35  41 51 61 71 45 55 65 75
+  // c1.val[1]: 03 13 23 33 07 17 27 37  43 53 63 73 47 57 67 77
+
+  const uint16x8x2_t c0 = vtrnq_u16(vreinterpretq_u16_u8(b0.val[0]),
+                                    vreinterpretq_u16_u8(b1.val[0]));
+  const uint16x8x2_t c1 = vtrnq_u16(vreinterpretq_u16_u8(b0.val[1]),
+                                    vreinterpretq_u16_u8(b1.val[1]));
+
+  // Unzip 32 bit elements resulting in:
+  // d0.val[0]: 00 10 20 30 40 50 60 70  01 11 21 31 41 51 61 71
+  // d0.val[1]: 04 14 24 34 44 54 64 74  05 15 25 35 45 55 65 75
+  // d1.val[0]: 02 12 22 32 42 52 62 72  03 13 23 33 43 53 63 73
+  // d1.val[1]: 06 16 26 36 46 56 66 76  07 17 27 37 47 57 67 77
+  const uint32x4x2_t d0 = vuzpq_u32(vreinterpretq_u32_u16(c0.val[0]),
+                                    vreinterpretq_u32_u16(c1.val[0]));
+  const uint32x4x2_t d1 = vuzpq_u32(vreinterpretq_u32_u16(c0.val[1]),
+                                    vreinterpretq_u32_u16(c1.val[1]));
+
+  *a0 = vreinterpret_u8_u32(vget_low_u32(d0.val[0]));
+  *a1 = vreinterpret_u8_u32(vget_high_u32(d0.val[0]));
+  *a2 = vreinterpret_u8_u32(vget_low_u32(d1.val[0]));
+  *a3 = vreinterpret_u8_u32(vget_high_u32(d1.val[0]));
+  *a4 = vreinterpret_u8_u32(vget_low_u32(d0.val[1]));
+  *a5 = vreinterpret_u8_u32(vget_high_u32(d0.val[1]));
+  *a6 = vreinterpret_u8_u32(vget_low_u32(d1.val[1]));
+  *a7 = vreinterpret_u8_u32(vget_high_u32(d1.val[1]));
+}
+
+static INLINE void transpose_u8_8x4(uint8x8_t *a0, uint8x8_t *a1, uint8x8_t *a2,
+                                    uint8x8_t *a3) {
+  // Swap 8 bit elements. Goes from:
+  // a0: 00 01 02 03 04 05 06 07
+  // a1: 10 11 12 13 14 15 16 17
+  // a2: 20 21 22 23 24 25 26 27
+  // a3: 30 31 32 33 34 35 36 37
+  // to:
+  // b0.val[0]: 00 10 02 12 04 14 06 16
+  // b0.val[1]: 01 11 03 13 05 15 07 17
+  // b1.val[0]: 20 30 22 32 24 34 26 36
+  // b1.val[1]: 21 31 23 33 25 35 27 37
+
+  const uint8x8x2_t b0 = vtrn_u8(*a0, *a1);
+  const uint8x8x2_t b1 = vtrn_u8(*a2, *a3);
+
+  // Swap 16 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30 04 14 24 34
+  // c0.val[1]: 02 12 22 32 06 16 26 36
+  // c1.val[0]: 01 11 21 31 05 15 25 35
+  // c1.val[1]: 03 13 23 33 07 17 27 37
+
+  const uint16x4x2_t c0 =
+      vtrn_u16(vreinterpret_u16_u8(b0.val[0]), vreinterpret_u16_u8(b1.val[0]));
+  const uint16x4x2_t c1 =
+      vtrn_u16(vreinterpret_u16_u8(b0.val[1]), vreinterpret_u16_u8(b1.val[1]));
+
+  *a0 = vreinterpret_u8_u16(c0.val[0]);
+  *a1 = vreinterpret_u8_u16(c1.val[0]);
+  *a2 = vreinterpret_u8_u16(c0.val[1]);
+  *a3 = vreinterpret_u8_u16(c1.val[1]);
+}
+
+static INLINE void transpose_u8_4x4(uint8x8_t *a0, uint8x8_t *a1) {
+  // Swap 16 bit elements. Goes from:
+  // a0: 00 01 02 03  10 11 12 13
+  // a1: 20 21 22 23  30 31 32 33
+  // to:
+  // b0.val[0]: 00 01 20 21  10 11 30 31
+  // b0.val[1]: 02 03 22 23  12 13 32 33
+
+  const uint16x4x2_t b0 =
+      vtrn_u16(vreinterpret_u16_u8(*a0), vreinterpret_u16_u8(*a1));
+
+  // Swap 32 bit elements resulting in:
+  // c0.val[0]: 00 01 20 21  02 03 22 23
+  // c0.val[1]: 10 11 30 31  12 13 32 33
+
+  const uint32x2x2_t c0 = vtrn_u32(vreinterpret_u32_u16(b0.val[0]),
+                                   vreinterpret_u32_u16(b0.val[1]));
+
+  // Swap 8 bit elements resulting in:
+  // d0.val[0]: 00 10 20 30  02 12 22 32
+  // d0.val[1]: 01 11 21 31  03 13 23 33
+
+  const uint8x8x2_t d0 =
+      vtrn_u8(vreinterpret_u8_u32(c0.val[0]), vreinterpret_u8_u32(c0.val[1]));
+
+  *a0 = d0.val[0];
+  *a1 = d0.val[1];
+}
+
+static INLINE void transpose_u8_4x8(uint8x8_t *a0, uint8x8_t *a1, uint8x8_t *a2,
+                                    uint8x8_t *a3, const uint8x8_t a4,
+                                    const uint8x8_t a5, const uint8x8_t a6,
+                                    const uint8x8_t a7) {
+  // Swap 32 bit elements. Goes from:
+  // a0: 00 01 02 03 XX XX XX XX
+  // a1: 10 11 12 13 XX XX XX XX
+  // a2: 20 21 22 23 XX XX XX XX
+  // a3; 30 31 32 33 XX XX XX XX
+  // a4: 40 41 42 43 XX XX XX XX
+  // a5: 50 51 52 53 XX XX XX XX
+  // a6: 60 61 62 63 XX XX XX XX
+  // a7: 70 71 72 73 XX XX XX XX
+  // to:
+  // b0.val[0]: 00 01 02 03 40 41 42 43
+  // b1.val[0]: 10 11 12 13 50 51 52 53
+  // b2.val[0]: 20 21 22 23 60 61 62 63
+  // b3.val[0]: 30 31 32 33 70 71 72 73
+
+  const uint32x2x2_t b0 =
+      vtrn_u32(vreinterpret_u32_u8(*a0), vreinterpret_u32_u8(a4));
+  const uint32x2x2_t b1 =
+      vtrn_u32(vreinterpret_u32_u8(*a1), vreinterpret_u32_u8(a5));
+  const uint32x2x2_t b2 =
+      vtrn_u32(vreinterpret_u32_u8(*a2), vreinterpret_u32_u8(a6));
+  const uint32x2x2_t b3 =
+      vtrn_u32(vreinterpret_u32_u8(*a3), vreinterpret_u32_u8(a7));
+
+  // Swap 16 bit elements resulting in:
+  // c0.val[0]: 00 01 20 21 40 41 60 61
+  // c0.val[1]: 02 03 22 23 42 43 62 63
+  // c1.val[0]: 10 11 30 31 50 51 70 71
+  // c1.val[1]: 12 13 32 33 52 53 72 73
+
+  const uint16x4x2_t c0 = vtrn_u16(vreinterpret_u16_u32(b0.val[0]),
+                                   vreinterpret_u16_u32(b2.val[0]));
+  const uint16x4x2_t c1 = vtrn_u16(vreinterpret_u16_u32(b1.val[0]),
+                                   vreinterpret_u16_u32(b3.val[0]));
+
+  // Swap 8 bit elements resulting in:
+  // d0.val[0]: 00 10 20 30 40 50 60 70
+  // d0.val[1]: 01 11 21 31 41 51 61 71
+  // d1.val[0]: 02 12 22 32 42 52 62 72
+  // d1.val[1]: 03 13 23 33 43 53 63 73
+
+  const uint8x8x2_t d0 =
+      vtrn_u8(vreinterpret_u8_u16(c0.val[0]), vreinterpret_u8_u16(c1.val[0]));
+  const uint8x8x2_t d1 =
+      vtrn_u8(vreinterpret_u8_u16(c0.val[1]), vreinterpret_u8_u16(c1.val[1]));
+
+  *a0 = d0.val[0];
+  *a1 = d0.val[1];
+  *a2 = d1.val[0];
+  *a3 = d1.val[1];
+}
+
+// Input:
+// 00 01 02 03
+// 10 11 12 13
+// 20 21 22 23
+// 30 31 32 33
+// Output:
+// 00 10 20 30
+// 01 11 21 31
+// 02 12 22 32
+// 03 13 23 33
+static INLINE void transpose_u16_4x4(uint16x4_t a[4]) {
+  // b:
+  // 00 10 02 12
+  // 01 11 03 13
+  const uint16x4x2_t b = vtrn_u16(a[0], a[1]);
+  // c:
+  // 20 30 22 32
+  // 21 31 23 33
+  const uint16x4x2_t c = vtrn_u16(a[2], a[3]);
+  // d:
+  // 00 10 20 30
+  // 02 12 22 32
+  const uint32x2x2_t d =
+      vtrn_u32(vreinterpret_u32_u16(b.val[0]), vreinterpret_u32_u16(c.val[0]));
+  // e:
+  // 01 11 21 31
+  // 03 13 23 33
+  const uint32x2x2_t e =
+      vtrn_u32(vreinterpret_u32_u16(b.val[1]), vreinterpret_u32_u16(c.val[1]));
+  a[0] = vreinterpret_u16_u32(d.val[0]);
+  a[1] = vreinterpret_u16_u32(e.val[0]);
+  a[2] = vreinterpret_u16_u32(d.val[1]);
+  a[3] = vreinterpret_u16_u32(e.val[1]);
+}
+
+// 4x8 Input:
+// a[0]: 00 01 02 03 04 05 06 07
+// a[1]: 10 11 12 13 14 15 16 17
+// a[2]: 20 21 22 23 24 25 26 27
+// a[3]: 30 31 32 33 34 35 36 37
+// 8x4 Output:
+// a[0]: 00 10 20 30 04 14 24 34
+// a[1]: 01 11 21 31 05 15 25 35
+// a[2]: 02 12 22 32 06 16 26 36
+// a[3]: 03 13 23 33 07 17 27 37
+static INLINE void transpose_u16_4x8q(uint16x8_t a[4]) {
+  // b0.val[0]: 00 10 02 12 04 14 06 16
+  // b0.val[1]: 01 11 03 13 05 15 07 17
+  // b1.val[0]: 20 30 22 32 24 34 26 36
+  // b1.val[1]: 21 31 23 33 25 35 27 37
+  const uint16x8x2_t b0 = vtrnq_u16(a[0], a[1]);
+  const uint16x8x2_t b1 = vtrnq_u16(a[2], a[3]);
+
+  // c0.val[0]: 00 10 20 30 04 14 24 34
+  // c0.val[1]: 02 12 22 32 06 16 26 36
+  // c1.val[0]: 01 11 21 31 05 15 25 35
+  // c1.val[1]: 03 13 23 33 07 17 27 37
+  const uint32x4x2_t c0 = vtrnq_u32(vreinterpretq_u32_u16(b0.val[0]),
+                                    vreinterpretq_u32_u16(b1.val[0]));
+  const uint32x4x2_t c1 = vtrnq_u32(vreinterpretq_u32_u16(b0.val[1]),
+                                    vreinterpretq_u32_u16(b1.val[1]));
+
+  a[0] = vreinterpretq_u16_u32(c0.val[0]);
+  a[1] = vreinterpretq_u16_u32(c1.val[0]);
+  a[2] = vreinterpretq_u16_u32(c0.val[1]);
+  a[3] = vreinterpretq_u16_u32(c1.val[1]);
+}
+
+static INLINE uint16x8x2_t aom_vtrnq_u64_to_u16(const uint32x4_t a0,
+                                                const uint32x4_t a1) {
+  uint16x8x2_t b0;
+  b0.val[0] = vcombine_u16(vreinterpret_u16_u32(vget_low_u32(a0)),
+                           vreinterpret_u16_u32(vget_low_u32(a1)));
+  b0.val[1] = vcombine_u16(vreinterpret_u16_u32(vget_high_u32(a0)),
+                           vreinterpret_u16_u32(vget_high_u32(a1)));
+  return b0;
+}
+
+// Special transpose for loop filter.
+// 4x8 Input:
+// p_q:  p3 p2 p1 p0 q0 q1 q2 q3
+// a[0]: 00 01 02 03 04 05 06 07
+// a[1]: 10 11 12 13 14 15 16 17
+// a[2]: 20 21 22 23 24 25 26 27
+// a[3]: 30 31 32 33 34 35 36 37
+// 8x4 Output:
+// a[0]: 03 13 23 33 04 14 24 34  p0q0
+// a[1]: 02 12 22 32 05 15 25 35  p1q1
+// a[2]: 01 11 21 31 06 16 26 36  p2q2
+// a[3]: 00 10 20 30 07 17 27 37  p3q3
+// Direct reapplication of the function will reset the high halves, but
+// reverse the low halves:
+// p_q:  p0 p1 p2 p3 q0 q1 q2 q3
+// a[0]: 33 32 31 30 04 05 06 07
+// a[1]: 23 22 21 20 14 15 16 17
+// a[2]: 13 12 11 10 24 25 26 27
+// a[3]: 03 02 01 00 34 35 36 37
+// Simply reordering the inputs (3, 2, 1, 0) will reset the low halves, but
+// reverse the high halves.
+// The standard transpose_u16_4x8q will produce the same reversals, but with the
+// order of the low halves also restored relative to the high halves. This is
+// preferable because it puts all values from the same source row back together,
+// but some post-processing is inevitable.
+static INLINE void loop_filter_transpose_u16_4x8q(uint16x8_t a[4]) {
+  // b0.val[0]: 00 10 02 12 04 14 06 16
+  // b0.val[1]: 01 11 03 13 05 15 07 17
+  // b1.val[0]: 20 30 22 32 24 34 26 36
+  // b1.val[1]: 21 31 23 33 25 35 27 37
+  const uint16x8x2_t b0 = vtrnq_u16(a[0], a[1]);
+  const uint16x8x2_t b1 = vtrnq_u16(a[2], a[3]);
+
+  // Reverse odd vectors to bring the appropriate items to the front of zips.
+  // b0.val[0]: 00 10 02 12 04 14 06 16
+  // r0       : 03 13 01 11 07 17 05 15
+  // b1.val[0]: 20 30 22 32 24 34 26 36
+  // r1       : 23 33 21 31 27 37 25 35
+  const uint32x4_t r0 = vrev64q_u32(vreinterpretq_u32_u16(b0.val[1]));
+  const uint32x4_t r1 = vrev64q_u32(vreinterpretq_u32_u16(b1.val[1]));
+
+  // Zip to complete the halves.
+  // c0.val[0]: 00 10 20 30 02 12 22 32  p3p1
+  // c0.val[1]: 04 14 24 34 06 16 26 36  q0q2
+  // c1.val[0]: 03 13 23 33 01 11 21 31  p0p2
+  // c1.val[1]: 07 17 27 37 05 15 25 35  q3q1
+  const uint32x4x2_t c0 = vzipq_u32(vreinterpretq_u32_u16(b0.val[0]),
+                                    vreinterpretq_u32_u16(b1.val[0]));
+  const uint32x4x2_t c1 = vzipq_u32(r0, r1);
+
+  // d0.val[0]: 00 10 20 30 07 17 27 37  p3q3
+  // d0.val[1]: 02 12 22 32 05 15 25 35  p1q1
+  // d1.val[0]: 03 13 23 33 04 14 24 34  p0q0
+  // d1.val[1]: 01 11 21 31 06 16 26 36  p2q2
+  const uint16x8x2_t d0 = aom_vtrnq_u64_to_u16(c0.val[0], c1.val[1]);
+  // The third row of c comes first here to swap p2 with q0.
+  const uint16x8x2_t d1 = aom_vtrnq_u64_to_u16(c1.val[0], c0.val[1]);
+
+  // 8x4 Output:
+  // a[0]: 03 13 23 33 04 14 24 34  p0q0
+  // a[1]: 02 12 22 32 05 15 25 35  p1q1
+  // a[2]: 01 11 21 31 06 16 26 36  p2q2
+  // a[3]: 00 10 20 30 07 17 27 37  p3q3
+  a[0] = d1.val[0];  // p0q0
+  a[1] = d0.val[1];  // p1q1
+  a[2] = d1.val[1];  // p2q2
+  a[3] = d0.val[0];  // p3q3
+}
+
+static INLINE void transpose_u16_4x8(uint16x4_t *a0, uint16x4_t *a1,
+                                     uint16x4_t *a2, uint16x4_t *a3,
+                                     uint16x4_t *a4, uint16x4_t *a5,
+                                     uint16x4_t *a6, uint16x4_t *a7,
+                                     uint16x8_t *o0, uint16x8_t *o1,
+                                     uint16x8_t *o2, uint16x8_t *o3) {
+  // Swap 16 bit elements. Goes from:
+  // a0: 00 01 02 03
+  // a1: 10 11 12 13
+  // a2: 20 21 22 23
+  // a3: 30 31 32 33
+  // a4: 40 41 42 43
+  // a5: 50 51 52 53
+  // a6: 60 61 62 63
+  // a7: 70 71 72 73
+  // to:
+  // b0.val[0]: 00 10 02 12
+  // b0.val[1]: 01 11 03 13
+  // b1.val[0]: 20 30 22 32
+  // b1.val[1]: 21 31 23 33
+  // b2.val[0]: 40 50 42 52
+  // b2.val[1]: 41 51 43 53
+  // b3.val[0]: 60 70 62 72
+  // b3.val[1]: 61 71 63 73
+
+  uint16x4x2_t b0 = vtrn_u16(*a0, *a1);
+  uint16x4x2_t b1 = vtrn_u16(*a2, *a3);
+  uint16x4x2_t b2 = vtrn_u16(*a4, *a5);
+  uint16x4x2_t b3 = vtrn_u16(*a6, *a7);
+
+  // Swap 32 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30
+  // c0.val[1]: 02 12 22 32
+  // c1.val[0]: 01 11 21 31
+  // c1.val[1]: 03 13 23 33
+  // c2.val[0]: 40 50 60 70
+  // c2.val[1]: 42 52 62 72
+  // c3.val[0]: 41 51 61 71
+  // c3.val[1]: 43 53 63 73
+
+  uint32x2x2_t c0 = vtrn_u32(vreinterpret_u32_u16(b0.val[0]),
+                             vreinterpret_u32_u16(b1.val[0]));
+  uint32x2x2_t c1 = vtrn_u32(vreinterpret_u32_u16(b0.val[1]),
+                             vreinterpret_u32_u16(b1.val[1]));
+  uint32x2x2_t c2 = vtrn_u32(vreinterpret_u32_u16(b2.val[0]),
+                             vreinterpret_u32_u16(b3.val[0]));
+  uint32x2x2_t c3 = vtrn_u32(vreinterpret_u32_u16(b2.val[1]),
+                             vreinterpret_u32_u16(b3.val[1]));
+
+  // Swap 64 bit elements resulting in:
+  // o0: 00 10 20 30 40 50 60 70
+  // o1: 01 11 21 31 41 51 61 71
+  // o2: 02 12 22 32 42 52 62 72
+  // o3: 03 13 23 33 43 53 63 73
+
+  *o0 = vcombine_u16(vreinterpret_u16_u32(c0.val[0]),
+                     vreinterpret_u16_u32(c2.val[0]));
+  *o1 = vcombine_u16(vreinterpret_u16_u32(c1.val[0]),
+                     vreinterpret_u16_u32(c3.val[0]));
+  *o2 = vcombine_u16(vreinterpret_u16_u32(c0.val[1]),
+                     vreinterpret_u16_u32(c2.val[1]));
+  *o3 = vcombine_u16(vreinterpret_u16_u32(c1.val[1]),
+                     vreinterpret_u16_u32(c3.val[1]));
+}
+
+static INLINE void transpose_s16_4x8(int16x4_t *a0, int16x4_t *a1,
+                                     int16x4_t *a2, int16x4_t *a3,
+                                     int16x4_t *a4, int16x4_t *a5,
+                                     int16x4_t *a6, int16x4_t *a7,
+                                     int16x8_t *o0, int16x8_t *o1,
+                                     int16x8_t *o2, int16x8_t *o3) {
+  // Swap 16 bit elements. Goes from:
+  // a0: 00 01 02 03
+  // a1: 10 11 12 13
+  // a2: 20 21 22 23
+  // a3: 30 31 32 33
+  // a4: 40 41 42 43
+  // a5: 50 51 52 53
+  // a6: 60 61 62 63
+  // a7: 70 71 72 73
+  // to:
+  // b0.val[0]: 00 10 02 12
+  // b0.val[1]: 01 11 03 13
+  // b1.val[0]: 20 30 22 32
+  // b1.val[1]: 21 31 23 33
+  // b2.val[0]: 40 50 42 52
+  // b2.val[1]: 41 51 43 53
+  // b3.val[0]: 60 70 62 72
+  // b3.val[1]: 61 71 63 73
+
+  int16x4x2_t b0 = vtrn_s16(*a0, *a1);
+  int16x4x2_t b1 = vtrn_s16(*a2, *a3);
+  int16x4x2_t b2 = vtrn_s16(*a4, *a5);
+  int16x4x2_t b3 = vtrn_s16(*a6, *a7);
+
+  // Swap 32 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30
+  // c0.val[1]: 02 12 22 32
+  // c1.val[0]: 01 11 21 31
+  // c1.val[1]: 03 13 23 33
+  // c2.val[0]: 40 50 60 70
+  // c2.val[1]: 42 52 62 72
+  // c3.val[0]: 41 51 61 71
+  // c3.val[1]: 43 53 63 73
+
+  int32x2x2_t c0 = vtrn_s32(vreinterpret_s32_s16(b0.val[0]),
+                            vreinterpret_s32_s16(b1.val[0]));
+  int32x2x2_t c1 = vtrn_s32(vreinterpret_s32_s16(b0.val[1]),
+                            vreinterpret_s32_s16(b1.val[1]));
+  int32x2x2_t c2 = vtrn_s32(vreinterpret_s32_s16(b2.val[0]),
+                            vreinterpret_s32_s16(b3.val[0]));
+  int32x2x2_t c3 = vtrn_s32(vreinterpret_s32_s16(b2.val[1]),
+                            vreinterpret_s32_s16(b3.val[1]));
+
+  // Swap 64 bit elements resulting in:
+  // o0: 00 10 20 30 40 50 60 70
+  // o1: 01 11 21 31 41 51 61 71
+  // o2: 02 12 22 32 42 52 62 72
+  // o3: 03 13 23 33 43 53 63 73
+
+  *o0 = vcombine_s16(vreinterpret_s16_s32(c0.val[0]),
+                     vreinterpret_s16_s32(c2.val[0]));
+  *o1 = vcombine_s16(vreinterpret_s16_s32(c1.val[0]),
+                     vreinterpret_s16_s32(c3.val[0]));
+  *o2 = vcombine_s16(vreinterpret_s16_s32(c0.val[1]),
+                     vreinterpret_s16_s32(c2.val[1]));
+  *o3 = vcombine_s16(vreinterpret_s16_s32(c1.val[1]),
+                     vreinterpret_s16_s32(c3.val[1]));
+}
+
+static INLINE void transpose_u16_8x8(uint16x8_t *a0, uint16x8_t *a1,
+                                     uint16x8_t *a2, uint16x8_t *a3,
+                                     uint16x8_t *a4, uint16x8_t *a5,
+                                     uint16x8_t *a6, uint16x8_t *a7) {
+  // Swap 16 bit elements. Goes from:
+  // a0: 00 01 02 03 04 05 06 07
+  // a1: 10 11 12 13 14 15 16 17
+  // a2: 20 21 22 23 24 25 26 27
+  // a3: 30 31 32 33 34 35 36 37
+  // a4: 40 41 42 43 44 45 46 47
+  // a5: 50 51 52 53 54 55 56 57
+  // a6: 60 61 62 63 64 65 66 67
+  // a7: 70 71 72 73 74 75 76 77
+  // to:
+  // b0.val[0]: 00 10 02 12 04 14 06 16
+  // b0.val[1]: 01 11 03 13 05 15 07 17
+  // b1.val[0]: 20 30 22 32 24 34 26 36
+  // b1.val[1]: 21 31 23 33 25 35 27 37
+  // b2.val[0]: 40 50 42 52 44 54 46 56
+  // b2.val[1]: 41 51 43 53 45 55 47 57
+  // b3.val[0]: 60 70 62 72 64 74 66 76
+  // b3.val[1]: 61 71 63 73 65 75 67 77
+
+  const uint16x8x2_t b0 = vtrnq_u16(*a0, *a1);
+  const uint16x8x2_t b1 = vtrnq_u16(*a2, *a3);
+  const uint16x8x2_t b2 = vtrnq_u16(*a4, *a5);
+  const uint16x8x2_t b3 = vtrnq_u16(*a6, *a7);
+
+  // Swap 32 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30 04 14 24 34
+  // c0.val[1]: 02 12 22 32 06 16 26 36
+  // c1.val[0]: 01 11 21 31 05 15 25 35
+  // c1.val[1]: 03 13 23 33 07 17 27 37
+  // c2.val[0]: 40 50 60 70 44 54 64 74
+  // c2.val[1]: 42 52 62 72 46 56 66 76
+  // c3.val[0]: 41 51 61 71 45 55 65 75
+  // c3.val[1]: 43 53 63 73 47 57 67 77
+
+  const uint32x4x2_t c0 = vtrnq_u32(vreinterpretq_u32_u16(b0.val[0]),
+                                    vreinterpretq_u32_u16(b1.val[0]));
+  const uint32x4x2_t c1 = vtrnq_u32(vreinterpretq_u32_u16(b0.val[1]),
+                                    vreinterpretq_u32_u16(b1.val[1]));
+  const uint32x4x2_t c2 = vtrnq_u32(vreinterpretq_u32_u16(b2.val[0]),
+                                    vreinterpretq_u32_u16(b3.val[0]));
+  const uint32x4x2_t c3 = vtrnq_u32(vreinterpretq_u32_u16(b2.val[1]),
+                                    vreinterpretq_u32_u16(b3.val[1]));
+
+  *a0 = vcombine_u16(vget_low_u16(vreinterpretq_u16_u32(c0.val[0])),
+                     vget_low_u16(vreinterpretq_u16_u32(c2.val[0])));
+  *a4 = vcombine_u16(vget_high_u16(vreinterpretq_u16_u32(c0.val[0])),
+                     vget_high_u16(vreinterpretq_u16_u32(c2.val[0])));
+
+  *a2 = vcombine_u16(vget_low_u16(vreinterpretq_u16_u32(c0.val[1])),
+                     vget_low_u16(vreinterpretq_u16_u32(c2.val[1])));
+  *a6 = vcombine_u16(vget_high_u16(vreinterpretq_u16_u32(c0.val[1])),
+                     vget_high_u16(vreinterpretq_u16_u32(c2.val[1])));
+
+  *a1 = vcombine_u16(vget_low_u16(vreinterpretq_u16_u32(c1.val[0])),
+                     vget_low_u16(vreinterpretq_u16_u32(c3.val[0])));
+  *a5 = vcombine_u16(vget_high_u16(vreinterpretq_u16_u32(c1.val[0])),
+                     vget_high_u16(vreinterpretq_u16_u32(c3.val[0])));
+
+  *a3 = vcombine_u16(vget_low_u16(vreinterpretq_u16_u32(c1.val[1])),
+                     vget_low_u16(vreinterpretq_u16_u32(c3.val[1])));
+  *a7 = vcombine_u16(vget_high_u16(vreinterpretq_u16_u32(c1.val[1])),
+                     vget_high_u16(vreinterpretq_u16_u32(c3.val[1])));
+}
+
+static INLINE void transpose_s16_8x8(int16x8_t *a0, int16x8_t *a1,
+                                     int16x8_t *a2, int16x8_t *a3,
+                                     int16x8_t *a4, int16x8_t *a5,
+                                     int16x8_t *a6, int16x8_t *a7) {
+  // Swap 16 bit elements. Goes from:
+  // a0: 00 01 02 03 04 05 06 07
+  // a1: 10 11 12 13 14 15 16 17
+  // a2: 20 21 22 23 24 25 26 27
+  // a3: 30 31 32 33 34 35 36 37
+  // a4: 40 41 42 43 44 45 46 47
+  // a5: 50 51 52 53 54 55 56 57
+  // a6: 60 61 62 63 64 65 66 67
+  // a7: 70 71 72 73 74 75 76 77
+  // to:
+  // b0.val[0]: 00 10 02 12 04 14 06 16
+  // b0.val[1]: 01 11 03 13 05 15 07 17
+  // b1.val[0]: 20 30 22 32 24 34 26 36
+  // b1.val[1]: 21 31 23 33 25 35 27 37
+  // b2.val[0]: 40 50 42 52 44 54 46 56
+  // b2.val[1]: 41 51 43 53 45 55 47 57
+  // b3.val[0]: 60 70 62 72 64 74 66 76
+  // b3.val[1]: 61 71 63 73 65 75 67 77
+
+  const int16x8x2_t b0 = vtrnq_s16(*a0, *a1);
+  const int16x8x2_t b1 = vtrnq_s16(*a2, *a3);
+  const int16x8x2_t b2 = vtrnq_s16(*a4, *a5);
+  const int16x8x2_t b3 = vtrnq_s16(*a6, *a7);
+
+  // Swap 32 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30 04 14 24 34
+  // c0.val[1]: 02 12 22 32 06 16 26 36
+  // c1.val[0]: 01 11 21 31 05 15 25 35
+  // c1.val[1]: 03 13 23 33 07 17 27 37
+  // c2.val[0]: 40 50 60 70 44 54 64 74
+  // c2.val[1]: 42 52 62 72 46 56 66 76
+  // c3.val[0]: 41 51 61 71 45 55 65 75
+  // c3.val[1]: 43 53 63 73 47 57 67 77
+
+  const int32x4x2_t c0 = vtrnq_s32(vreinterpretq_s32_s16(b0.val[0]),
+                                   vreinterpretq_s32_s16(b1.val[0]));
+  const int32x4x2_t c1 = vtrnq_s32(vreinterpretq_s32_s16(b0.val[1]),
+                                   vreinterpretq_s32_s16(b1.val[1]));
+  const int32x4x2_t c2 = vtrnq_s32(vreinterpretq_s32_s16(b2.val[0]),
+                                   vreinterpretq_s32_s16(b3.val[0]));
+  const int32x4x2_t c3 = vtrnq_s32(vreinterpretq_s32_s16(b2.val[1]),
+                                   vreinterpretq_s32_s16(b3.val[1]));
+
+  *a0 = vcombine_s16(vget_low_s16(vreinterpretq_s16_s32(c0.val[0])),
+                     vget_low_s16(vreinterpretq_s16_s32(c2.val[0])));
+  *a4 = vcombine_s16(vget_high_s16(vreinterpretq_s16_s32(c0.val[0])),
+                     vget_high_s16(vreinterpretq_s16_s32(c2.val[0])));
+
+  *a2 = vcombine_s16(vget_low_s16(vreinterpretq_s16_s32(c0.val[1])),
+                     vget_low_s16(vreinterpretq_s16_s32(c2.val[1])));
+  *a6 = vcombine_s16(vget_high_s16(vreinterpretq_s16_s32(c0.val[1])),
+                     vget_high_s16(vreinterpretq_s16_s32(c2.val[1])));
+
+  *a1 = vcombine_s16(vget_low_s16(vreinterpretq_s16_s32(c1.val[0])),
+                     vget_low_s16(vreinterpretq_s16_s32(c3.val[0])));
+  *a5 = vcombine_s16(vget_high_s16(vreinterpretq_s16_s32(c1.val[0])),
+                     vget_high_s16(vreinterpretq_s16_s32(c3.val[0])));
+
+  *a3 = vcombine_s16(vget_low_s16(vreinterpretq_s16_s32(c1.val[1])),
+                     vget_low_s16(vreinterpretq_s16_s32(c3.val[1])));
+  *a7 = vcombine_s16(vget_high_s16(vreinterpretq_s16_s32(c1.val[1])),
+                     vget_high_s16(vreinterpretq_s16_s32(c3.val[1])));
+}
+
+static INLINE int16x8x2_t aom_vtrnq_s64_to_s16(int32x4_t a0, int32x4_t a1) {
+  int16x8x2_t b0;
+  b0.val[0] = vcombine_s16(vreinterpret_s16_s32(vget_low_s32(a0)),
+                           vreinterpret_s16_s32(vget_low_s32(a1)));
+  b0.val[1] = vcombine_s16(vreinterpret_s16_s32(vget_high_s32(a0)),
+                           vreinterpret_s16_s32(vget_high_s32(a1)));
+  return b0;
+}
+
+static INLINE void transpose_s16_8x8q(int16x8_t *a0, int16x8_t *out) {
+  // Swap 16 bit elements. Goes from:
+  // a0: 00 01 02 03 04 05 06 07
+  // a1: 10 11 12 13 14 15 16 17
+  // a2: 20 21 22 23 24 25 26 27
+  // a3: 30 31 32 33 34 35 36 37
+  // a4: 40 41 42 43 44 45 46 47
+  // a5: 50 51 52 53 54 55 56 57
+  // a6: 60 61 62 63 64 65 66 67
+  // a7: 70 71 72 73 74 75 76 77
+  // to:
+  // b0.val[0]: 00 10 02 12 04 14 06 16
+  // b0.val[1]: 01 11 03 13 05 15 07 17
+  // b1.val[0]: 20 30 22 32 24 34 26 36
+  // b1.val[1]: 21 31 23 33 25 35 27 37
+  // b2.val[0]: 40 50 42 52 44 54 46 56
+  // b2.val[1]: 41 51 43 53 45 55 47 57
+  // b3.val[0]: 60 70 62 72 64 74 66 76
+  // b3.val[1]: 61 71 63 73 65 75 67 77
+
+  const int16x8x2_t b0 = vtrnq_s16(*a0, *(a0 + 1));
+  const int16x8x2_t b1 = vtrnq_s16(*(a0 + 2), *(a0 + 3));
+  const int16x8x2_t b2 = vtrnq_s16(*(a0 + 4), *(a0 + 5));
+  const int16x8x2_t b3 = vtrnq_s16(*(a0 + 6), *(a0 + 7));
+
+  // Swap 32 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30 04 14 24 34
+  // c0.val[1]: 02 12 22 32 06 16 26 36
+  // c1.val[0]: 01 11 21 31 05 15 25 35
+  // c1.val[1]: 03 13 23 33 07 17 27 37
+  // c2.val[0]: 40 50 60 70 44 54 64 74
+  // c2.val[1]: 42 52 62 72 46 56 66 76
+  // c3.val[0]: 41 51 61 71 45 55 65 75
+  // c3.val[1]: 43 53 63 73 47 57 67 77
+
+  const int32x4x2_t c0 = vtrnq_s32(vreinterpretq_s32_s16(b0.val[0]),
+                                   vreinterpretq_s32_s16(b1.val[0]));
+  const int32x4x2_t c1 = vtrnq_s32(vreinterpretq_s32_s16(b0.val[1]),
+                                   vreinterpretq_s32_s16(b1.val[1]));
+  const int32x4x2_t c2 = vtrnq_s32(vreinterpretq_s32_s16(b2.val[0]),
+                                   vreinterpretq_s32_s16(b3.val[0]));
+  const int32x4x2_t c3 = vtrnq_s32(vreinterpretq_s32_s16(b2.val[1]),
+                                   vreinterpretq_s32_s16(b3.val[1]));
+
+  // Swap 64 bit elements resulting in:
+  // d0.val[0]: 00 10 20 30 40 50 60 70
+  // d0.val[1]: 04 14 24 34 44 54 64 74
+  // d1.val[0]: 01 11 21 31 41 51 61 71
+  // d1.val[1]: 05 15 25 35 45 55 65 75
+  // d2.val[0]: 02 12 22 32 42 52 62 72
+  // d2.val[1]: 06 16 26 36 46 56 66 76
+  // d3.val[0]: 03 13 23 33 43 53 63 73
+  // d3.val[1]: 07 17 27 37 47 57 67 77
+  const int16x8x2_t d0 = aom_vtrnq_s64_to_s16(c0.val[0], c2.val[0]);
+  const int16x8x2_t d1 = aom_vtrnq_s64_to_s16(c1.val[0], c3.val[0]);
+  const int16x8x2_t d2 = aom_vtrnq_s64_to_s16(c0.val[1], c2.val[1]);
+  const int16x8x2_t d3 = aom_vtrnq_s64_to_s16(c1.val[1], c3.val[1]);
+
+  *out = d0.val[0];
+  *(out + 1) = d1.val[0];
+  *(out + 2) = d2.val[0];
+  *(out + 3) = d3.val[0];
+  *(out + 4) = d0.val[1];
+  *(out + 5) = d1.val[1];
+  *(out + 6) = d2.val[1];
+  *(out + 7) = d3.val[1];
+}
+
+static INLINE void transpose_s16_4x4d(int16x4_t *a0, int16x4_t *a1,
+                                      int16x4_t *a2, int16x4_t *a3) {
+  // Swap 16 bit elements. Goes from:
+  // a0: 00 01 02 03
+  // a1: 10 11 12 13
+  // a2: 20 21 22 23
+  // a3: 30 31 32 33
+  // to:
+  // b0.val[0]: 00 10 02 12
+  // b0.val[1]: 01 11 03 13
+  // b1.val[0]: 20 30 22 32
+  // b1.val[1]: 21 31 23 33
+
+  const int16x4x2_t b0 = vtrn_s16(*a0, *a1);
+  const int16x4x2_t b1 = vtrn_s16(*a2, *a3);
+
+  // Swap 32 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30
+  // c0.val[1]: 02 12 22 32
+  // c1.val[0]: 01 11 21 31
+  // c1.val[1]: 03 13 23 33
+
+  const int32x2x2_t c0 = vtrn_s32(vreinterpret_s32_s16(b0.val[0]),
+                                  vreinterpret_s32_s16(b1.val[0]));
+  const int32x2x2_t c1 = vtrn_s32(vreinterpret_s32_s16(b0.val[1]),
+                                  vreinterpret_s32_s16(b1.val[1]));
+
+  *a0 = vreinterpret_s16_s32(c0.val[0]);
+  *a1 = vreinterpret_s16_s32(c1.val[0]);
+  *a2 = vreinterpret_s16_s32(c0.val[1]);
+  *a3 = vreinterpret_s16_s32(c1.val[1]);
+}
+
+static INLINE int32x4x2_t aom_vtrnq_s64_to_s32(int32x4_t a0, int32x4_t a1) {
+  int32x4x2_t b0;
+  b0.val[0] = vcombine_s32(vget_low_s32(a0), vget_low_s32(a1));
+  b0.val[1] = vcombine_s32(vget_high_s32(a0), vget_high_s32(a1));
+  return b0;
+}
+
+static INLINE void transpose_s32_4x4(int32x4_t *a0, int32x4_t *a1,
+                                     int32x4_t *a2, int32x4_t *a3) {
+  // Swap 32 bit elements. Goes from:
+  // a0: 00 01 02 03
+  // a1: 10 11 12 13
+  // a2: 20 21 22 23
+  // a3: 30 31 32 33
+  // to:
+  // b0.val[0]: 00 10 02 12
+  // b0.val[1]: 01 11 03 13
+  // b1.val[0]: 20 30 22 32
+  // b1.val[1]: 21 31 23 33
+
+  const int32x4x2_t b0 = vtrnq_s32(*a0, *a1);
+  const int32x4x2_t b1 = vtrnq_s32(*a2, *a3);
+
+  // Swap 64 bit elements resulting in:
+  // c0.val[0]: 00 10 20 30
+  // c0.val[1]: 02 12 22 32
+  // c1.val[0]: 01 11 21 31
+  // c1.val[1]: 03 13 23 33
+
+  const int32x4x2_t c0 = aom_vtrnq_s64_to_s32(b0.val[0], b1.val[0]);
+  const int32x4x2_t c1 = aom_vtrnq_s64_to_s32(b0.val[1], b1.val[1]);
+
+  *a0 = c0.val[0];
+  *a1 = c1.val[0];
+  *a2 = c0.val[1];
+  *a3 = c1.val[1];
+}
+
+#endif  // AOM_AOM_DSP_ARM_TRANSPOSE_NEON_H_
+
diff --git a/Source/Lib/Common/ASM_NEON/wiener_convolve_neon.c b/Source/Lib/Common/ASM_NEON/wiener_convolve_neon.c
new file mode 100644
index 0000000..2f38021
--- /dev/null
+++ b/Source/Lib/Common/ASM_NEON/wiener_convolve_neon.c
@@ -0,0 +1,544 @@
+/*
+ * Copyright (c) 2018, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <assert.h>
+#include "convolve.h"
+#include <arm_neon.h>
+
+#include "convolve_neon.h"
+#include "EbDefinitions.h"
+#include "common_dsp_rtcd.h"
+
+#include "mem_neon.h"
+#include "transpose_neon.h"
+
+// #include "config/aom_config.h"
+// #include "config/av1_rtcd.h"
+
+// #include "aom_dsp/txfm_common.h"
+// #include "aom_dsp/arm/mem_neon.h"
+// #include "aom_dsp/arm/transpose_neon.h"
+// #include "aom_ports/mem.h"
+// #include "av1/common/common.h"
+// #include "av1/common/arm/convolve_neon.h"
+
+/* Wiener filter 2D
+   Apply horizontal filter and store in a temporary buffer. When applying
+   vertical filter, overwrite the original pixel values.
+ */
+
+void svt_av1_wiener_convolve_add_src_neon(const uint8_t *const src, const ptrdiff_t src_stride, uint8_t *const dst, const ptrdiff_t dst_stride,
+                                          const int16_t *const filter_x, const int16_t *const filter_y,
+                                          const int32_t k_width, const int32_t k_height, const ConvolveParams *const conv_params)
+{
+// void svt_av1_wiener_convolve_add_src_neon(const uint8_t *src, ptrdiff_t src_stride,
+//                                       uint8_t *dst, ptrdiff_t dst_stride,
+//                                       const int16_t *filter_x, int x_step_q4,
+//                                       const int16_t *filter_y, int y_step_q4,
+//                                       int w, int h,
+//                                       const ConvolveParams *conv_params) {
+  uint16_t *d_tmp;
+  uint8_t *d;
+  const uint8_t *src_ptr, *s_tmp;
+  uint16_t *dst_ptr;
+  // (void)x_step_q4;
+  // (void)y_step_q4;
+  int32_t w = k_width;
+  int32_t h = k_height;
+  int width, height;
+  const int bd = 8;
+  const int intermediate_height = h + SUBPEL_TAPS - 1;
+  const int center_tap = ((SUBPEL_TAPS - 1) / 2);
+  int16_t filter_x_tmp[7], filter_y_tmp[7];
+
+  DECLARE_ALIGNED(16, uint16_t,
+                  temp[(MAX_SB_SIZE + HORIZ_EXTRA_ROWS) * MAX_SB_SIZE]);
+
+  assert(x_step_q4 == 16 && y_step_q4 == 16);
+  assert(!(w % 8));
+
+  assert(w <= MAX_SB_SIZE);
+  assert(h <= MAX_SB_SIZE);
+
+  assert(filter_x[7] == 0);
+  assert(filter_y[7] == 0);
+
+  /* assumption of horizontal filtering output will not exceed 15 bit.
+     ((bd) + 1 + FILTER_BITS - conv_params->round_0) <= 15
+     16 - conv_params->round_0 <= 15 -- (conv_params->round_0) >= 1
+   */
+  assert((conv_params->round_0) >= 1);
+
+  memcpy(&filter_x_tmp[0], filter_x, sizeof(*filter_x) * FILTER_BITS);
+  memcpy(&filter_y_tmp[0], filter_y, sizeof(*filter_y) * FILTER_BITS);
+
+  filter_x_tmp[3] += (1 << FILTER_BITS);
+  filter_y_tmp[3] += (1 << FILTER_BITS);
+
+  s_tmp = src - center_tap * src_stride - center_tap;
+  dst_ptr = temp;
+  src_ptr = s_tmp;
+  height = intermediate_height;
+
+  /* if height is a multiple of 8 */
+  if (!(h & 7)) {
+    int16x8_t res0, res1, res2, res3;
+    uint16x8_t res4;
+    uint8x8_t t0, t1, t2, t3, t4, t5, t6, t7;
+#if defined(__aarch64__)
+    uint16x8_t res5, res6, res7, res8, res9, res10, res11;
+    uint8x8_t t8, t9, t10, t11, t12, t13, t14;
+
+    do {
+      const uint8_t *s;
+
+      __builtin_prefetch(src_ptr + 0 * src_stride);
+      __builtin_prefetch(src_ptr + 1 * src_stride);
+      __builtin_prefetch(src_ptr + 2 * src_stride);
+      __builtin_prefetch(src_ptr + 3 * src_stride);
+      __builtin_prefetch(src_ptr + 4 * src_stride);
+      __builtin_prefetch(src_ptr + 5 * src_stride);
+      __builtin_prefetch(src_ptr + 6 * src_stride);
+      __builtin_prefetch(src_ptr + 7 * src_stride);
+
+      load_u8_8x8(src_ptr, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+      transpose_u8_8x8(&t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
+
+      s = src_ptr + 7;
+      d_tmp = dst_ptr;
+      width = w;
+
+      __builtin_prefetch(dst_ptr + 0 * dst_stride);
+      __builtin_prefetch(dst_ptr + 1 * dst_stride);
+      __builtin_prefetch(dst_ptr + 2 * dst_stride);
+      __builtin_prefetch(dst_ptr + 3 * dst_stride);
+      __builtin_prefetch(dst_ptr + 4 * dst_stride);
+      __builtin_prefetch(dst_ptr + 5 * dst_stride);
+      __builtin_prefetch(dst_ptr + 6 * dst_stride);
+      __builtin_prefetch(dst_ptr + 7 * dst_stride);
+
+      do {
+        load_u8_8x8(s, src_stride, &t7, &t8, &t9, &t10, &t11, &t12, &t13, &t14);
+        transpose_u8_8x8(&t7, &t8, &t9, &t10, &t11, &t12, &t13, &t14);
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(t0, t6));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t1, t5));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t2, t4));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+        res4 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                          bd, conv_params->round_0);
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(t1, t7));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t2, t6));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t3, t5));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t4));
+        res5 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                          bd, conv_params->round_0);
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(t2, t8));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t3, t7));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t4, t6));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t5));
+        res6 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                          bd, conv_params->round_0);
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(t3, t9));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t4, t8));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t5, t7));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t6));
+        res7 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                          bd, conv_params->round_0);
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(t4, t10));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t5, t9));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t6, t8));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t7));
+        res8 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                          bd, conv_params->round_0);
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(t5, t11));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t6, t10));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t7, t9));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t8));
+        res9 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                          bd, conv_params->round_0);
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(t6, t12));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t7, t11));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t8, t10));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t9));
+        res10 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                           bd, conv_params->round_0);
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(t7, t13));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t8, t12));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t9, t11));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t10));
+        res11 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                           bd, conv_params->round_0);
+
+        transpose_u16_8x8(&res4, &res5, &res6, &res7, &res8, &res9, &res10,
+                          &res11);
+        store_u16_8x8(d_tmp, MAX_SB_SIZE, res4, res5, res6, res7, res8, res9,
+                      res10, res11);
+
+        t0 = t8;
+        t1 = t9;
+        t2 = t10;
+        t3 = t11;
+        t4 = t12;
+        t5 = t13;
+        t6 = t14;
+        s += 8;
+        d_tmp += 8;
+        width -= 8;
+      } while (width > 0);
+      src_ptr += 8 * src_stride;
+      dst_ptr += 8 * MAX_SB_SIZE;
+      height -= 8;
+    } while (height > 0);
+#else
+    uint8x8_t temp_0;
+
+    do {
+      const uint8_t *s;
+
+      __builtin_prefetch(src_ptr);
+
+      t0 = vld1_u8(src_ptr);  // a0 a1 a2 a3 a4 a5 a6 a7
+      s = src_ptr + 8;
+      d_tmp = dst_ptr;
+      width = w;
+
+      __builtin_prefetch(dst_ptr);
+
+      do {
+        t7 = vld1_u8(s);  // a8 a9 a10 a11 a12 a13 a14 a15
+        temp_0 = t0;
+        t0 = t7;
+
+        t1 = vext_u8(temp_0, t7, 1);  // a1 a2 a3 a4 a5 a6 a7 a8
+        t2 = vext_u8(temp_0, t7, 2);  // a2 a3 a4 a5 a6 a7 a8 a9
+        t3 = vext_u8(temp_0, t7, 3);  // a3 a4 a5 a6 a7 a8 a9 a10
+        t4 = vext_u8(temp_0, t7, 4);  // a4 a5 a6 a7 a8 a9 a10 a11
+        t5 = vext_u8(temp_0, t7, 5);  // a5 a6 a7 a8 a9 a10 a11 a12
+        t6 = vext_u8(temp_0, t7, 6);  // a6 a7 a8 a9 a10 a11 a12 a13
+        t7 = vext_u8(temp_0, t7, 7);  // a7 a8 a9 a10 a11 a12 a13 a14
+
+        res0 = vreinterpretq_s16_u16(vaddl_u8(temp_0, t6));
+        res1 = vreinterpretq_s16_u16(vaddl_u8(t1, t5));
+        res2 = vreinterpretq_s16_u16(vaddl_u8(t2, t4));
+        res3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+        res4 = wiener_convolve8_horiz_8x8(res0, res1, res2, res3, filter_x_tmp,
+                                          bd, conv_params->round_0);
+
+        vst1q_u16(d_tmp, res4);
+
+        s += 8;
+        d_tmp += 8;
+        width -= 8;
+      } while (width > 0);
+      src_ptr += src_stride;
+      dst_ptr += MAX_SB_SIZE;
+      height--;
+    } while (height > 0);
+#endif
+  } else {
+    /*if height is a multiple of 4*/
+    const uint8_t *s;
+    int16x8_t tt0, tt1, tt2, tt3;
+    uint16x8_t d0;
+    uint8x8_t t0, t1, t2, t3;
+
+#if defined(__aarch64__)
+    uint16x4_t res0, res1, res2, res3, res4, res5, res6, res7;
+    uint16x8_t d1, d2, d3;
+    int16x4_t s0, s1, s2, s3, s4, s5, s6, s7, s8, s9, s10;
+    int16x4_t s11, s12, s13, s14;
+    do {
+      __builtin_prefetch(src_ptr + 0 * src_stride);
+      __builtin_prefetch(src_ptr + 1 * src_stride);
+      __builtin_prefetch(src_ptr + 2 * src_stride);
+      __builtin_prefetch(src_ptr + 3 * src_stride);
+
+      load_u8_8x4(src_ptr, src_stride, &t0, &t1, &t2, &t3); /*8x4*/
+      transpose_u8_8x4(&t0, &t1, &t2,
+                       &t3); /*first 8 pixels of 4 rows transposed-- 4x8*/
+
+      tt0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+      tt1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+      tt2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+      tt3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+
+      s0 = vget_low_s16(tt0);  /*pa0 pb0 pc0 pd0 -- pixel_a0*/
+      s1 = vget_low_s16(tt1);  /*pa1 pb1 pc1 pd1 */
+      s2 = vget_low_s16(tt2);  /*pa2 pb2 pc2 pd2 */
+      s3 = vget_low_s16(tt3);  /*pa3 pb3 pc3 pd3 */
+      s4 = vget_high_s16(tt0); /*pa4 pb4 pc4 pd4 */
+      s5 = vget_high_s16(tt1); /*pa5 pb5 pc5 pd5 */
+      s6 = vget_high_s16(tt2); /*pa6 pb6 pc6 pd6 */
+
+      __builtin_prefetch(dst_ptr + 0 * dst_stride);
+      __builtin_prefetch(dst_ptr + 1 * dst_stride);
+      __builtin_prefetch(dst_ptr + 2 * dst_stride);
+      __builtin_prefetch(dst_ptr + 3 * dst_stride);
+
+      s = src_ptr + 7;
+      d_tmp = dst_ptr;
+      width = w;
+
+      do {
+        load_u8_8x4(s, src_stride, &t0, &t1, &t2, &t3); /*8x4*/
+        transpose_u8_8x4(&t0, &t1, &t2, &t3);
+
+        tt0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+        tt1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+        tt2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+        tt3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+
+        s7 = vget_low_s16(tt0); /*pa7  pb7  pc7  pd7  */ /*4x8*/
+        s8 = vget_low_s16(tt1);   /*pa8  pb8  pc8  pd8  */
+        s9 = vget_low_s16(tt2);   /*pa9  pb9  pc9  pd9  */
+        s10 = vget_low_s16(tt3);  /*pa10 pb10 pc10 pd10 */
+        s11 = vget_high_s16(tt0); /*pa11 pb11 pc11 pd11 */
+        s12 = vget_high_s16(tt1); /*pa12 pb12 pc12 pd12 */
+        s13 = vget_high_s16(tt2); /*pa13 pb13 pc13 pd13 */
+        s14 = vget_high_s16(tt3); /*pa14 pb14 pc14 pd14 */
+
+        res0 = wiener_convolve8_horiz_4x8(
+            s0, s1, s2, s3, s4, s5, s6, filter_x_tmp, bd, conv_params->round_0);
+        res1 = wiener_convolve8_horiz_4x8(
+            s1, s2, s3, s4, s5, s6, s7, filter_x_tmp, bd, conv_params->round_0);
+        res2 = wiener_convolve8_horiz_4x8(
+            s2, s3, s4, s5, s6, s7, s8, filter_x_tmp, bd, conv_params->round_0);
+        res3 = wiener_convolve8_horiz_4x8(
+            s3, s4, s5, s6, s7, s8, s9, filter_x_tmp, bd, conv_params->round_0);
+        res4 =
+            wiener_convolve8_horiz_4x8(s4, s5, s6, s7, s8, s9, s10,
+                                       filter_x_tmp, bd, conv_params->round_0);
+        res5 =
+            wiener_convolve8_horiz_4x8(s5, s6, s7, s8, s9, s10, s11,
+                                       filter_x_tmp, bd, conv_params->round_0);
+        res6 =
+            wiener_convolve8_horiz_4x8(s6, s7, s8, s9, s10, s11, s12,
+                                       filter_x_tmp, bd, conv_params->round_0);
+        res7 =
+            wiener_convolve8_horiz_4x8(s7, s8, s9, s10, s11, s12, s13,
+                                       filter_x_tmp, bd, conv_params->round_0);
+
+        transpose_u16_4x8(&res0, &res1, &res2, &res3, &res4, &res5, &res6,
+                          &res7, &d0, &d1, &d2, &d3);
+
+        store_u16_8x4(d_tmp, MAX_SB_SIZE, d0, d1, d2, d3);
+
+        s0 = s8;
+        s1 = s9;
+        s2 = s10;
+        s3 = s11;
+        s4 = s12;
+        s5 = s13;
+        s6 = s14;
+        s += 8;
+        d_tmp += 8;
+        width -= 8;
+      } while (width > 0);
+
+      src_ptr += 4 * src_stride;
+      dst_ptr += 4 * MAX_SB_SIZE;
+      height -= 4;
+    } while (height > 0);
+#else
+    uint8x8_t temp_0, t4, t5, t6, t7;
+
+    do {
+      __builtin_prefetch(src_ptr);
+
+      t0 = vld1_u8(src_ptr);  // a0 a1 a2 a3 a4 a5 a6 a7
+
+      __builtin_prefetch(dst_ptr);
+
+      s = src_ptr + 8;
+      d_tmp = dst_ptr;
+      width = w;
+
+      do {
+        t7 = vld1_u8(s);  // a8 a9 a10 a11 a12 a13 a14 a15
+        temp_0 = t0;
+        t0 = t7;
+
+        t1 = vext_u8(temp_0, t7, 1);  // a1 a2 a3 a4 a5 a6 a7 a8
+        t2 = vext_u8(temp_0, t7, 2);  // a2 a3 a4 a5 a6 a7 a8 a9
+        t3 = vext_u8(temp_0, t7, 3);  // a3 a4 a5 a6 a7 a8 a9 a10
+        t4 = vext_u8(temp_0, t7, 4);  // a4 a5 a6 a7 a8 a9 a10 a11
+        t5 = vext_u8(temp_0, t7, 5);  // a5 a6 a7 a8 a9 a10 a11 a12
+        t6 = vext_u8(temp_0, t7, 6);  // a6 a7 a8 a9 a10 a11 a12 a13
+        t7 = vext_u8(temp_0, t7, 7);  // a7 a8 a9 a10 a11 a12 a13 a14
+
+        tt0 = vreinterpretq_s16_u16(vaddl_u8(temp_0, t6));
+        tt1 = vreinterpretq_s16_u16(vaddl_u8(t1, t5));
+        tt2 = vreinterpretq_s16_u16(vaddl_u8(t2, t4));
+        tt3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+        d0 = wiener_convolve8_horiz_8x8(tt0, tt1, tt2, tt3, filter_x_tmp, bd,
+                                        conv_params->round_0);
+
+        vst1q_u16(d_tmp, d0);
+
+        s += 8;
+        d_tmp += 8;
+        width -= 8;
+      } while (width > 0);
+
+      src_ptr += src_stride;
+      dst_ptr += MAX_SB_SIZE;
+      height -= 1;
+    } while (height > 0);
+#endif
+  }
+
+  {
+    int16x8_t s0, s1, s2, s3, s4, s5, s6, s7;
+    uint8x8_t t0;
+#if defined(__aarch64__)
+    int16x8_t s8, s9, s10;
+    uint8x8_t t1, t2, t3;
+#endif
+    int16_t *src_tmp_ptr, *s;
+    uint8_t *dst_tmp_ptr;
+    height = h;
+    width = w;
+    src_tmp_ptr = (int16_t *)temp;
+    dst_tmp_ptr = dst;
+    int src_stride = MAX_SB_SIZE;
+
+    do {
+      s = src_tmp_ptr;
+      s0 = vld1q_s16(s);
+      s += src_stride;
+      s1 = vld1q_s16(s);
+      s += src_stride;
+      s2 = vld1q_s16(s);
+      s += src_stride;
+      s3 = vld1q_s16(s);
+      s += src_stride;
+      s4 = vld1q_s16(s);
+      s += src_stride;
+      s5 = vld1q_s16(s);
+      s += src_stride;
+      s6 = vld1q_s16(s);
+      s += src_stride;
+      d = dst_tmp_ptr;
+      height = h;
+
+#if defined(__aarch64__)
+      do {
+        __builtin_prefetch(dst_tmp_ptr + 0 * dst_stride);
+        __builtin_prefetch(dst_tmp_ptr + 1 * dst_stride);
+        __builtin_prefetch(dst_tmp_ptr + 2 * dst_stride);
+        __builtin_prefetch(dst_tmp_ptr + 3 * dst_stride);
+
+        s7 = vld1q_s16(s);
+        s += src_stride;
+        s8 = vld1q_s16(s);
+        s += src_stride;
+        s9 = vld1q_s16(s);
+        s += src_stride;
+        s10 = vld1q_s16(s);
+        s += src_stride;
+
+        t0 = wiener_convolve8_vert_4x8(s0, s1, s2, s3, s4, s5, s6, filter_y_tmp,
+                                       bd, conv_params->round_1);
+        t1 = wiener_convolve8_vert_4x8(s1, s2, s3, s4, s5, s6, s7, filter_y_tmp,
+                                       bd, conv_params->round_1);
+        t2 = wiener_convolve8_vert_4x8(s2, s3, s4, s5, s6, s7, s8, filter_y_tmp,
+                                       bd, conv_params->round_1);
+        t3 = wiener_convolve8_vert_4x8(s3, s4, s5, s6, s7, s8, s9, filter_y_tmp,
+                                       bd, conv_params->round_1);
+
+        vst1_u8(d, t0);
+        d += dst_stride;
+        vst1_u8(d, t1);
+        d += dst_stride;
+        vst1_u8(d, t2);
+        d += dst_stride;
+        vst1_u8(d, t3);
+        d += dst_stride;
+
+        s0 = s4;
+        s1 = s5;
+        s2 = s6;
+        s3 = s7;
+        s4 = s8;
+        s5 = s9;
+        s6 = s10;
+        height -= 4;
+      } while (height > 3);
+
+      if (height != 0) {
+        __builtin_prefetch(dst_tmp_ptr + 0 * dst_stride);
+        __builtin_prefetch(dst_tmp_ptr + 1 * dst_stride);
+
+        do {
+          s7 = vld1q_s16(s);
+          s += src_stride;
+
+          t0 =
+              wiener_convolve8_vert_4x8(s0, s1, s2, s3, s4, s5, s6,
+                                        filter_y_tmp, bd, conv_params->round_1);
+          vst1_u8(d, t0);
+          d += dst_stride;
+
+          s0 = s1;
+          s1 = s2;
+          s2 = s3;
+          s3 = s4;
+          s4 = s5;
+          s5 = s6;
+          s6 = s7;
+          height -= 1;
+        } while (height > 0);
+      }
+
+      src_tmp_ptr += 8;
+      dst_tmp_ptr += 8;
+
+      w -= 8;
+    } while (w > 0);
+#else
+      do {
+        __builtin_prefetch(dst_tmp_ptr + 0 * dst_stride);
+
+        s7 = vld1q_s16(s);
+        s += src_stride;
+
+        t0 = wiener_convolve8_vert_4x8(s0, s1, s2, s3, s4, s5, s6, filter_y_tmp,
+                                       bd, conv_params->round_1);
+
+        vst1_u8(d, t0);
+        d += dst_stride;
+
+        s0 = s1;
+        s1 = s2;
+        s2 = s3;
+        s3 = s4;
+        s4 = s5;
+        s5 = s6;
+        s6 = s7;
+        height -= 1;
+      } while (height > 0);
+
+      src_tmp_ptr += 8;
+      dst_tmp_ptr += 8;
+
+      w -= 8;
+    } while (w > 0);
+#endif
+  }
+}
diff --git a/Source/Lib/Common/ASM_SSE2/CMakeLists.txt b/Source/Lib/Common/ASM_SSE2/CMakeLists.txt
index bedeaa8..c308db4 100644
--- a/Source/Lib/Common/ASM_SSE2/CMakeLists.txt
+++ b/Source/Lib/Common/ASM_SSE2/CMakeLists.txt
@@ -16,8 +16,9 @@ include_directories(${PROJECT_SOURCE_DIR}/Source/API/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Common/Codec/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Common/C_DEFAULT/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_SSE2/)
-
-check_both_flags_add(-msse2)
+if(NOT HAVE_AARCH64_PLATFORM)
+    check_both_flags_add(-msse2)
+endif()
 
 if(CMAKE_C_COMPILER_ID STREQUAL "Intel" AND NOT WIN32)
     check_both_flags_add(-static-intel -w)
@@ -46,12 +47,14 @@ set(all_files
     wiener_convolve_sse2.c
     )
 
-set(asm_files
-    intrapred_sse2.asm
-    EbPictureOperators_SSE2.asm
-    highbd_intrapred_sse2_.asm
-    aom_subpixel_8t_sse2.asm
-    subtract_sse2.asm)
+if(NOT HAVE_AARCH64_PLATFORM)
+    set(asm_files
+        intrapred_sse2.asm
+        EbPictureOperators_SSE2.asm
+        highbd_intrapred_sse2_.asm
+        aom_subpixel_8t_sse2.asm
+        subtract_sse2.asm)
+endif()
 
 add_library(COMMON_ASM_SSE2 OBJECT ${all_files})
 
diff --git a/Source/Lib/Common/ASM_SSE2/EbAvcStyleMcp_Intrinsic_SSE2.c b/Source/Lib/Common/ASM_SSE2/EbAvcStyleMcp_Intrinsic_SSE2.c
index eb6d36e..317b582 100644
--- a/Source/Lib/Common/ASM_SSE2/EbAvcStyleMcp_Intrinsic_SSE2.c
+++ b/Source/Lib/Common/ASM_SSE2/EbAvcStyleMcp_Intrinsic_SSE2.c
@@ -10,7 +10,7 @@
 */
 #include "EbDefinitions.h"
 #include "EbMcp_SSE2.h" // THIS SHOULD BE _SSE2 in the future
-#include <emmintrin.h>
+#include "simd.h"
 #include "common_dsp_rtcd.h"
 
 void svt_picture_average_kernel_sse2_intrin(EbByte src0, uint32_t src0_stride, EbByte src1, uint32_t src1_stride,
diff --git a/Source/Lib/Common/ASM_SSE2/EbDeblockingFilter_Intrinsic_SSE2.c b/Source/Lib/Common/ASM_SSE2/EbDeblockingFilter_Intrinsic_SSE2.c
index 594339e..5d0b9a9 100644
--- a/Source/Lib/Common/ASM_SSE2/EbDeblockingFilter_Intrinsic_SSE2.c
+++ b/Source/Lib/Common/ASM_SSE2/EbDeblockingFilter_Intrinsic_SSE2.c
@@ -11,7 +11,7 @@
  */
 
 #include "synonyms.h"
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 
 #include "EbDeblockingFilter_SSE2.h"
diff --git a/Source/Lib/Common/ASM_SSE2/EbHighbdIntraPrediction_SSE2.h b/Source/Lib/Common/ASM_SSE2/EbHighbdIntraPrediction_SSE2.h
index a8871ff..51c533d 100644
--- a/Source/Lib/Common/ASM_SSE2/EbHighbdIntraPrediction_SSE2.h
+++ b/Source/Lib/Common/ASM_SSE2/EbHighbdIntraPrediction_SSE2.h
@@ -12,7 +12,7 @@
 #ifndef EbHighbdIntraPrediction_SSE2_h
 #define EbHighbdIntraPrediction_SSE2_h
 
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
 
diff --git a/Source/Lib/Common/ASM_SSE2/EbIntraPrediction_AV1_Intrinsic_SSE2.c b/Source/Lib/Common/ASM_SSE2/EbIntraPrediction_AV1_Intrinsic_SSE2.c
index bb30fc7..39393ca 100644
--- a/Source/Lib/Common/ASM_SSE2/EbIntraPrediction_AV1_Intrinsic_SSE2.c
+++ b/Source/Lib/Common/ASM_SSE2/EbIntraPrediction_AV1_Intrinsic_SSE2.c
@@ -10,7 +10,7 @@
 */
 
 #include "EbDefinitions.h"
-#include <emmintrin.h>
+#include "simd.h"
 #include "common_dsp_rtcd.h"
 
 static INLINE __m128i dc_sum_16(const uint8_t *ref) {
diff --git a/Source/Lib/Common/ASM_SSE2/EbPackUnPack_Intrinsic_SSE2.c b/Source/Lib/Common/ASM_SSE2/EbPackUnPack_Intrinsic_SSE2.c
index 9d04bac..20ba8cf 100644
--- a/Source/Lib/Common/ASM_SSE2/EbPackUnPack_Intrinsic_SSE2.c
+++ b/Source/Lib/Common/ASM_SSE2/EbPackUnPack_Intrinsic_SSE2.c
@@ -10,7 +10,7 @@
 */
 
 #include "EbDefinitions.h"
-#include <emmintrin.h>
+#include "simd.h"
 #include <stdint.h>
 
 /****************************************************************************************
diff --git a/Source/Lib/Common/ASM_SSE2/EbPictureOperators_Intrinsic_SSE2.c b/Source/Lib/Common/ASM_SSE2/EbPictureOperators_Intrinsic_SSE2.c
index de0e117..d8f7dfc 100644
--- a/Source/Lib/Common/ASM_SSE2/EbPictureOperators_Intrinsic_SSE2.c
+++ b/Source/Lib/Common/ASM_SSE2/EbPictureOperators_Intrinsic_SSE2.c
@@ -10,7 +10,7 @@
 */
 
 #include "EbPictureOperators_SSE2.h"
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 
 /******************************************************************************************************
@@ -209,8 +209,8 @@ void svt_residual_kernel16bit_sse2_intrin(uint16_t *input, uint32_t input_stride
 * faster memcopy for <= 64B blocks, great w/ inlining and size known at compile time (or w/ PGO)
 * THIS NEEDS TO STAY IN A HEADER FOR BEST PERFORMANCE
 ********************************************************************************************/
-#ifdef ARCH_X86_64
-#include <immintrin.h>
+#if defined(ARCH_X86_64) || defined(ARCH_AARCH64)
+#include "simd.h"
 #if defined(__GNUC__) && !defined(__clang__) && !defined(__ICC__)
 __attribute__((optimize("unroll-loops")))
 #endif
diff --git a/Source/Lib/Common/ASM_SSE2/EbPictureOperators_SSE2.h b/Source/Lib/Common/ASM_SSE2/EbPictureOperators_SSE2.h
index 9b0d525..4861c19 100644
--- a/Source/Lib/Common/ASM_SSE2/EbPictureOperators_SSE2.h
+++ b/Source/Lib/Common/ASM_SSE2/EbPictureOperators_SSE2.h
@@ -12,7 +12,7 @@
 #ifndef EbPictureOperators_SSE2_h
 #define EbPictureOperators_SSE2_h
 
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 
 #ifdef __cplusplus
diff --git a/Source/Lib/Common/ASM_SSE2/av1_txfm_sse2.h b/Source/Lib/Common/ASM_SSE2/av1_txfm_sse2.h
index 1660c12..0f56cd8 100644
--- a/Source/Lib/Common/ASM_SSE2/av1_txfm_sse2.h
+++ b/Source/Lib/Common/ASM_SSE2/av1_txfm_sse2.h
@@ -11,7 +11,7 @@
 #ifndef AV1_COMMON_X86_AV1_TXFM_SSE2_H_
 #define AV1_COMMON_X86_AV1_TXFM_SSE2_H_
 
-#include <emmintrin.h> // SSE2
+#include "simd.h"
 
 #ifdef __cplusplus
 extern "C" {
diff --git a/Source/Lib/Common/ASM_SSE2/convolve_2d_sse2.c b/Source/Lib/Common/ASM_SSE2/convolve_2d_sse2.c
index 1df0a22..4654676 100644
--- a/Source/Lib/Common/ASM_SSE2/convolve_2d_sse2.c
+++ b/Source/Lib/Common/ASM_SSE2/convolve_2d_sse2.c
@@ -8,7 +8,7 @@
  * Media Patent License 1.0 was not distributed with this source code in the
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
 #include "filter.h"
diff --git a/Source/Lib/Common/ASM_SSE2/convolve_sse2.c b/Source/Lib/Common/ASM_SSE2/convolve_sse2.c
index 94d7535..e3d7630 100644
--- a/Source/Lib/Common/ASM_SSE2/convolve_sse2.c
+++ b/Source/Lib/Common/ASM_SSE2/convolve_sse2.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
 #include "filter.h"
diff --git a/Source/Lib/Common/ASM_SSE2/highbd_intrapred_sse2.c b/Source/Lib/Common/ASM_SSE2/highbd_intrapred_sse2.c
index c7fdad4..95f4f1f 100644
--- a/Source/Lib/Common/ASM_SSE2/highbd_intrapred_sse2.c
+++ b/Source/Lib/Common/ASM_SSE2/highbd_intrapred_sse2.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
  */
 
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbHighbdIntraPrediction_SSE2.h"
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
diff --git a/Source/Lib/Common/ASM_SSE2/highbd_subtract_sse2.c b/Source/Lib/Common/ASM_SSE2/highbd_subtract_sse2.c
index e6648d9..98701ee 100644
--- a/Source/Lib/Common/ASM_SSE2/highbd_subtract_sse2.c
+++ b/Source/Lib/Common/ASM_SSE2/highbd_subtract_sse2.c
@@ -10,7 +10,7 @@
  */
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
-#include <emmintrin.h>
+#include "simd.h"
 
 typedef void (*SubtractWxHFuncType)(int16_t *diff, ptrdiff_t diff_stride, const uint16_t *src, ptrdiff_t src_stride,
                                     const uint16_t *pred, ptrdiff_t pred_stride);
diff --git a/Source/Lib/Common/ASM_SSE2/jnt_convolve_2d_sse2.c b/Source/Lib/Common/ASM_SSE2/jnt_convolve_2d_sse2.c
index a5b51ee..8bc59d3 100644
--- a/Source/Lib/Common/ASM_SSE2/jnt_convolve_2d_sse2.c
+++ b/Source/Lib/Common/ASM_SSE2/jnt_convolve_2d_sse2.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
 #include "filter.h"
diff --git a/Source/Lib/Common/ASM_SSE2/jnt_convolve_sse2.c b/Source/Lib/Common/ASM_SSE2/jnt_convolve_sse2.c
index 8887708..18a36a9 100644
--- a/Source/Lib/Common/ASM_SSE2/jnt_convolve_sse2.c
+++ b/Source/Lib/Common/ASM_SSE2/jnt_convolve_sse2.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
 #include "filter.h"
diff --git a/Source/Lib/Common/ASM_SSE2/lpf_common_sse2.h b/Source/Lib/Common/ASM_SSE2/lpf_common_sse2.h
index c376b97..4b3774f 100644
--- a/Source/Lib/Common/ASM_SSE2/lpf_common_sse2.h
+++ b/Source/Lib/Common/ASM_SSE2/lpf_common_sse2.h
@@ -12,7 +12,7 @@
 #ifndef AOM_AOM_DSP_X86_LPF_COMMON_SSE2_H_
 #define AOM_AOM_DSP_X86_LPF_COMMON_SSE2_H_
 
-#include <emmintrin.h> // SSE2
+#include "simd.h"
 
 static INLINE void highbd_transpose6x6_sse2(__m128i *x0, __m128i *x1, __m128i *x2, __m128i *x3, __m128i *x4,
                                             __m128i *x5, __m128i *d0, __m128i *d1, __m128i *d2, __m128i *d3,
diff --git a/Source/Lib/Common/ASM_SSE2/synonyms.h b/Source/Lib/Common/ASM_SSE2/synonyms.h
index bb1e0b5..dba300b 100644
--- a/Source/Lib/Common/ASM_SSE2/synonyms.h
+++ b/Source/Lib/Common/ASM_SSE2/synonyms.h
@@ -12,7 +12,7 @@
 #ifndef AOM_DSP_X86_SYNONYMS_H_
 #define AOM_DSP_X86_SYNONYMS_H_
 
-#include <immintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
 //#define EB_TEST_SIMD_ALIGN
diff --git a/Source/Lib/Common/ASM_SSE2/transpose_sse2.h b/Source/Lib/Common/ASM_SSE2/transpose_sse2.h
index ff8826b..2a1e100 100644
--- a/Source/Lib/Common/ASM_SSE2/transpose_sse2.h
+++ b/Source/Lib/Common/ASM_SSE2/transpose_sse2.h
@@ -12,7 +12,7 @@
 #ifndef AOM_DSP_X86_TRANSPOSE_SSE2_H_
 #define AOM_DSP_X86_TRANSPOSE_SSE2_H_
 
-#include <emmintrin.h> // SSE2
+#include "simd.h"
 
 // nclude "./aom_config.h"
 
diff --git a/Source/Lib/Common/ASM_SSE2/wiener_convolve_sse2.c b/Source/Lib/Common/ASM_SSE2/wiener_convolve_sse2.c
index 34eb5de..e56adfd 100644
--- a/Source/Lib/Common/ASM_SSE2/wiener_convolve_sse2.c
+++ b/Source/Lib/Common/ASM_SSE2/wiener_convolve_sse2.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <emmintrin.h>
+#include "simd.h"
 #include "common_dsp_rtcd.h"
 #include "convolve.h"
 
diff --git a/Source/Lib/Common/ASM_SSE4_1/CMakeLists.txt b/Source/Lib/Common/ASM_SSE4_1/CMakeLists.txt
index f430bee..e40fb8a 100644
--- a/Source/Lib/Common/ASM_SSE4_1/CMakeLists.txt
+++ b/Source/Lib/Common/ASM_SSE4_1/CMakeLists.txt
@@ -21,7 +21,9 @@ include_directories(${PROJECT_SOURCE_DIR}/Source/API/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_AVX2/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_AVX512/)
 
-check_both_flags_add(-msse4.1)
+if(NOT HAVE_AARCH64_PLATFORM)
+    check_both_flags_add(-msse4.1)
+endif()
 
 if(CMAKE_C_COMPILER_ID STREQUAL "Intel" AND NOT WIN32)
     check_both_flags_add(-static-intel -w)
diff --git a/Source/Lib/Common/ASM_SSE4_1/EbBlend_a64_mask_sse4.c b/Source/Lib/Common/ASM_SSE4_1/EbBlend_a64_mask_sse4.c
index 252b644..0f884a5 100644
--- a/Source/Lib/Common/ASM_SSE4_1/EbBlend_a64_mask_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/EbBlend_a64_mask_sse4.c
@@ -11,7 +11,7 @@
  */
 
 #include <assert.h>
-#include <smmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 #include "EbBlend_sse4.h"
 #include "common_dsp_rtcd.h"
diff --git a/Source/Lib/Common/ASM_SSE4_1/EbBlend_sse4.h b/Source/Lib/Common/ASM_SSE4_1/EbBlend_sse4.h
index b3546b8..38f797c 100644
--- a/Source/Lib/Common/ASM_SSE4_1/EbBlend_sse4.h
+++ b/Source/Lib/Common/ASM_SSE4_1/EbBlend_sse4.h
@@ -15,7 +15,7 @@
 #include <assert.h>
 
 #include "EbDefinitions.h"
-#include <smmintrin.h>
+#include "simd.h"
 #include "synonyms.h"
 
 static const uint8_t g_blend_a64_mask_shuffle[32] = {
diff --git a/Source/Lib/Common/ASM_SSE4_1/EbIntraPrediction16bit_Intrinsic_SSE4_1.c b/Source/Lib/Common/ASM_SSE4_1/EbIntraPrediction16bit_Intrinsic_SSE4_1.c
index adda216..5f167ef 100644
--- a/Source/Lib/Common/ASM_SSE4_1/EbIntraPrediction16bit_Intrinsic_SSE4_1.c
+++ b/Source/Lib/Common/ASM_SSE4_1/EbIntraPrediction16bit_Intrinsic_SSE4_1.c
@@ -10,7 +10,7 @@
 */
 
 #include "EbDefinitions.h"
-#include <smmintrin.h>
+#include "simd.h"
 
 void svt_av1_filter_intra_edge_sse4_1(uint8_t *p, int32_t sz, int32_t strength) {
     if (!strength)
diff --git a/Source/Lib/Common/ASM_SSE4_1/EbMemory_SSE4_1.h b/Source/Lib/Common/ASM_SSE4_1/EbMemory_SSE4_1.h
index 539c48c..bf326a2 100644
--- a/Source/Lib/Common/ASM_SSE4_1/EbMemory_SSE4_1.h
+++ b/Source/Lib/Common/ASM_SSE4_1/EbMemory_SSE4_1.h
@@ -13,7 +13,7 @@
 #define EbMemory_SSE4_1_h
 
 #include "EbDefinitions.h"
-#include <smmintrin.h>
+#include "simd.h"
 
 static INLINE __m128i load8bit_4x2_sse4_1(const void *const src, const ptrdiff_t strideInByte) {
     const __m128i s = _mm_cvtsi32_si128(*(int32_t *)((uint8_t *)src));
diff --git a/Source/Lib/Common/ASM_SSE4_1/EbPictureOperators_Intrinsic_SSE4_1.c b/Source/Lib/Common/ASM_SSE4_1/EbPictureOperators_Intrinsic_SSE4_1.c
index 4f15f00..b8d4db6 100644
--- a/Source/Lib/Common/ASM_SSE4_1/EbPictureOperators_Intrinsic_SSE4_1.c
+++ b/Source/Lib/Common/ASM_SSE4_1/EbPictureOperators_Intrinsic_SSE4_1.c
@@ -9,7 +9,7 @@
 * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
 */
 
-#include <smmintrin.h>
+#include "simd.h"
 #include "common_dsp_rtcd.h"
 #include "EbPictureOperators_SSE2.h"
 #include "synonyms.h"
diff --git a/Source/Lib/Common/ASM_SSE4_1/av1_convolve_scale_sse4.c b/Source/Lib/Common/ASM_SSE4_1/av1_convolve_scale_sse4.c
index 275cc76..b282a8f 100644
--- a/Source/Lib/Common/ASM_SSE4_1/av1_convolve_scale_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/av1_convolve_scale_sse4.c
@@ -10,7 +10,7 @@
  */
 
 #include <assert.h>
-#include <smmintrin.h>
+#include "simd.h"
 #include "common_dsp_rtcd.h"
 #include "convolve.h"
 
diff --git a/Source/Lib/Common/ASM_SSE4_1/cdef_block_sse4_1.c b/Source/Lib/Common/ASM_SSE4_1/cdef_block_sse4_1.c
index e352bf9..a5c19bd 100644
--- a/Source/Lib/Common/ASM_SSE4_1/cdef_block_sse4_1.c
+++ b/Source/Lib/Common/ASM_SSE4_1/cdef_block_sse4_1.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 #include "EbDefinitions.h"
-#include <smmintrin.h>
+#include "simd.h"
 #include "EbCdef.h"
 #include "EbBitstreamUnit.h"
 
diff --git a/Source/Lib/Common/ASM_SSE4_1/filterintra_sse4.c b/Source/Lib/Common/ASM_SSE4_1/filterintra_sse4.c
index e1b46b4..b302166 100644
--- a/Source/Lib/Common/ASM_SSE4_1/filterintra_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/filterintra_sse4.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
  */
 
-#include <smmintrin.h>
+#include "simd.h"
 #include "common_dsp_rtcd.h"
 #include "synonyms.h"
 
diff --git a/Source/Lib/Common/ASM_SSE4_1/highbd_convolve_2d_sse4.c b/Source/Lib/Common/ASM_SSE4_1/highbd_convolve_2d_sse4.c
index 65d2583..3b5074c 100644
--- a/Source/Lib/Common/ASM_SSE4_1/highbd_convolve_2d_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/highbd_convolve_2d_sse4.c
@@ -9,8 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <tmmintrin.h>
-#include <smmintrin.h>
+#include "simd.h"
 #include <assert.h>
 
 #include "EbDefinitions.h"
diff --git a/Source/Lib/Common/ASM_SSE4_1/highbd_inv_txfm_sse4.c b/Source/Lib/Common/ASM_SSE4_1/highbd_inv_txfm_sse4.c
index 7e5038d..2dce391 100644
--- a/Source/Lib/Common/ASM_SSE4_1/highbd_inv_txfm_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/highbd_inv_txfm_sse4.c
@@ -11,7 +11,7 @@
 */
 
 #include <assert.h>
-#include <smmintrin.h> /* SSE4.1 */
+#include "simd.h" /* SSE4.1 */
 
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
diff --git a/Source/Lib/Common/ASM_SSE4_1/highbd_jnt_convolve_sse4.c b/Source/Lib/Common/ASM_SSE4_1/highbd_jnt_convolve_sse4.c
index b3d1486..e97d659 100644
--- a/Source/Lib/Common/ASM_SSE4_1/highbd_jnt_convolve_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/highbd_jnt_convolve_sse4.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <smmintrin.h>
+#include "simd.h"
 #include <assert.h>
 
 #include "EbDefinitions.h"
diff --git a/Source/Lib/Common/ASM_SSE4_1/highbd_txfm_utility_sse4.h b/Source/Lib/Common/ASM_SSE4_1/highbd_txfm_utility_sse4.h
index 841125e..7cbefdf 100644
--- a/Source/Lib/Common/ASM_SSE4_1/highbd_txfm_utility_sse4.h
+++ b/Source/Lib/Common/ASM_SSE4_1/highbd_txfm_utility_sse4.h
@@ -12,7 +12,7 @@
 #ifndef _HIGHBD_TXFM_UTILITY_SSE4_H
 #define _HIGHBD_TXFM_UTILITY_SSE4_H
 
-#include <smmintrin.h> /* SSE4.1 */
+#include "simd.h" /* SSE4.1 */
 
 #define TRANSPOSE_4X4(x0, x1, x2, x3, y0, y1, y2, y3) \
     do {                                              \
diff --git a/Source/Lib/Common/ASM_SSE4_1/reconinter_sse4.c b/Source/Lib/Common/ASM_SSE4_1/reconinter_sse4.c
index 787b48d..3c96aa1 100644
--- a/Source/Lib/Common/ASM_SSE4_1/reconinter_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/reconinter_sse4.c
@@ -9,8 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <emmintrin.h> // SSE2
-#include <smmintrin.h> /* SSE4.1 */
+#include "simd.h"
 
 #include "EbDefinitions.h"
 
diff --git a/Source/Lib/Common/ASM_SSE4_1/selfguided_sse4.c b/Source/Lib/Common/ASM_SSE4_1/selfguided_sse4.c
index 1ba5fca..31b7bb9 100644
--- a/Source/Lib/Common/ASM_SSE4_1/selfguided_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/selfguided_sse4.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <smmintrin.h>
+#include "simd.h"
 
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
diff --git a/Source/Lib/Common/ASM_SSE4_1/warp_plane_sse4.c b/Source/Lib/Common/ASM_SSE4_1/warp_plane_sse4.c
index bb9bc9e..c4939fc 100644
--- a/Source/Lib/Common/ASM_SSE4_1/warp_plane_sse4.c
+++ b/Source/Lib/Common/ASM_SSE4_1/warp_plane_sse4.c
@@ -9,14 +9,175 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <emmintrin.h>
-#include <smmintrin.h>
+#include "simd.h"
 
 #include "common_dsp_rtcd.h"
 #include "EbWarpedMotion.h"
-
+#ifdef ARCH_AARCH64
+DECLARE_ALIGNED(8, const int8_t,
+eb_av1_filter_8bit[WARPEDPIXEL_PREC_SHIFTS * 3 + 1][8]) = {
+#if WARPEDPIXEL_PREC_BITS == 6
+        // [-1, 0)
+        { 0, 127,   0, 0,   0,   1, 0, 0}, { 0, 127,   0, 0,  -1,   2, 0, 0},
+        { 1, 127,  -1, 0,  -3,   4, 0, 0}, { 1, 126,  -2, 0,  -4,   6, 1, 0},
+        { 1, 126,  -3, 0,  -5,   8, 1, 0}, { 1, 125,  -4, 0,  -6,  11, 1, 0},
+        { 1, 124,  -4, 0,  -7,  13, 1, 0}, { 2, 123,  -5, 0,  -8,  15, 1, 0},
+        { 2, 122,  -6, 0,  -9,  18, 1, 0}, { 2, 121,  -6, 0, -10,  20, 1, 0},
+        { 2, 120,  -7, 0, -11,  22, 2, 0}, { 2, 119,  -8, 0, -12,  25, 2, 0},
+        { 3, 117,  -8, 0, -13,  27, 2, 0}, { 3, 116,  -9, 0, -13,  29, 2, 0},
+        { 3, 114, -10, 0, -14,  32, 3, 0}, { 3, 113, -10, 0, -15,  35, 2, 0},
+        { 3, 111, -11, 0, -15,  37, 3, 0}, { 3, 109, -11, 0, -16,  40, 3, 0},
+        { 3, 108, -12, 0, -16,  42, 3, 0}, { 4, 106, -13, 0, -17,  45, 3, 0},
+        { 4, 104, -13, 0, -17,  47, 3, 0}, { 4, 102, -14, 0, -17,  50, 3, 0},
+        { 4, 100, -14, 0, -17,  52, 3, 0}, { 4,  98, -15, 0, -18,  55, 4, 0},
+        { 4,  96, -15, 0, -18,  58, 3, 0}, { 4,  94, -16, 0, -18,  60, 4, 0},
+        { 4,  91, -16, 0, -18,  63, 4, 0}, { 4,  89, -16, 0, -18,  65, 4, 0},
+        { 4,  87, -17, 0, -18,  68, 4, 0}, { 4,  85, -17, 0, -18,  70, 4, 0},
+        { 4,  82, -17, 0, -18,  73, 4, 0}, { 4,  80, -17, 0, -18,  75, 4, 0},
+        { 4,  78, -18, 0, -18,  78, 4, 0}, { 4,  75, -18, 0, -17,  80, 4, 0},
+        { 4,  73, -18, 0, -17,  82, 4, 0}, { 4,  70, -18, 0, -17,  85, 4, 0},
+        { 4,  68, -18, 0, -17,  87, 4, 0}, { 4,  65, -18, 0, -16,  89, 4, 0},
+        { 4,  63, -18, 0, -16,  91, 4, 0}, { 4,  60, -18, 0, -16,  94, 4, 0},
+        { 3,  58, -18, 0, -15,  96, 4, 0}, { 4,  55, -18, 0, -15,  98, 4, 0},
+        { 3,  52, -17, 0, -14, 100, 4, 0}, { 3,  50, -17, 0, -14, 102, 4, 0},
+        { 3,  47, -17, 0, -13, 104, 4, 0}, { 3,  45, -17, 0, -13, 106, 4, 0},
+        { 3,  42, -16, 0, -12, 108, 3, 0}, { 3,  40, -16, 0, -11, 109, 3, 0},
+        { 3,  37, -15, 0, -11, 111, 3, 0}, { 2,  35, -15, 0, -10, 113, 3, 0},
+        { 3,  32, -14, 0, -10, 114, 3, 0}, { 2,  29, -13, 0,  -9, 116, 3, 0},
+        { 2,  27, -13, 0,  -8, 117, 3, 0}, { 2,  25, -12, 0,  -8, 119, 2, 0},
+        { 2,  22, -11, 0,  -7, 120, 2, 0}, { 1,  20, -10, 0,  -6, 121, 2, 0},
+        { 1,  18,  -9, 0,  -6, 122, 2, 0}, { 1,  15,  -8, 0,  -5, 123, 2, 0},
+        { 1,  13,  -7, 0,  -4, 124, 1, 0}, { 1,  11,  -6, 0,  -4, 125, 1, 0},
+        { 1,   8,  -5, 0,  -3, 126, 1, 0}, { 1,   6,  -4, 0,  -2, 126, 1, 0},
+        { 0,   4,  -3, 0,  -1, 127, 1, 0}, { 0,   2,  -1, 0,   0, 127, 0, 0},
+        // [0, 1)
+        { 0,   0,   1, 0, 0, 127,   0,  0}, { 0,  -1,   2, 0, 0, 127,   0,  0},
+        { 0,  -3,   4, 1, 1, 127,  -2,  0}, { 0,  -5,   6, 1, 1, 127,  -2,  0},
+        { 0,  -6,   8, 1, 2, 126,  -3,  0}, {-1,  -7,  11, 2, 2, 126,  -4, -1},
+        {-1,  -8,  13, 2, 3, 125,  -5, -1}, {-1, -10,  16, 3, 3, 124,  -6, -1},
+        {-1, -11,  18, 3, 4, 123,  -7, -1}, {-1, -12,  20, 3, 4, 122,  -7, -1},
+        {-1, -13,  23, 3, 4, 121,  -8, -1}, {-2, -14,  25, 4, 5, 120,  -9, -1},
+        {-1, -15,  27, 4, 5, 119, -10, -1}, {-1, -16,  30, 4, 5, 118, -11, -1},
+        {-2, -17,  33, 5, 6, 116, -12, -1}, {-2, -17,  35, 5, 6, 114, -12, -1},
+        {-2, -18,  38, 5, 6, 113, -13, -1}, {-2, -19,  41, 6, 7, 111, -14, -2},
+        {-2, -19,  43, 6, 7, 110, -15, -2}, {-2, -20,  46, 6, 7, 108, -15, -2},
+        {-2, -20,  49, 6, 7, 106, -16, -2}, {-2, -21,  51, 7, 7, 104, -16, -2},
+        {-2, -21,  54, 7, 7, 102, -17, -2}, {-2, -21,  56, 7, 8, 100, -18, -2},
+        {-2, -22,  59, 7, 8,  98, -18, -2}, {-2, -22,  62, 7, 8,  96, -19, -2},
+        {-2, -22,  64, 7, 8,  94, -19, -2}, {-2, -22,  67, 8, 8,  91, -20, -2},
+        {-2, -22,  69, 8, 8,  89, -20, -2}, {-2, -22,  72, 8, 8,  87, -21, -2},
+        {-2, -21,  74, 8, 8,  84, -21, -2}, {-2, -22,  77, 8, 8,  82, -21, -2},
+        {-2, -21,  79, 8, 8,  79, -21, -2}, {-2, -21,  82, 8, 8,  77, -22, -2},
+        {-2, -21,  84, 8, 8,  74, -21, -2}, {-2, -21,  87, 8, 8,  72, -22, -2},
+        {-2, -20,  89, 8, 8,  69, -22, -2}, {-2, -20,  91, 8, 8,  67, -22, -2},
+        {-2, -19,  94, 8, 7,  64, -22, -2}, {-2, -19,  96, 8, 7,  62, -22, -2},
+        {-2, -18,  98, 8, 7,  59, -22, -2}, {-2, -18, 100, 8, 7,  56, -21, -2},
+        {-2, -17, 102, 7, 7,  54, -21, -2}, {-2, -16, 104, 7, 7,  51, -21, -2},
+        {-2, -16, 106, 7, 6,  49, -20, -2}, {-2, -15, 108, 7, 6,  46, -20, -2},
+        {-2, -15, 110, 7, 6,  43, -19, -2}, {-2, -14, 111, 7, 6,  41, -19, -2},
+        {-1, -13, 113, 6, 5,  38, -18, -2}, {-1, -12, 114, 6, 5,  35, -17, -2},
+        {-1, -12, 116, 6, 5,  33, -17, -2}, {-1, -11, 118, 5, 4,  30, -16, -1},
+        {-1, -10, 119, 5, 4,  27, -15, -1}, {-1,  -9, 120, 5, 4,  25, -14, -2},
+        {-1,  -8, 121, 4, 3,  23, -13, -1}, {-1,  -7, 122, 4, 3,  20, -12, -1},
+        {-1,  -7, 123, 4, 3,  18, -11, -1}, {-1,  -6, 124, 3, 3,  16, -10, -1},
+        {-1,  -5, 125, 3, 2,  13,  -8, -1}, {-1,  -4, 126, 2, 2,  11,  -7, -1},
+        { 0,  -3, 126, 2, 1,   8,  -6,  0}, { 0,  -2, 127, 1, 1,   6,  -5,  0},
+        { 0,  -2, 127, 1, 1,   4,  -3,  0}, { 0,   0, 127, 0, 0,   2,  -1,  0},
+        // [1, 2)
+        { 0, 0, 127,   0, 0,   1,   0, 0}, { 0, 0, 127,   0, 0,  -1,   2, 0},
+        { 0, 1, 127,  -1, 0,  -3,   4, 0}, { 0, 1, 126,  -2, 0,  -4,   6, 1},
+        { 0, 1, 126,  -3, 0,  -5,   8, 1}, { 0, 1, 125,  -4, 0,  -6,  11, 1},
+        { 0, 1, 124,  -4, 0,  -7,  13, 1}, { 0, 2, 123,  -5, 0,  -8,  15, 1},
+        { 0, 2, 122,  -6, 0,  -9,  18, 1}, { 0, 2, 121,  -6, 0, -10,  20, 1},
+        { 0, 2, 120,  -7, 0, -11,  22, 2}, { 0, 2, 119,  -8, 0, -12,  25, 2},
+        { 0, 3, 117,  -8, 0, -13,  27, 2}, { 0, 3, 116,  -9, 0, -13,  29, 2},
+        { 0, 3, 114, -10, 0, -14,  32, 3}, { 0, 3, 113, -10, 0, -15,  35, 2},
+        { 0, 3, 111, -11, 0, -15,  37, 3}, { 0, 3, 109, -11, 0, -16,  40, 3},
+        { 0, 3, 108, -12, 0, -16,  42, 3}, { 0, 4, 106, -13, 0, -17,  45, 3},
+        { 0, 4, 104, -13, 0, -17,  47, 3}, { 0, 4, 102, -14, 0, -17,  50, 3},
+        { 0, 4, 100, -14, 0, -17,  52, 3}, { 0, 4,  98, -15, 0, -18,  55, 4},
+        { 0, 4,  96, -15, 0, -18,  58, 3}, { 0, 4,  94, -16, 0, -18,  60, 4},
+        { 0, 4,  91, -16, 0, -18,  63, 4}, { 0, 4,  89, -16, 0, -18,  65, 4},
+        { 0, 4,  87, -17, 0, -18,  68, 4}, { 0, 4,  85, -17, 0, -18,  70, 4},
+        { 0, 4,  82, -17, 0, -18,  73, 4}, { 0, 4,  80, -17, 0, -18,  75, 4},
+        { 0, 4,  78, -18, 0, -18,  78, 4}, { 0, 4,  75, -18, 0, -17,  80, 4},
+        { 0, 4,  73, -18, 0, -17,  82, 4}, { 0, 4,  70, -18, 0, -17,  85, 4},
+        { 0, 4,  68, -18, 0, -17,  87, 4}, { 0, 4,  65, -18, 0, -16,  89, 4},
+        { 0, 4,  63, -18, 0, -16,  91, 4}, { 0, 4,  60, -18, 0, -16,  94, 4},
+        { 0, 3,  58, -18, 0, -15,  96, 4}, { 0, 4,  55, -18, 0, -15,  98, 4},
+        { 0, 3,  52, -17, 0, -14, 100, 4}, { 0, 3,  50, -17, 0, -14, 102, 4},
+        { 0, 3,  47, -17, 0, -13, 104, 4}, { 0, 3,  45, -17, 0, -13, 106, 4},
+        { 0, 3,  42, -16, 0, -12, 108, 3}, { 0, 3,  40, -16, 0, -11, 109, 3},
+        { 0, 3,  37, -15, 0, -11, 111, 3}, { 0, 2,  35, -15, 0, -10, 113, 3},
+        { 0, 3,  32, -14, 0, -10, 114, 3}, { 0, 2,  29, -13, 0,  -9, 116, 3},
+        { 0, 2,  27, -13, 0,  -8, 117, 3}, { 0, 2,  25, -12, 0,  -8, 119, 2},
+        { 0, 2,  22, -11, 0,  -7, 120, 2}, { 0, 1,  20, -10, 0,  -6, 121, 2},
+        { 0, 1,  18,  -9, 0,  -6, 122, 2}, { 0, 1,  15,  -8, 0,  -5, 123, 2},
+        { 0, 1,  13,  -7, 0,  -4, 124, 1}, { 0, 1,  11,  -6, 0,  -4, 125, 1},
+        { 0, 1,   8,  -5, 0,  -3, 126, 1}, { 0, 1,   6,  -4, 0,  -2, 126, 1},
+        { 0, 0,   4,  -3, 0,  -1, 127, 1}, { 0, 0,   2,  -1, 0,   0, 127, 0},
+        // dummy (replicate row index 191)
+        { 0, 0,   2,  -1, 0,   0, 127, 0},
+
+      #else
+        // [-1, 0)
+        { 0, 127,   0, 0,   0,   1, 0, 0}, { 1, 127,  -1, 0,  -3,   4, 0, 0},
+        { 1, 126,  -3, 0,  -5,   8, 1, 0}, { 1, 124,  -4, 0,  -7,  13, 1, 0},
+        { 2, 122,  -6, 0,  -9,  18, 1, 0}, { 2, 120,  -7, 0, -11,  22, 2, 0},
+        { 3, 117,  -8, 0, -13,  27, 2, 0}, { 3, 114, -10, 0, -14,  32, 3, 0},
+        { 3, 111, -11, 0, -15,  37, 3, 0}, { 3, 108, -12, 0, -16,  42, 3, 0},
+        { 4, 104, -13, 0, -17,  47, 3, 0}, { 4, 100, -14, 0, -17,  52, 3, 0},
+        { 4,  96, -15, 0, -18,  58, 3, 0}, { 4,  91, -16, 0, -18,  63, 4, 0},
+        { 4,  87, -17, 0, -18,  68, 4, 0}, { 4,  82, -17, 0, -18,  73, 4, 0},
+        { 4,  78, -18, 0, -18,  78, 4, 0}, { 4,  73, -18, 0, -17,  82, 4, 0},
+        { 4,  68, -18, 0, -17,  87, 4, 0}, { 4,  63, -18, 0, -16,  91, 4, 0},
+        { 3,  58, -18, 0, -15,  96, 4, 0}, { 3,  52, -17, 0, -14, 100, 4, 0},
+        { 3,  47, -17, 0, -13, 104, 4, 0}, { 3,  42, -16, 0, -12, 108, 3, 0},
+        { 3,  37, -15, 0, -11, 111, 3, 0}, { 3,  32, -14, 0, -10, 114, 3, 0},
+        { 2,  27, -13, 0,  -8, 117, 3, 0}, { 2,  22, -11, 0,  -7, 120, 2, 0},
+        { 1,  18,  -9, 0,  -6, 122, 2, 0}, { 1,  13,  -7, 0,  -4, 124, 1, 0},
+        { 1,   8,  -5, 0,  -3, 126, 1, 0}, { 0,   4,  -3, 0,  -1, 127, 1, 0},
+        // [0, 1)
+        { 0,   0,   1, 0, 0, 127,   0,  0}, { 0,  -3,   4, 1, 1, 127,  -2,  0},
+        { 0,  -6,   8, 1, 2, 126,  -3,  0}, {-1,  -8,  13, 2, 3, 125,  -5, -1},
+        {-1, -11,  18, 3, 4, 123,  -7, -1}, {-1, -13,  23, 3, 4, 121,  -8, -1},
+        {-1, -15,  27, 4, 5, 119, -10, -1}, {-2, -17,  33, 5, 6, 116, -12, -1},
+        {-2, -18,  38, 5, 6, 113, -13, -1}, {-2, -19,  43, 6, 7, 110, -15, -2},
+        {-2, -20,  49, 6, 7, 106, -16, -2}, {-2, -21,  54, 7, 7, 102, -17, -2},
+        {-2, -22,  59, 7, 8,  98, -18, -2}, {-2, -22,  64, 7, 8,  94, -19, -2},
+        {-2, -22,  69, 8, 8,  89, -20, -2}, {-2, -21,  74, 8, 8,  84, -21, -2},
+        {-2, -21,  79, 8, 8,  79, -21, -2}, {-2, -21,  84, 8, 8,  74, -21, -2},
+        {-2, -20,  89, 8, 8,  69, -22, -2}, {-2, -19,  94, 8, 7,  64, -22, -2},
+        {-2, -18,  98, 8, 7,  59, -22, -2}, {-2, -17, 102, 7, 7,  54, -21, -2},
+        {-2, -16, 106, 7, 6,  49, -20, -2}, {-2, -15, 110, 7, 6,  43, -19, -2},
+        {-1, -13, 113, 6, 5,  38, -18, -2}, {-1, -12, 116, 6, 5,  33, -17, -2},
+        {-1, -10, 119, 5, 4,  27, -15, -1}, {-1,  -8, 121, 4, 3,  23, -13, -1},
+        {-1,  -7, 123, 4, 3,  18, -11, -1}, {-1,  -5, 125, 3, 2,  13,  -8, -1},
+        { 0,  -3, 126, 2, 1,   8,  -6,  0}, { 0,  -2, 127, 1, 1,   4,  -3,  0},
+        // [1, 2)
+        { 0,  0, 127,   0, 0,   1,   0, 0}, { 0, 1, 127,  -1, 0,  -3,   4, 0},
+        { 0,  1, 126,  -3, 0,  -5,   8, 1}, { 0, 1, 124,  -4, 0,  -7,  13, 1},
+        { 0,  2, 122,  -6, 0,  -9,  18, 1}, { 0, 2, 120,  -7, 0, -11,  22, 2},
+        { 0,  3, 117,  -8, 0, -13,  27, 2}, { 0, 3, 114, -10, 0, -14,  32, 3},
+        { 0,  3, 111, -11, 0, -15,  37, 3}, { 0, 3, 108, -12, 0, -16,  42, 3},
+        { 0,  4, 104, -13, 0, -17,  47, 3}, { 0, 4, 100, -14, 0, -17,  52, 3},
+        { 0,  4,  96, -15, 0, -18,  58, 3}, { 0, 4,  91, -16, 0, -18,  63, 4},
+        { 0,  4,  87, -17, 0, -18,  68, 4}, { 0, 4,  82, -17, 0, -18,  73, 4},
+        { 0,  4,  78, -18, 0, -18,  78, 4}, { 0, 4,  73, -18, 0, -17,  82, 4},
+        { 0,  4,  68, -18, 0, -17,  87, 4}, { 0, 4,  63, -18, 0, -16,  91, 4},
+        { 0,  3,  58, -18, 0, -15,  96, 4}, { 0, 3,  52, -17, 0, -14, 100, 4},
+        { 0,  3,  47, -17, 0, -13, 104, 4}, { 0, 3,  42, -16, 0, -12, 108, 3},
+        { 0,  3,  37, -15, 0, -11, 111, 3}, { 0, 3,  32, -14, 0, -10, 114, 3},
+        { 0,  2,  27, -13, 0,  -8, 117, 3}, { 0, 2,  22, -11, 0,  -7, 120, 2},
+        { 0,  1,  18,  -9, 0,  -6, 122, 2}, { 0, 1,  13,  -7, 0,  -4, 124, 1},
+        { 0,  1,   8,  -5, 0,  -3, 126, 1}, { 0, 0,   4,  -3, 0,  -1, 127, 1},
+        // dummy (replicate row index 95)
+        { 0, 0,   4,  -3, 0,  -1, 127, 1},
+      #endif  // WARPEDPIXEL_PREC_BITS == 6
+};
+#else
 extern int8_t eb_av1_filter_8bit[WARPEDPIXEL_PREC_SHIFTS * 3 + 1][8];
-
+#endif
 // Shuffle masks: we want to convert a sequence of bytes 0, 1, 2, ..., 15
 // in an SSE register into two sequences:
 // 0, 2, 2, 4, ..., 12, 12, 14, <don't care>
diff --git a/Source/Lib/Common/ASM_SSSE3/CMakeLists.txt b/Source/Lib/Common/ASM_SSSE3/CMakeLists.txt
index 37d0166..e146ed5 100644
--- a/Source/Lib/Common/ASM_SSSE3/CMakeLists.txt
+++ b/Source/Lib/Common/ASM_SSSE3/CMakeLists.txt
@@ -20,7 +20,9 @@ include_directories(${PROJECT_SOURCE_DIR}/Source/API/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_AVX2/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_AVX512/)
 
-check_both_flags_add(-mssse3)
+if(NOT HAVE_AARCH64_PLATFORM)
+    check_both_flags_add(-mssse3)
+endif()
 
 if(CMAKE_C_COMPILER_ID STREQUAL "Intel" AND NOT WIN32)
     check_both_flags_add(-static-intel -w)
@@ -40,9 +42,11 @@ set(all_files
     reconinter_ssse3.c
     )
 
-set(asm_files
-    aom_subpixel_bilinear_ssse3.asm
-    aom_subpixel_8t_ssse3.asm)
+if(NOT HAVE_AARCH64_PLATFORM)
+    set(asm_files
+        aom_subpixel_bilinear_ssse3.asm
+        aom_subpixel_8t_ssse3.asm)
+endif()
 
 add_library(COMMON_ASM_SSSE3 OBJECT ${all_files})
 
diff --git a/Source/Lib/Common/ASM_SSSE3/EbHighbdIntraPrediction_Intrinsic_SSSE3.c b/Source/Lib/Common/ASM_SSSE3/EbHighbdIntraPrediction_Intrinsic_SSSE3.c
index d3a695a..3ab757d 100644
--- a/Source/Lib/Common/ASM_SSSE3/EbHighbdIntraPrediction_Intrinsic_SSSE3.c
+++ b/Source/Lib/Common/ASM_SSSE3/EbHighbdIntraPrediction_Intrinsic_SSSE3.c
@@ -9,7 +9,7 @@
 * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
 */
 
-#include <tmmintrin.h>
+#include "simd.h"
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
 
diff --git a/Source/Lib/Common/ASM_SSSE3/aom_subpixel_8t_intrin_ssse3.c b/Source/Lib/Common/ASM_SSSE3/aom_subpixel_8t_intrin_ssse3.c
index bd90e05..588f985 100644
--- a/Source/Lib/Common/ASM_SSSE3/aom_subpixel_8t_intrin_ssse3.c
+++ b/Source/Lib/Common/ASM_SSSE3/aom_subpixel_8t_intrin_ssse3.c
@@ -9,12 +9,12 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <tmmintrin.h>
+#include "simd.h"
 #include "common_dsp_rtcd.h"
 
 #include "convolve.h"
 #include "transpose_sse2.h"
-
+#ifndef ARCH_AARCH64
 // filters for 8_h8 and 16_h8
 DECLARE_ALIGNED(32, static const uint8_t, filt_h4[]) = {
     0, 1, 1, 2, 2, 3, 3, 4,  4,  5,  5,  6,  6,  7,  7,  8,  0, 1, 1, 2, 2, 3, 3, 4,  4,  5,  5,  6,  6,  7,  7,  8,
@@ -567,3 +567,4 @@ static void svt_aom_filter_block1d16_v4_ssse3(const uint8_t *src_ptr, ptrdiff_t
 
 FUN_CONV_1D(horiz, x_step_q4, filter_x, h, src, , ssse3);
 FUN_CONV_1D(vert, y_step_q4, filter_y, v, src - src_stride * 3, , ssse3);
+#endif
diff --git a/Source/Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.c b/Source/Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.c
index 27c9505..238c78f 100644
--- a/Source/Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.c
+++ b/Source/Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.c
@@ -11,7 +11,7 @@
 
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
-#include <tmmintrin.h>
+#include "simd.h"
 #include "EbInvTransforms.h"
 #include "av1_inv_txfm_ssse3.h"
 #include "av1_txfm_sse2.h"
diff --git a/Source/Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.h b/Source/Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.h
index a30fbe7..e2ec376 100644
--- a/Source/Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.h
+++ b/Source/Lib/Common/ASM_SSSE3/av1_inv_txfm_ssse3.h
@@ -11,8 +11,7 @@
 #ifndef AV1_COMMON_X86_AV1_INV_TXFM_SSSE3_H_
 #define AV1_COMMON_X86_AV1_INV_TXFM_SSSE3_H_
 
-#include <emmintrin.h> // SSE2
-#include <tmmintrin.h> // SSSE3
+#include "simd.h"
 
 #ifdef __cplusplus
 extern "C" {
diff --git a/Source/Lib/Common/ASM_SSSE3/highbd_convolve_2d_ssse3.c b/Source/Lib/Common/ASM_SSSE3/highbd_convolve_2d_ssse3.c
index dd4aa77..d016a98 100644
--- a/Source/Lib/Common/ASM_SSSE3/highbd_convolve_2d_ssse3.c
+++ b/Source/Lib/Common/ASM_SSSE3/highbd_convolve_2d_ssse3.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <tmmintrin.h>
+#include "simd.h"
 #include <assert.h>
 
 #include "EbDefinitions.h"
diff --git a/Source/Lib/Common/ASM_SSSE3/highbd_convolve_ssse3.c b/Source/Lib/Common/ASM_SSSE3/highbd_convolve_ssse3.c
index eaa00c4..8bd6031 100644
--- a/Source/Lib/Common/ASM_SSSE3/highbd_convolve_ssse3.c
+++ b/Source/Lib/Common/ASM_SSSE3/highbd_convolve_ssse3.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <tmmintrin.h>
+#include "simd.h"
 #include <assert.h>
 
 #include "EbDefinitions.h"
diff --git a/Source/Lib/Common/ASM_SSSE3/highbd_wiener_convolve_ssse3.c b/Source/Lib/Common/ASM_SSSE3/highbd_wiener_convolve_ssse3.c
index 5066207..6538988 100644
--- a/Source/Lib/Common/ASM_SSSE3/highbd_wiener_convolve_ssse3.c
+++ b/Source/Lib/Common/ASM_SSSE3/highbd_wiener_convolve_ssse3.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <tmmintrin.h>
+#include "simd.h"
 #include "common_dsp_rtcd.h"
 #include "convolve.h"
 //void svt_av1_highbd_wiener_convolve_add_src_ssse3(
diff --git a/Source/Lib/Common/ASM_SSSE3/intrapred_ssse3.c b/Source/Lib/Common/ASM_SSSE3/intrapred_ssse3.c
index f5851f7..0af4bac 100644
--- a/Source/Lib/Common/ASM_SSSE3/intrapred_ssse3.c
+++ b/Source/Lib/Common/ASM_SSSE3/intrapred_ssse3.c
@@ -11,7 +11,7 @@
 
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
-#include <tmmintrin.h>
+#include "simd.h"
 
 // Weights are quadratic from '1' to '1 / BlockSize', scaled by
 // 2^sm_weight_log2_scale.
diff --git a/Source/Lib/Common/ASM_SSSE3/jnt_convolve_ssse3.c b/Source/Lib/Common/ASM_SSSE3/jnt_convolve_ssse3.c
index a741b05..c2ed85d 100644
--- a/Source/Lib/Common/ASM_SSSE3/jnt_convolve_ssse3.c
+++ b/Source/Lib/Common/ASM_SSSE3/jnt_convolve_ssse3.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <tmmintrin.h>
+#include "simd.h"
 
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
diff --git a/Source/Lib/Common/ASM_SSSE3/reconinter_ssse3.c b/Source/Lib/Common/ASM_SSSE3/reconinter_ssse3.c
index 1dffa91..50d25b0 100644
--- a/Source/Lib/Common/ASM_SSSE3/reconinter_ssse3.c
+++ b/Source/Lib/Common/ASM_SSSE3/reconinter_ssse3.c
@@ -11,7 +11,7 @@
 
 #include "EbDefinitions.h"
 #include "common_dsp_rtcd.h"
-#include <tmmintrin.h>
+#include "simd.h"
 #include "synonyms.h"
 
 void svt_av1_build_compound_diffwtd_mask_highbd_ssse3(uint8_t *mask, DIFFWTD_MASK_TYPE mask_type, const uint8_t *src0,
diff --git a/Source/Lib/Common/CMakeLists.txt b/Source/Lib/Common/CMakeLists.txt
index f1974e2..b58d8ae 100644
--- a/Source/Lib/Common/CMakeLists.txt
+++ b/Source/Lib/Common/CMakeLists.txt
@@ -23,3 +23,10 @@ if(NOT COMPILE_C_ONLY AND HAVE_X86_PLATFORM)
     add_subdirectory(ASM_AVX2)
     add_subdirectory(ASM_AVX512)
 endif()
+
+if(NOT COMPILE_C_ONLY AND HAVE_AARCH64_PLATFORM)
+    add_subdirectory(ASM_SSE2)
+    add_subdirectory(ASM_SSSE3)
+    add_subdirectory(ASM_SSE4_1)
+    add_subdirectory(ASM_NEON)
+endif()
diff --git a/Source/Lib/Common/Codec/EbDefinitions.h b/Source/Lib/Common/Codec/EbDefinitions.h
index 3c0cea7..b5c26b6 100644
--- a/Source/Lib/Common/Codec/EbDefinitions.h
+++ b/Source/Lib/Common/Codec/EbDefinitions.h
@@ -1,4 +1,4 @@
-﻿/*
+/*
 * Copyright(c) 2019 Intel Corporation
 * Copyright (c) 2016, Alliance for Open Media. All rights reserved
 *
@@ -445,6 +445,8 @@ typedef int16_t InterpKernel[SUBPEL_TAPS];
 #ifdef ARCH_X86_64
 extern void RunEmms();
 #define aom_clear_system_state() RunEmms()
+#elif defined(ARCH_AARCH64)
+#define aom_clear_system_state()
 #endif
 
 /* Shift down with rounding for use when n >= 0, value >= 0 */
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.c b/Source/Lib/Common/Codec/common_dsp_rtcd.c
index 7baa6fb..4f83758 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.c
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.c
@@ -48,7 +48,9 @@
 #define HAS_AVX512PF EB_CPU_FLAGS_AVX512PF
 #define HAS_AVX512BW EB_CPU_FLAGS_AVX512BW
 #define HAS_AVX512VL EB_CPU_FLAGS_AVX512VL
-
+#ifdef ARCH_AARCH64
+#define HAS_NEON     EB_CPU_FLAGS_NEON
+#endif
 // coeff: 16 bits, dynamic range [-32640, 32640].
 // length: value range {16, 64, 256, 1024}.
 int svt_aom_satd_c(const TranLow *coeff, int length) {
@@ -115,6 +117,15 @@ EbCpuFlags svt_aom_get_cpu_flags_to_use() {
     return flags;
 }
 #endif /*ARCH_X86_64*/
+#ifdef ARCH_AARCH64
+EbCpuFlags svt_aom_get_cpu_flags() {
+    return EB_CPU_FLAGS_NEON;
+}
+
+EbCpuFlags svt_aom_get_cpu_flags_to_use() {
+    return EB_CPU_FLAGS_NEON;
+}
+#endif // ARCH_ARCH64
 
 #ifdef ARCH_X86_64
 #if EN_AVX512_SUPPORT
@@ -135,26 +146,44 @@ EbCpuFlags svt_aom_get_cpu_flags_to_use() {
     if (((uintptr_t)NULL != (uintptr_t)avx)    && (flags & HAS_AVX))    ptr = avx;                \
     if (((uintptr_t)NULL != (uintptr_t)avx2)   && (flags & HAS_AVX2))   ptr = avx2;               \
     SET_FUNCTIONS_AVX512(ptr, avx512)
-#else /* ARCH_X86_64 */
+#define SET_FUNCTIONS_ARCH(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
+    SET_FUNCTIONS_X86(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512)
+
+#elif defined(ARCH_AARCH64)
+#define SET_FUNCTIONS_AARCH64(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
+    if (((uintptr_t)NULL != (uintptr_t)mmx)    && (flags & HAS_NEON))    ptr = mmx;               \
+    if (((uintptr_t)NULL != (uintptr_t)sse)    && (flags & HAS_NEON))    ptr = sse;               \
+    if (((uintptr_t)NULL != (uintptr_t)sse2)   && (flags & HAS_NEON))    ptr = sse2;              \
+    if (((uintptr_t)NULL != (uintptr_t)sse3)   && (flags & HAS_NEON))    ptr = sse3;              \
+    if (((uintptr_t)NULL != (uintptr_t)ssse3)  && (flags & HAS_NEON))    ptr = ssse3;             \
+    if (((uintptr_t)NULL != (uintptr_t)sse4_1) && (flags & HAS_NEON))    ptr = sse4_1;            \
+    if (((uintptr_t)NULL != (uintptr_t)sse4_2) && (flags & HAS_NEON))    ptr = sse4_2;            \
+    if (((uintptr_t)NULL != (uintptr_t)neon)   && (flags & HAS_NEON))    ptr = neon;
+#define SET_FUNCTIONS_ARCH(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
+    SET_FUNCTIONS_AARCH64(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon)
+
+#else // ARCH_AARCH64
 #define SET_FUNCTIONS_X86(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512)
-#endif /* ARCH_X86_64 */
+#define SET_FUNCTIONS_AARCH64(ptr, c, neon)
+#define SET_FUNCTIONS_ARCH(ptr, c, neon)
+#endif // ARCH_X86_64
 
 #if EXCLUDE_HASH
-#define SET_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512)     \
+#define SET_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon)     \
     do {                                                                                          \
-        if (check_pointer_was_set && ptr != 0) {                                                                           \
-            printf("Error: %s:%i: Pointer \"%s\" is set before!\n", __FILE__, 0, #ptr);    \
+        if (check_pointer_was_set && ptr != 0) {                                                  \
+            printf("Error: %s:%i: Pointer \"%s\" is set before!\n", __FILE__, 0, #ptr);           \
             assert(0);                                                                            \
         }                                                                                         \
         if ((uintptr_t)NULL == (uintptr_t)c) {                                                    \
-            printf("Error: %s:%i: Pointer \"%s\" on C is NULL!\n", __FILE__, 0, #ptr);     \
+            printf("Error: %s:%i: Pointer \"%s\" on C is NULL!\n", __FILE__, 0, #ptr);            \
             assert(0);                                                                            \
         }                                                                                         \
         ptr = c;                                                                                  \
-        SET_FUNCTIONS_X86(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512) \
+        SET_FUNCTIONS_ARCH(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
     } while (0)
 #else
-#define SET_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512)     \
+#define SET_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon)     \
     do {                                                                                          \
         if (check_pointer_was_set && ptr != 0) {                                                                           \
             printf("Error: %s:%i: Pointer \"%s\" is set before!\n", __FILE__, __LINE__, #ptr);    \
@@ -165,24 +194,47 @@ EbCpuFlags svt_aom_get_cpu_flags_to_use() {
             assert(0);                                                                            \
         }                                                                                         \
         ptr = c;                                                                                  \
-        SET_FUNCTIONS_X86(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512) \
+        SET_FUNCTIONS_ARCH(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
     } while (0)
 #endif
 
 /* Macros SET_* use local variable EbCpuFlags flags and Bool check_pointer_was_set */
-#define SET_ONLY_C(ptr, c)                                  SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
-#define SET_SSE2(ptr, c, sse2)                              SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, 0)
-#define SET_SSE2_AVX2(ptr, c, sse2, avx2)                   SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, 0)
-#define SET_SSE2_AVX512(ptr, c, sse2, avx512)               SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, avx512)
-#define SET_SSSE3(ptr, c, ssse3)                            SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, 0, 0)
-#define SET_SSE41(ptr, c, sse4_1)                           SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, 0, 0)
-#define SET_SSE41_AVX2(ptr, c, sse4_1, avx2)                SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, 0)
-#define SET_SSE41_AVX2_AVX512(ptr, c, sse4_1, avx2, avx512) SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, avx512)
-#define SET_AVX2(ptr, c, avx2)                              SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, avx2, 0)
-#define SET_AVX2_AVX512(ptr, c, avx2, avx512)               SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, avx2, avx512)
-#define SET_SSE2_AVX2_AVX512(ptr, c, sse2, avx2, avx512)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, avx512)
-#define SET_SSE2_SSSE3_AVX2_AVX512(ptr, c, sse2, ssse3, avx2, avx512)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, avx2, avx512)
-#define SET_SSSE3_AVX2(ptr, c,ssse3, avx2)                  SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, avx2, 0)
+#define SET_ONLY_C(ptr, c)                                  SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2(ptr, c, sse2)                              SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2_SSSE3(ptr, c, sse2, ssse3)                 SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2_AVX2(ptr, c, sse2, avx2)                   SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, 0, 0)
+#define SET_SSE2_AVX512(ptr, c, sse2, avx512)               SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, avx512, 0)
+#define SET_SSE2_SSSE3_AVX2_AVX512(ptr, c, sse2, ssse3, avx2, avx512) SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, avx2, avx512, 0)
+#define SET_SSSE3(ptr, c, ssse3)                            SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, 0, 0, 0)
+#define SET_SSSE3_AVX2(ptr, c, ssse3, avx2)                 SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, avx2, 0, 0)
+#define SET_SSE41(ptr, c, sse4_1)                           SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, 0, 0, 0)
+#define SET_SSE41_AVX2(ptr, c, sse4_1, avx2)                SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, 0, 0)
+#define SET_SSE41_AVX2_AVX512(ptr, c, sse4_1, avx2, avx512) SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, avx512, 0)
+#define SET_AVX2(ptr, c, avx2)                              SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, avx2, 0, 0)
+#define SET_AVX2_AVX512(ptr, c, avx2, avx512)               SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, avx2, avx512, 0)
+#define SET_SSE2_AVX2_AVX512(ptr, c, sse2, avx2, avx512)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, avx512, 0)
+
+#ifdef ARCH_AARCH64
+#define SET_NEON(ptr, c, neon)                              SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, neon)
+#define SET_SSSE3_AVX2_NEON(ptr, c, ssse3, avx2, neon)      SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, avx2, 0, neon)
+#define SET_SSE2_NEON(ptr, c, sse2, neon)                   SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, 0, neon)
+#define SET_SSE2_SSSE3_NEON(ptr, c, sse2, ssse3, neon)      SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, 0, 0, neon)
+#define SET_SSE2_AVX2_NEON(ptr, c, sse2, avx2, neon)        SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, 0, neon)
+#define SET_SSE2_AVX2_AVX512_NEON(ptr, c, sse2, avx2, avx512, neon)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, avx512, neon)
+#define SET_SSE2_SSSE3_AVX2_AVX512_NEON(ptr, c, sse2, ssse3, avx2, avx512, neon) SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, avx2, avx512, neon)
+#define SET_ONLY_TEMP2_C(ptr, c, sse2)                      SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_ONLY_TEMP3_C(ptr, c, sse2, ssse3)               SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_ONLY_TEMP5_C(ptr, c, sse2, ssse3, sse4_1, avx2) SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_TEMP_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512) \
+                                                            SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#else
+#define SET_SSE2_NEON(ptr, c, sse2, neon)                   SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_SSSE3_AVX2_NEON(ptr, c, ssse3, avx2, neon)      SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, avx2, 0, 0)
+#define SET_SSE2_SSSE3_NEON(ptr, c, sse2, ssse3, neon)      SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2_AVX2_NEON(ptr, c, sse2, avx2, neon)        SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, 0, 0)
+#define SET_SSE2_AVX2_AVX512_NEON(ptr, c, sse2, avx2, avx512, neon)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, avx512, 0)
+#define SET_SSE2_SSSE3_AVX2_AVX512_NEON(ptr, c, sse2, ssse3, avx2, avx512, neon) SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, avx2, avx512, 0)
+#endif // ARCH_AARCH64
 
 
 void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
@@ -264,7 +316,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_SSE2_AVX2(svt_residual_kernel16bit, svt_residual_kernel16bit_c, svt_residual_kernel16bit_sse2_intrin, svt_residual_kernel16bit_avx2);
     SET_SSE2(svt_picture_average_kernel, svt_picture_average_kernel_c, svt_picture_average_kernel_sse2_intrin);
     SET_SSE2(svt_picture_average_kernel1_line, svt_picture_average_kernel1_line_c, svt_picture_average_kernel1_line_sse2_intrin);
-    SET_SSE2_AVX2_AVX512(svt_av1_wiener_convolve_add_src, svt_av1_wiener_convolve_add_src_c, svt_av1_wiener_convolve_add_src_sse2, svt_av1_wiener_convolve_add_src_avx2, svt_av1_wiener_convolve_add_src_avx512);
+    SET_SSE2_AVX2_AVX512_NEON(svt_av1_wiener_convolve_add_src, svt_av1_wiener_convolve_add_src_c, svt_av1_wiener_convolve_add_src_sse2, svt_av1_wiener_convolve_add_src_avx2, svt_av1_wiener_convolve_add_src_avx512, svt_av1_wiener_convolve_add_src_neon);
     SET_SSE41(svt_av1_convolve_2d_scale, svt_av1_convolve_2d_scale_c, svt_av1_convolve_2d_scale_sse4_1);
     SET_SSSE3_AVX2(svt_av1_highbd_convolve_y_sr, svt_av1_highbd_convolve_y_sr_c, svt_av1_highbd_convolve_y_sr_ssse3, svt_av1_highbd_convolve_y_sr_avx2);
     SET_SSSE3_AVX2(svt_av1_highbd_convolve_2d_sr, svt_av1_highbd_convolve_2d_sr_c, svt_av1_highbd_convolve_2d_sr_ssse3, svt_av1_highbd_convolve_2d_sr_avx2);
@@ -275,16 +327,21 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_SSE41_AVX2(svt_av1_highbd_jnt_convolve_x, svt_av1_highbd_jnt_convolve_x_c, svt_av1_highbd_jnt_convolve_x_sse4_1, svt_av1_highbd_jnt_convolve_x_avx2);
     SET_SSE41_AVX2(svt_av1_highbd_jnt_convolve_y, svt_av1_highbd_jnt_convolve_y_c, svt_av1_highbd_jnt_convolve_y_sse4_1, svt_av1_highbd_jnt_convolve_y_avx2);
     SET_SSSE3_AVX2(svt_av1_highbd_convolve_x_sr, svt_av1_highbd_convolve_x_sr_c, svt_av1_highbd_convolve_x_sr_ssse3, svt_av1_highbd_convolve_x_sr_avx2);
-    SET_SSE2_AVX2_AVX512(svt_av1_convolve_2d_sr, svt_av1_convolve_2d_sr_c,svt_av1_convolve_2d_sr_sse2, svt_av1_convolve_2d_sr_avx2, svt_av1_convolve_2d_sr_avx512);
+    SET_SSE2_AVX2_AVX512_NEON(svt_av1_convolve_2d_sr, svt_av1_convolve_2d_sr_c,svt_av1_convolve_2d_sr_sse2, svt_av1_convolve_2d_sr_avx2, svt_av1_convolve_2d_sr_avx512, svt_av1_convolve_2d_sr_neon);
     SET_SSE2_AVX2_AVX512(svt_av1_convolve_2d_copy_sr, svt_av1_convolve_2d_copy_sr_c, svt_av1_convolve_2d_copy_sr_sse2, svt_av1_convolve_2d_copy_sr_avx2, svt_av1_convolve_2d_copy_sr_avx512);
-    SET_SSE2_AVX2_AVX512(svt_av1_convolve_x_sr, svt_av1_convolve_x_sr_c, svt_av1_convolve_x_sr_sse2, svt_av1_convolve_x_sr_avx2, svt_av1_convolve_x_sr_avx512);
-    SET_SSE2_AVX2_AVX512(svt_av1_convolve_y_sr, svt_av1_convolve_y_sr_c, svt_av1_convolve_y_sr_sse2, svt_av1_convolve_y_sr_avx2, svt_av1_convolve_y_sr_avx512);
+    SET_SSE2_AVX2_AVX512_NEON(svt_av1_convolve_x_sr, svt_av1_convolve_x_sr_c, svt_av1_convolve_x_sr_sse2, svt_av1_convolve_x_sr_avx2, svt_av1_convolve_x_sr_avx512, svt_av1_convolve_x_sr_neon);
+    SET_SSE2_AVX2_AVX512_NEON(svt_av1_convolve_y_sr, svt_av1_convolve_y_sr_c, svt_av1_convolve_y_sr_sse2, svt_av1_convolve_y_sr_avx2, svt_av1_convolve_y_sr_avx512, svt_av1_convolve_y_sr_neon);
     SET_SSE2_SSSE3_AVX2_AVX512(svt_av1_jnt_convolve_2d, svt_av1_jnt_convolve_2d_c, svt_av1_jnt_convolve_2d_sse2, svt_av1_jnt_convolve_2d_ssse3, svt_av1_jnt_convolve_2d_avx2, svt_av1_jnt_convolve_2d_avx512);
     SET_SSE2_AVX2_AVX512(svt_av1_jnt_convolve_2d_copy, svt_av1_jnt_convolve_2d_copy_c, svt_av1_jnt_convolve_2d_copy_sse2, svt_av1_jnt_convolve_2d_copy_avx2, svt_av1_jnt_convolve_2d_copy_avx512);
     SET_SSE2_AVX2_AVX512(svt_av1_jnt_convolve_x, svt_av1_jnt_convolve_x_c, svt_av1_jnt_convolve_x_sse2, svt_av1_jnt_convolve_x_avx2, svt_av1_jnt_convolve_x_avx512);
     SET_SSE2_AVX2_AVX512(svt_av1_jnt_convolve_y, svt_av1_jnt_convolve_y_c, svt_av1_jnt_convolve_y_sse2, svt_av1_jnt_convolve_y_avx2, svt_av1_jnt_convolve_y_avx512);
+#ifdef ARCH_AARCH64
+    SET_NEON(svt_aom_convolve8_horiz, svt_aom_convolve8_horiz_c, svt_aom_convolve8_horiz_ssse3_neon);
+    SET_NEON(svt_aom_convolve8_vert, svt_aom_convolve8_vert_c, svt_aom_convolve8_vert_ssse3_neon);
+#else
     SET_SSSE3_AVX2(svt_aom_convolve8_horiz, svt_aom_convolve8_horiz_c, svt_aom_convolve8_horiz_ssse3, svt_aom_convolve8_horiz_avx2);
     SET_SSSE3_AVX2(svt_aom_convolve8_vert, svt_aom_convolve8_vert_c, svt_aom_convolve8_vert_ssse3, svt_aom_convolve8_vert_avx2);
+#endif
     SET_SSE41_AVX2(svt_av1_build_compound_diffwtd_mask, svt_av1_build_compound_diffwtd_mask_c, svt_av1_build_compound_diffwtd_mask_sse4_1, svt_av1_build_compound_diffwtd_mask_avx2);
     SET_SSSE3_AVX2(svt_av1_build_compound_diffwtd_mask_highbd, svt_av1_build_compound_diffwtd_mask_highbd_c, svt_av1_build_compound_diffwtd_mask_highbd_ssse3, svt_av1_build_compound_diffwtd_mask_highbd_avx2);
     SET_SSE2_AVX2(svt_av1_wedge_sse_from_residuals, svt_av1_wedge_sse_from_residuals_c, svt_av1_wedge_sse_from_residuals_sse2, svt_av1_wedge_sse_from_residuals_avx2);
@@ -354,6 +411,88 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_AVX2(svt_aom_highbd_paeth_predictor_64x32, svt_aom_highbd_paeth_predictor_64x32_c, svt_aom_highbd_paeth_predictor_64x32_avx2);
     SET_AVX2(svt_aom_highbd_paeth_predictor_64x64, svt_aom_highbd_paeth_predictor_64x64_c, svt_aom_highbd_paeth_predictor_64x64_avx2);
     SET_SSE2(svt_aom_sum_squares_i16, svt_aom_sum_squares_i16_c, svt_aom_sum_squares_i16_sse2);
+#ifdef ARCH_AARCH64
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_4x4, svt_aom_dc_predictor_4x4_c, svt_aom_dc_predictor_4x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_4x8, svt_aom_dc_predictor_4x8_c, svt_aom_dc_predictor_4x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_4x16, svt_aom_dc_predictor_4x16_c, svt_aom_dc_predictor_4x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_8x4, svt_aom_dc_predictor_8x4_c, svt_aom_dc_predictor_8x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_8x8, svt_aom_dc_predictor_8x8_c, svt_aom_dc_predictor_8x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_8x16, svt_aom_dc_predictor_8x16_c, svt_aom_dc_predictor_8x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_8x32, svt_aom_dc_predictor_8x32_c, svt_aom_dc_predictor_8x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_16x4, svt_aom_dc_predictor_16x4_c, svt_aom_dc_predictor_16x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_16x8, svt_aom_dc_predictor_16x8_c, svt_aom_dc_predictor_16x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_16x16, svt_aom_dc_predictor_16x16_c, svt_aom_dc_predictor_16x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_16x32, svt_aom_dc_predictor_16x32_c, svt_aom_dc_predictor_16x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_16x64, svt_aom_dc_predictor_16x64_c, svt_aom_dc_predictor_16x64_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_32x8, svt_aom_dc_predictor_32x8_c, svt_aom_dc_predictor_32x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_32x16, svt_aom_dc_predictor_32x16_c, svt_aom_dc_predictor_32x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_32x32, svt_aom_dc_predictor_32x32_c, svt_aom_dc_predictor_32x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_32x64, svt_aom_dc_predictor_32x64_c, svt_aom_dc_predictor_32x64_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_64x16, svt_aom_dc_predictor_64x16_c, svt_aom_dc_predictor_64x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_64x32, svt_aom_dc_predictor_64x32_c, svt_aom_dc_predictor_64x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_predictor_64x64, svt_aom_dc_predictor_64x64_c, svt_aom_dc_predictor_64x64_avx2);
+
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_4x4, svt_aom_dc_top_predictor_4x4_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_4x8, svt_aom_dc_top_predictor_4x8_c, svt_aom_dc_top_predictor_4x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_4x16, svt_aom_dc_top_predictor_4x16_c, svt_aom_dc_top_predictor_4x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_8x4, svt_aom_dc_top_predictor_8x4_c, svt_aom_dc_top_predictor_8x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_8x8, svt_aom_dc_top_predictor_8x8_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_8x16, svt_aom_dc_top_predictor_8x16_c, svt_aom_dc_top_predictor_8x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_8x32, svt_aom_dc_top_predictor_8x32_c, svt_aom_dc_top_predictor_8x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_16x4, svt_aom_dc_top_predictor_16x4_c, svt_aom_dc_top_predictor_16x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_16x8, svt_aom_dc_top_predictor_16x8_c, svt_aom_dc_top_predictor_16x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_16x16, svt_aom_dc_top_predictor_16x16_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_16x32, svt_aom_dc_top_predictor_16x32_c, svt_aom_dc_top_predictor_16x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_16x64, svt_aom_dc_top_predictor_16x64_c, svt_aom_dc_top_predictor_16x64_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_32x8, svt_aom_dc_top_predictor_32x8_c, svt_aom_dc_top_predictor_32x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_32x16, svt_aom_dc_top_predictor_32x16_c, svt_aom_dc_top_predictor_32x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_32x32, svt_aom_dc_top_predictor_32x32_c, svt_aom_dc_top_predictor_32x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_32x64, svt_aom_dc_top_predictor_32x64_c, svt_aom_dc_top_predictor_32x64_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_64x16, svt_aom_dc_top_predictor_64x16_c, svt_aom_dc_top_predictor_64x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_64x32, svt_aom_dc_top_predictor_64x32_c, svt_aom_dc_top_predictor_64x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_top_predictor_64x64, svt_aom_dc_top_predictor_64x64_c, svt_aom_dc_top_predictor_64x64_avx2);
+
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_4x4, svt_aom_dc_left_predictor_4x4_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_4x8, svt_aom_dc_left_predictor_4x8_c, svt_aom_dc_left_predictor_4x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_4x16, svt_aom_dc_left_predictor_4x16_c, svt_aom_dc_left_predictor_4x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_8x4, svt_aom_dc_left_predictor_8x4_c, svt_aom_dc_left_predictor_8x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_8x8, svt_aom_dc_left_predictor_8x8_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_8x16, svt_aom_dc_left_predictor_8x16_c, svt_aom_dc_left_predictor_8x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_8x32, svt_aom_dc_left_predictor_8x32_c, svt_aom_dc_left_predictor_8x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_16x4, svt_aom_dc_left_predictor_16x4_c, svt_aom_dc_left_predictor_16x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_16x8, svt_aom_dc_left_predictor_16x8_c, svt_aom_dc_left_predictor_16x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_16x16, svt_aom_dc_left_predictor_16x16_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_16x32, svt_aom_dc_left_predictor_16x32_c, svt_aom_dc_left_predictor_16x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_16x64, svt_aom_dc_left_predictor_16x64_c, svt_aom_dc_left_predictor_16x64_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_32x8, svt_aom_dc_left_predictor_32x8_c, svt_aom_dc_left_predictor_32x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_32x16, svt_aom_dc_left_predictor_32x16_c, svt_aom_dc_left_predictor_32x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_32x32, svt_aom_dc_left_predictor_32x32_c, svt_aom_dc_left_predictor_32x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_32x64, svt_aom_dc_left_predictor_32x64_c, svt_aom_dc_left_predictor_32x64_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_64x16, svt_aom_dc_left_predictor_64x16_c, svt_aom_dc_left_predictor_64x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_64x32, svt_aom_dc_left_predictor_64x32_c, svt_aom_dc_left_predictor_64x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_left_predictor_64x64, svt_aom_dc_left_predictor_64x64_c, svt_aom_dc_left_predictor_64x64_avx2);
+
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_4x4, svt_aom_dc_128_predictor_4x4_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_4x8, svt_aom_dc_128_predictor_4x8_c, svt_aom_dc_128_predictor_4x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_4x16, svt_aom_dc_128_predictor_4x16_c, svt_aom_dc_128_predictor_4x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_8x4, svt_aom_dc_128_predictor_8x4_c, svt_aom_dc_128_predictor_8x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_8x8, svt_aom_dc_128_predictor_8x8_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_8x16, svt_aom_dc_128_predictor_8x16_c, svt_aom_dc_128_predictor_8x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_8x32, svt_aom_dc_128_predictor_8x32_c, svt_aom_dc_128_predictor_8x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_16x4, svt_aom_dc_128_predictor_16x4_c, svt_aom_dc_128_predictor_16x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_16x8, svt_aom_dc_128_predictor_16x8_c, svt_aom_dc_128_predictor_16x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_16x16, svt_aom_dc_128_predictor_16x16_c, 0);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_16x32, svt_aom_dc_128_predictor_16x32_c, svt_aom_dc_128_predictor_16x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_16x64, svt_aom_dc_128_predictor_16x64_c, svt_aom_dc_128_predictor_16x64_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_32x8, svt_aom_dc_128_predictor_32x8_c, svt_aom_dc_128_predictor_32x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_32x16, svt_aom_dc_128_predictor_32x16_c, svt_aom_dc_128_predictor_32x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_32x32, svt_aom_dc_128_predictor_32x32_c, svt_aom_dc_128_predictor_32x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_32x64, svt_aom_dc_128_predictor_32x64_c, svt_aom_dc_128_predictor_32x64_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_64x16, svt_aom_dc_128_predictor_64x16_c, svt_aom_dc_128_predictor_64x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_64x32, svt_aom_dc_128_predictor_64x32_c, svt_aom_dc_128_predictor_64x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_dc_128_predictor_64x64, svt_aom_dc_128_predictor_64x64_c, svt_aom_dc_128_predictor_64x64_avx2);
+
+#else
     SET_SSE2(svt_aom_dc_predictor_4x4, svt_aom_dc_predictor_4x4_c, svt_aom_dc_predictor_4x4_sse2);
     SET_SSE2(svt_aom_dc_predictor_4x8, svt_aom_dc_predictor_4x8_c, svt_aom_dc_predictor_4x8_sse2);
     SET_SSE2(svt_aom_dc_predictor_4x16, svt_aom_dc_predictor_4x16_c, svt_aom_dc_predictor_4x16_sse2);
@@ -433,6 +572,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_AVX2(svt_aom_dc_128_predictor_64x16, svt_aom_dc_128_predictor_64x16_c, svt_aom_dc_128_predictor_64x16_avx2);
     SET_AVX2(svt_aom_dc_128_predictor_64x32, svt_aom_dc_128_predictor_64x32_c, svt_aom_dc_128_predictor_64x32_avx2);
     SET_AVX2(svt_aom_dc_128_predictor_64x64, svt_aom_dc_128_predictor_64x64_c, svt_aom_dc_128_predictor_64x64_avx2);
+#endif
 
     SET_SSSE3(svt_aom_smooth_h_predictor_4x4, svt_aom_smooth_h_predictor_4x4_c, svt_aom_smooth_h_predictor_4x4_ssse3);
     SET_SSSE3(svt_aom_smooth_h_predictor_4x8, svt_aom_smooth_h_predictor_4x8_c, svt_aom_smooth_h_predictor_4x8_ssse3);
@@ -493,7 +633,47 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_SSSE3(svt_aom_smooth_predictor_64x16, svt_aom_smooth_predictor_64x16_c, svt_aom_smooth_predictor_64x16_ssse3);
     SET_SSSE3(svt_aom_smooth_predictor_64x32, svt_aom_smooth_predictor_64x32_c, svt_aom_smooth_predictor_64x32_ssse3);
     SET_SSSE3(svt_aom_smooth_predictor_64x64, svt_aom_smooth_predictor_64x64_c, svt_aom_smooth_predictor_64x64_ssse3);
-
+ #ifdef ARCH_AARCH64
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_4x4, svt_aom_v_predictor_4x4_c, svt_aom_v_predictor_4x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_4x8, svt_aom_v_predictor_4x8_c, svt_aom_v_predictor_4x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_4x16, svt_aom_v_predictor_4x16_c, svt_aom_v_predictor_4x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_8x4, svt_aom_v_predictor_8x4_c, svt_aom_v_predictor_8x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_8x8, svt_aom_v_predictor_8x8_c, svt_aom_v_predictor_8x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_8x16, svt_aom_v_predictor_8x16_c, svt_aom_v_predictor_8x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_8x32, svt_aom_v_predictor_8x32_c, svt_aom_v_predictor_8x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_16x4, svt_aom_v_predictor_16x4_c, svt_aom_v_predictor_16x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_16x8, svt_aom_v_predictor_16x8_c, svt_aom_v_predictor_16x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_16x16, svt_aom_v_predictor_16x16_c, svt_aom_v_predictor_16x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_16x32, svt_aom_v_predictor_16x32_c, svt_aom_v_predictor_16x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_16x64, svt_aom_v_predictor_16x64_c, svt_aom_v_predictor_16x64_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_32x8, svt_aom_v_predictor_32x8_c, svt_aom_v_predictor_32x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_32x16, svt_aom_v_predictor_32x16_c, svt_aom_v_predictor_32x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_32x32, svt_aom_v_predictor_32x32_c, svt_aom_v_predictor_32x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_32x64, svt_aom_v_predictor_32x64_c, svt_aom_v_predictor_32x64_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_64x16, svt_aom_v_predictor_64x16_c, svt_aom_v_predictor_64x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_64x32, svt_aom_v_predictor_64x32_c, svt_aom_v_predictor_64x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_v_predictor_64x64, svt_aom_v_predictor_64x64_c, svt_aom_v_predictor_64x64_avx2);
+
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_4x4, svt_aom_h_predictor_4x4_c, svt_aom_h_predictor_4x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_4x8, svt_aom_h_predictor_4x8_c, svt_aom_h_predictor_4x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_4x16, svt_aom_h_predictor_4x16_c, svt_aom_h_predictor_4x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_8x4, svt_aom_h_predictor_8x4_c, svt_aom_h_predictor_8x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_8x8, svt_aom_h_predictor_8x8_c, svt_aom_h_predictor_8x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_8x16, svt_aom_h_predictor_8x16_c, svt_aom_h_predictor_8x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_8x32, svt_aom_h_predictor_8x32_c, svt_aom_h_predictor_8x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_16x4, svt_aom_h_predictor_16x4_c, svt_aom_h_predictor_16x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_16x8, svt_aom_h_predictor_16x8_c, svt_aom_h_predictor_16x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_16x16, svt_aom_h_predictor_16x16_c, svt_aom_h_predictor_16x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_16x32, svt_aom_h_predictor_16x32_c, svt_aom_h_predictor_16x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_16x64, svt_aom_h_predictor_16x64_c, svt_aom_h_predictor_16x64_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_32x8, svt_aom_h_predictor_32x8_c, svt_aom_h_predictor_32x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_32x16, svt_aom_h_predictor_32x16_c, svt_aom_h_predictor_32x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_32x32, svt_aom_h_predictor_32x32_c, svt_aom_h_predictor_32x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_32x64, svt_aom_h_predictor_32x64_c, svt_aom_h_predictor_32x64_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_64x16, svt_aom_h_predictor_64x16_c, svt_aom_h_predictor_64x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_64x32, svt_aom_h_predictor_64x32_c, svt_aom_h_predictor_64x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_h_predictor_64x64, svt_aom_h_predictor_64x64_c, svt_aom_h_predictor_64x64_sse2);
+ #else
     SET_SSE2(svt_aom_v_predictor_4x4, svt_aom_v_predictor_4x4_c, svt_aom_v_predictor_4x4_sse2);
     SET_SSE2(svt_aom_v_predictor_4x8, svt_aom_v_predictor_4x8_c, svt_aom_v_predictor_4x8_sse2);
     SET_SSE2(svt_aom_v_predictor_4x16, svt_aom_v_predictor_4x16_c, svt_aom_v_predictor_4x16_sse2);
@@ -533,6 +713,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_SSE2(svt_aom_h_predictor_64x16, svt_aom_h_predictor_64x16_c, svt_aom_h_predictor_64x16_sse2);
     SET_SSE2(svt_aom_h_predictor_64x32, svt_aom_h_predictor_64x32_c, svt_aom_h_predictor_64x32_sse2);
     SET_SSE2(svt_aom_h_predictor_64x64, svt_aom_h_predictor_64x64_c, svt_aom_h_predictor_64x64_sse2);
+#endif
     SET_SSE41_AVX2(svt_aom_cdef_find_dir, svt_aom_cdef_find_dir_c, svt_aom_cdef_find_dir_sse4_1, svt_aom_cdef_find_dir_avx2);
     SET_SSE41_AVX2(svt_aom_cdef_find_dir_dual, svt_aom_cdef_find_dir_dual_c, svt_aom_cdef_find_dir_dual_sse4_1, svt_aom_cdef_find_dir_dual_avx2);
     SET_SSE41_AVX2(svt_cdef_filter_block, svt_cdef_filter_block_c, svt_av1_cdef_filter_block_sse4_1, svt_cdef_filter_block_avx2);
@@ -566,6 +747,27 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_SSE2(svt_aom_lpf_vertical_14, svt_aom_lpf_vertical_14_c, svt_aom_lpf_vertical_14_sse2);
 
     // svt_aom_highbd_v_predictor
+#ifdef ARCH_AARCH64
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_4x4, svt_aom_highbd_v_predictor_4x4_c, svt_aom_highbd_v_predictor_4x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_4x8, svt_aom_highbd_v_predictor_4x8_c, svt_aom_highbd_v_predictor_4x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_4x16, svt_aom_highbd_v_predictor_4x16_c, svt_aom_highbd_v_predictor_4x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_8x4, svt_aom_highbd_v_predictor_8x4_c, svt_aom_highbd_v_predictor_8x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_8x8, svt_aom_highbd_v_predictor_8x8_c, svt_aom_highbd_v_predictor_8x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_8x16, svt_aom_highbd_v_predictor_8x16_c, svt_aom_highbd_v_predictor_8x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_8x32, svt_aom_highbd_v_predictor_8x32_c, svt_aom_highbd_v_predictor_8x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_16x4, svt_aom_highbd_v_predictor_16x4_c, svt_aom_highbd_v_predictor_16x4_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_16x8, svt_aom_highbd_v_predictor_16x8_c, svt_aom_highbd_v_predictor_16x8_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_16x16, svt_aom_highbd_v_predictor_16x16_c, svt_aom_highbd_v_predictor_16x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_16x32, svt_aom_highbd_v_predictor_16x32_c, svt_aom_highbd_v_predictor_16x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_v_predictor_16x64, svt_aom_highbd_v_predictor_16x64_c, svt_aom_highbd_v_predictor_16x64_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_v_predictor_32x8, svt_aom_highbd_v_predictor_32x8_c, svt_aom_highbd_v_predictor_32x8_avx2, aom_highbd_v_predictor_32x8_avx512);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_v_predictor_32x16, svt_aom_highbd_v_predictor_32x16_c, svt_aom_highbd_v_predictor_32x16_avx2, aom_highbd_v_predictor_32x16_avx512);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_v_predictor_32x32, svt_aom_highbd_v_predictor_32x32_c, svt_aom_highbd_v_predictor_32x32_avx2, aom_highbd_v_predictor_32x32_avx512);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_v_predictor_32x64, svt_aom_highbd_v_predictor_32x64_c, svt_aom_highbd_v_predictor_32x64_avx2, aom_highbd_v_predictor_32x64_avx512);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_v_predictor_64x16, svt_aom_highbd_v_predictor_64x16_c, svt_aom_highbd_v_predictor_64x16_avx2, aom_highbd_v_predictor_64x16_avx512);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_v_predictor_64x32, svt_aom_highbd_v_predictor_64x32_c, svt_aom_highbd_v_predictor_64x32_avx2, aom_highbd_v_predictor_64x32_avx512);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_v_predictor_64x64, svt_aom_highbd_v_predictor_64x64_c, svt_aom_highbd_v_predictor_64x64_avx2, aom_highbd_v_predictor_64x64_avx512);
+#else
     SET_SSE2(svt_aom_highbd_v_predictor_4x4, svt_aom_highbd_v_predictor_4x4_c, svt_aom_highbd_v_predictor_4x4_sse2);
     SET_SSE2(svt_aom_highbd_v_predictor_4x8, svt_aom_highbd_v_predictor_4x8_c, svt_aom_highbd_v_predictor_4x8_sse2);
     SET_SSE2(svt_aom_highbd_v_predictor_4x16, svt_aom_highbd_v_predictor_4x16_c, svt_aom_highbd_v_predictor_4x16_sse2);
@@ -585,7 +787,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_AVX2_AVX512(svt_aom_highbd_v_predictor_64x16, svt_aom_highbd_v_predictor_64x16_c, svt_aom_highbd_v_predictor_64x16_avx2, aom_highbd_v_predictor_64x16_avx512);
     SET_AVX2_AVX512(svt_aom_highbd_v_predictor_64x32, svt_aom_highbd_v_predictor_64x32_c, svt_aom_highbd_v_predictor_64x32_avx2, aom_highbd_v_predictor_64x32_avx512);
     SET_AVX2_AVX512(svt_aom_highbd_v_predictor_64x64, svt_aom_highbd_v_predictor_64x64_c, svt_aom_highbd_v_predictor_64x64_avx2, aom_highbd_v_predictor_64x64_avx512);
-
+#endif
     //aom_highbd_smooth_predictor
     SET_SSSE3(svt_aom_highbd_smooth_predictor_4x4, svt_aom_highbd_smooth_predictor_4x4_c, svt_aom_highbd_smooth_predictor_4x4_ssse3);
     SET_SSSE3(svt_aom_highbd_smooth_predictor_4x8, svt_aom_highbd_smooth_predictor_4x8_c, svt_aom_highbd_smooth_predictor_4x8_ssse3);
@@ -669,7 +871,20 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_AVX2_AVX512(svt_aom_highbd_dc_left_predictor_64x16, svt_aom_highbd_dc_left_predictor_64x16_c, svt_aom_highbd_dc_left_predictor_64x16_avx2, aom_highbd_dc_left_predictor_64x16_avx512);
     SET_AVX2_AVX512(svt_aom_highbd_dc_left_predictor_64x32, svt_aom_highbd_dc_left_predictor_64x32_c, svt_aom_highbd_dc_left_predictor_64x32_avx2, aom_highbd_dc_left_predictor_64x32_avx512);
     SET_AVX2_AVX512(svt_aom_highbd_dc_left_predictor_64x64, svt_aom_highbd_dc_left_predictor_64x64_c, svt_aom_highbd_dc_left_predictor_64x64_avx2, aom_highbd_dc_left_predictor_64x64_avx512);
-
+#ifdef ARCH_AARCH64
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_4x4, svt_aom_highbd_dc_predictor_4x4_c, svt_aom_highbd_dc_predictor_4x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_4x8, svt_aom_highbd_dc_predictor_4x8_c, svt_aom_highbd_dc_predictor_4x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_4x16, svt_aom_highbd_dc_predictor_4x16_c, svt_aom_highbd_dc_predictor_4x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_8x4, svt_aom_highbd_dc_predictor_8x4_c, svt_aom_highbd_dc_predictor_8x4_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_8x8, svt_aom_highbd_dc_predictor_8x8_c, svt_aom_highbd_dc_predictor_8x8_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_8x16, svt_aom_highbd_dc_predictor_8x16_c, svt_aom_highbd_dc_predictor_8x16_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_8x32, svt_aom_highbd_dc_predictor_8x32_c, svt_aom_highbd_dc_predictor_8x32_sse2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_16x4, svt_aom_highbd_dc_predictor_16x4_c, svt_aom_highbd_dc_predictor_16x4_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_16x8, svt_aom_highbd_dc_predictor_16x8_c, svt_aom_highbd_dc_predictor_16x8_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_16x16, svt_aom_highbd_dc_predictor_16x16_c, svt_aom_highbd_dc_predictor_16x16_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_16x32, svt_aom_highbd_dc_predictor_16x32_c, svt_aom_highbd_dc_predictor_16x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_dc_predictor_16x64, svt_aom_highbd_dc_predictor_16x64_c, svt_aom_highbd_dc_predictor_16x64_avx2);
+#else
     SET_SSE2(svt_aom_highbd_dc_predictor_4x4, svt_aom_highbd_dc_predictor_4x4_c, svt_aom_highbd_dc_predictor_4x4_sse2);
     SET_SSE2(svt_aom_highbd_dc_predictor_4x8, svt_aom_highbd_dc_predictor_4x8_c, svt_aom_highbd_dc_predictor_4x8_sse2);
     SET_SSE2(svt_aom_highbd_dc_predictor_4x16, svt_aom_highbd_dc_predictor_4x16_c, svt_aom_highbd_dc_predictor_4x16_sse2);
@@ -682,6 +897,7 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_AVX2(svt_aom_highbd_dc_predictor_16x16, svt_aom_highbd_dc_predictor_16x16_c, svt_aom_highbd_dc_predictor_16x16_avx2);
     SET_AVX2(svt_aom_highbd_dc_predictor_16x32, svt_aom_highbd_dc_predictor_16x32_c, svt_aom_highbd_dc_predictor_16x32_avx2);
     SET_AVX2(svt_aom_highbd_dc_predictor_16x64, svt_aom_highbd_dc_predictor_16x64_c, svt_aom_highbd_dc_predictor_16x64_avx2);
+#endif
     SET_AVX2_AVX512(svt_aom_highbd_dc_predictor_32x8, svt_aom_highbd_dc_predictor_32x8_c, svt_aom_highbd_dc_predictor_32x8_avx2, aom_highbd_dc_predictor_32x8_avx512);
     SET_AVX2_AVX512(svt_aom_highbd_dc_predictor_32x16, svt_aom_highbd_dc_predictor_32x16_c, svt_aom_highbd_dc_predictor_32x16_avx2, aom_highbd_dc_predictor_32x16_avx512);
     SET_AVX2_AVX512(svt_aom_highbd_dc_predictor_32x32, svt_aom_highbd_dc_predictor_32x32_c, svt_aom_highbd_dc_predictor_32x32_avx2, aom_highbd_dc_predictor_32x32_avx512);
@@ -731,7 +947,11 @@ void svt_aom_setup_common_rtcd_internal(EbCpuFlags flags) {
     SET_AVX2_AVX512(svt_aom_highbd_h_predictor_64x16, svt_aom_highbd_h_predictor_64x16_c, svt_aom_highbd_h_predictor_64x16_avx2, aom_highbd_h_predictor_64x16_avx512);
     SET_AVX2_AVX512(svt_aom_highbd_h_predictor_64x32, svt_aom_highbd_h_predictor_64x32_c, svt_aom_highbd_h_predictor_64x32_avx2, aom_highbd_h_predictor_64x32_avx512);
     SET_AVX2_AVX512(svt_aom_highbd_h_predictor_64x64, svt_aom_highbd_h_predictor_64x64_c, svt_aom_highbd_h_predictor_64x64_avx2, aom_highbd_h_predictor_64x64_avx512);
+#ifdef ARCH_AARCH64
+    SET_ONLY_C(svt_log2f, svt_aom_log2f_32);
+#else
     SET_SSE2(svt_log2f, svt_aom_log2f_32, Log2f_ASM);
+#endif
     SET_SSE2(svt_memcpy, svt_memcpy_c, svt_memcpy_intrin_sse);
     SET_AVX2(svt_aom_hadamard_32x32, svt_aom_hadamard_32x32_c, svt_aom_hadamard_32x32_avx2);
     SET_AVX2(svt_aom_hadamard_16x16, svt_aom_hadamard_16x16_c, svt_aom_hadamard_16x16_avx2);
diff --git a/Source/Lib/Common/Codec/common_dsp_rtcd.h b/Source/Lib/Common/Codec/common_dsp_rtcd.h
index 7f96465..ae40860 100644
--- a/Source/Lib/Common/Codec/common_dsp_rtcd.h
+++ b/Source/Lib/Common/Codec/common_dsp_rtcd.h
@@ -37,7 +37,10 @@
 #define HAS_AVX512PF EB_CPU_FLAGS_AVX512PF
 #define HAS_AVX512BW EB_CPU_FLAGS_AVX512BW
 #define HAS_AVX512VL EB_CPU_FLAGS_AVX512VL
-
+#ifdef ARCH_AARCH64
+#define HAS_NEON     EB_CPU_FLAGS_NEON
+#endif
+ 
 #ifdef RTCD_C
 #define RTCD_EXTERN                //CHKN RTCD call in effect. declare the function pointers in  encHandle.
 #else
@@ -58,7 +61,7 @@ extern "C" {
 #endif
 
     // Helper Functions
-#ifdef ARCH_X86_64
+#if defined(ARCH_X86_64) || defined(ARCH_AARCH64)
     EbCpuFlags svt_aom_get_cpu_flags();
     EbCpuFlags svt_aom_get_cpu_flags_to_use();
 #endif
@@ -219,9 +222,15 @@ extern "C" {
     void svt_av1_highbd_convolve_x_sr_c(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
     RTCD_EXTERN void(*svt_av1_highbd_convolve_x_sr)(const uint16_t *src, int32_t src_stride, uint16_t *dst, int32_t dst_stride, int32_t w, int32_t h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params, int32_t bd);
     void svt_aom_convolve8_horiz_c(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h);
+#ifdef ARCH_AARCH64
+    void svt_aom_convolve8_horiz_ssse3_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h);
+#endif
     RTCD_EXTERN void(*svt_aom_convolve8_horiz)(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h);
     void svt_aom_convolve8_vert_c(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h);
-    RTCD_EXTERN void(*svt_aom_convolve8_vert)(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h);
+#ifdef ARCH_AARCH64
+    void svt_aom_convolve8_vert_ssse3_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h);
+#endif
+RTCD_EXTERN void(*svt_aom_convolve8_vert)(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h);
     void svt_av1_build_compound_diffwtd_mask_c(uint8_t *mask, DIFFWTD_MASK_TYPE mask_type, const uint8_t *src0, int src0_stride, const uint8_t *src1, int src1_stride, int h, int w);
     RTCD_EXTERN void(*svt_av1_build_compound_diffwtd_mask)(uint8_t *mask, DIFFWTD_MASK_TYPE mask_type, const uint8_t *src0, int src0_stride, const uint8_t *src1, int src1_stride, int h, int w);
     void svt_av1_build_compound_diffwtd_mask_highbd_c(uint8_t *mask, DIFFWTD_MASK_TYPE mask_type, const uint8_t *src0, int src0_stride, const uint8_t *src1, int src1_stride, int h, int w, int bd);
@@ -1090,7 +1099,7 @@ extern "C" {
     void svt_aom_hadamard_4x4_c(const int16_t* src_diff, ptrdiff_t src_stride, int32_t* coeff);
     #define svt_aom_hadamard_4x4 svt_aom_hadamard_4x4_c
 
-#ifdef ARCH_X86_64
+#if defined(ARCH_X86_64) || defined(ARCH_AARCH64)
 
     void svt_aom_blend_a64_vmask_sse4_1(uint8_t *dst, uint32_t dst_stride, const uint8_t *src0, uint32_t src0_stride, const uint8_t *src1, uint32_t src1_stride, const uint8_t *mask, int w, int h);
 
@@ -1283,6 +1292,9 @@ extern "C" {
 
     void svt_av1_highbd_wiener_convolve_add_src_ssse3(const uint8_t *const src, const ptrdiff_t src_stride, uint8_t *const dst, const ptrdiff_t dst_stride, const int16_t *const filter_x, const int16_t *const filter_y, const int32_t w, const int32_t h, const ConvolveParams *const conv_params, const int32_t bd);
     void svt_av1_highbd_wiener_convolve_add_src_avx2(const uint8_t *const src, const ptrdiff_t src_stride, uint8_t *const dst, const ptrdiff_t dst_stride, const int16_t *const filter_x, const int16_t *const filter_y, const int32_t w, const int32_t h, const ConvolveParams *const conv_params, const int32_t bd);
+#ifdef ARCH_AARCH64
+    void svt_av1_wiener_convolve_add_src_neon(const uint8_t *const src, const ptrdiff_t src_stride, uint8_t *const dst, const ptrdiff_t dst_stride, const int16_t *const filter_x, const int16_t *const filter_y, const int32_t w, const int32_t h, const ConvolveParams *const conv_params);
+#endif
 
     void svt_apply_selfguided_restoration_sse4_1(const uint8_t *dat, int32_t width, int32_t height, int32_t stride, int32_t eps, const int32_t *xqd, uint8_t *dst, int32_t dst_stride, int32_t *tmpbuf, int32_t bit_depth, int32_t highbd);
     void svt_apply_selfguided_restoration_avx2(const uint8_t *dat, int32_t width, int32_t height, int32_t stride, int32_t eps, const int32_t *xqd, uint8_t *dst, int32_t dst_stride, int32_t *tmpbuf, int32_t bit_depth, int32_t highbd);
@@ -1298,6 +1310,9 @@ extern "C" {
     void svt_av1_convolve_2d_sr_avx2(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
     void svt_av1_convolve_2d_sr_avx512(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
 
+#ifdef ARCH_AARCH64
+    void svt_av1_convolve_2d_sr_neon(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
+#endif
     void svt_av1_jnt_convolve_2d_copy_sse2(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
     void svt_av1_jnt_convolve_2d_copy_avx2(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
     void svt_av1_jnt_convolve_2d_copy_avx512(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
@@ -1305,7 +1320,10 @@ extern "C" {
     void svt_av1_convolve_x_sr_sse2(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
     void svt_av1_convolve_x_sr_avx2(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
     void svt_av1_convolve_x_sr_avx512(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
-
+#ifdef ARCH_AARCH64
+    void svt_av1_convolve_x_sr_neon(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
+    void svt_av1_convolve_y_sr_neon(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
+#endif
     void svt_av1_convolve_y_sr_sse2(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
     void svt_av1_convolve_y_sr_avx2(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
     void svt_av1_convolve_y_sr_avx512(const uint8_t *src, int32_t src_stride, uint8_t *dst, int32_t dst_stride, int32_t w, int32_t h, InterpFilterParams *filter_params_x, InterpFilterParams *filter_params_y, const int32_t subpel_x_q4, const int32_t subpel_y_q4, ConvolveParams *conv_params);
diff --git a/Source/Lib/Common/Codec/simd.h b/Source/Lib/Common/Codec/simd.h
new file mode 100644
index 0000000..84afee0
--- /dev/null
+++ b/Source/Lib/Common/Codec/simd.h
@@ -0,0 +1,20 @@
+#ifndef SIMD_H_
+#define SIMD_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#ifdef ARCH_AARCH64
+    // SSE emulations on ARM Neon.
+    #include "sse2neon.h"
+#elif defined(ARCH_X86_64)
+    // Native SSE
+    #include <smmintrin.h>
+#endif
+
+#ifdef __cplusplus
+} // extern "C"
+#endif
+
+#endif // SIMD_H_
diff --git a/Source/Lib/Decoder/CMakeLists.txt b/Source/Lib/Decoder/CMakeLists.txt
index 0f220a7..7d9f5fa 100644
--- a/Source/Lib/Decoder/CMakeLists.txt
+++ b/Source/Lib/Decoder/CMakeLists.txt
@@ -134,6 +134,16 @@ if(NOT COMPILE_C_ONLY AND HAVE_X86_PLATFORM)
         $<TARGET_OBJECTS:COMMON_ASM_SSE4_1>
         $<TARGET_OBJECTS:COMMON_ASM_AVX2>
         $<TARGET_OBJECTS:COMMON_ASM_AVX512>)
+elseif(NOT COMPILE_C_ONLY AND HAVE_AARCH64_PLATFORM)
+    add_library(SvtAv1Dec
+        ${all_files}
+        $<TARGET_OBJECTS:COMMON_CODEC>
+        $<TARGET_OBJECTS:FASTFEAT>
+        $<TARGET_OBJECTS:COMMON_C_DEFAULT>
+        $<TARGET_OBJECTS:COMMON_ASM_SSE2>
+        $<TARGET_OBJECTS:COMMON_ASM_SSSE3>
+        $<TARGET_OBJECTS:COMMON_ASM_SSE4_1>
+        $<TARGET_OBJECTS:COMMON_ASM_NEON>)
 else()
     add_library(SvtAv1Dec
         ${all_files}
diff --git a/Source/Lib/Encoder/ASM_NEON/CMakeLists.txt b/Source/Lib/Encoder/ASM_NEON/CMakeLists.txt
new file mode 100644
index 0000000..7842f0c
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/CMakeLists.txt
@@ -0,0 +1,15 @@
+# ASM_NEON Directory CMakeLists.txt
+
+# Include Encoder Subdirectories
+include_directories(../../../API
+        ../../Encoder/Codec
+    ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/C_DEFAULT/
+    ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_NEON/)
+link_directories(${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_NEON/)
+
+file(GLOB all_files
+    "*.h"
+    "*.c")
+
+add_library(ENCODER_ASM_NEON OBJECT ${all_files})
+
diff --git a/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_Intrinsic_NEON.c b/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_Intrinsic_NEON.c
new file mode 100644
index 0000000..8ebe9b3
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_Intrinsic_NEON.c
@@ -0,0 +1,470 @@
+/*
+* Copyright(c) 2019 Intel Corporation
+*
+* This source code is subject to the terms of the BSD 2 Clause License and
+* the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+* was not distributed with this source code in the LICENSE file, you can
+* obtain it at https://www.aomedia.org/license/software-license. If the Alliance for Open
+* Media Patent License 1.0 was not distributed with this source code in the
+* PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
+*/
+
+#include <assert.h>
+
+#ifndef ARCH_AARCH64
+#error "ARM neon support is required"
+#endif
+
+#include "EbDefinitions.h"
+#include "EbComputeSAD_C.h"
+#include "EbUtility.h"
+#include <arm_neon.h>
+
+#include "EbComputeSAD.h"
+#include "mcomp.h"
+
+uint32_t sad_sad(uint16_t width, uint16_t height, uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride)
+{
+    uint32_t sad = 0;
+    for (int y = 0;y < height;y++) {
+        for (int x = 0;x < width;x++) {
+            sad += EB_ABS_DIFF(src[x + y * src_stride], ref[x + y * ref_stride]);
+        }
+    }
+    return sad;
+}
+FORCE_INLINE uint32_t get_idx_of_element(uint16x8_t v, uint16_t e) {
+    const uint16x8_t kVl = { 0x8000, 0x4000, 0x2000, 0x1000, 0x0800, 0x0400, 0x0200, 0x0100 };
+    return __builtin_clz(vaddvq_u16(vandq_u16(kVl, vceqq_u16(v, vdupq_n_u16(e))))) - 16;
+}
+
+#define SAD8x1_ABL(a,s,r) a = vabal_u8(a, s, r);
+#define SAD8x2x1_ABL(a0,a1,s,r) a0 = vabal_u8(a0, vget_low_u8(s), vget_low_u8(r));\
+                              a1 = vabal_high_u8(a1, s, r);
+
+uint32_t SAD4xN(uint16_t width, uint16_t height, uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride) {
+    (void)width;
+    uint16x8_t acc8a = vdupq_n_u16(0);
+#pragma unroll 4
+    for (int y = 0; y < height;y++) {
+        SAD8x1_ABL(acc8a, vld1_u8(src), vld1_u8(ref));
+        src += src_stride;
+        ref += ref_stride;
+    }
+    return vaddv_u16(vget_low_u16(acc8a));
+}
+uint32_t SAD6xN(uint16_t width, uint16_t height, uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride) {
+    (void)width;
+    uint16x8_t acc8a = vdupq_n_u16(0);
+#pragma unroll 4
+    for (int y = 0; y < height;y++) {
+        SAD8x1_ABL(acc8a, vld1_u8(src), vld1_u8(ref));
+        src += src_stride;
+        ref += ref_stride;
+    }
+    const uint16x8_t mask6 = { 0xFFFF,0xFFFF,0xFFFF,0xFFFF, 0xFFFF,0xFFFF,0x0000,0x0000 };
+    acc8a = vandq_u16(acc8a, mask6);
+    return vaddvq_u16(acc8a);
+}
+
+uint32_t SAD8xN(uint16_t width, uint16_t height, uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride) {
+    (void)width;
+
+    uint16x8_t acc8a = vdupq_n_u16(0);
+ #pragma unroll 8
+    for (int y = 0; y < height;y++) {
+        SAD8x1_ABL(acc8a, vld1_u8(src), vld1_u8(ref));
+        src += src_stride;
+        ref += ref_stride;
+    }
+    uint32x4_t res = vpaddlq_u16(acc8a);
+    return vaddvq_u32(res);
+}
+uint32_t SAD12xN(uint16_t width, uint16_t height, uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride) {
+    (void)width;
+
+    uint16x8_t acc8a = vdupq_n_u16(0);
+    uint16x8_t acc8b = vdupq_n_u16(0);
+#pragma unroll 4
+    for (int y = 0; y < height;y++) {
+        SAD8x2x1_ABL(acc8a, acc8b, vld1q_u8(src), vld1q_u8(ref));
+        src += src_stride;
+        ref += ref_stride;
+    }
+    uint32x4_t res = vpaddlq_u16(acc8a);
+
+    return vaddvq_u32(res) + vaddv_u16(vget_low_u16(acc8b));
+}
+uint32_t SAD16xN(uint16_t width, uint16_t height, uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride) {
+    (void)width;
+
+    uint16x8_t acc8a = vdupq_n_u16(0);
+    uint16x8_t acc8b = vdupq_n_u16(0);
+#pragma unroll 8
+    for (int y = 0; y < height;y++) {
+        SAD8x2x1_ABL(acc8a, acc8b, vld1q_u8(src), vld1q_u8(ref));
+        src += src_stride;
+        ref += ref_stride;
+    }
+    uint32x4_t res = vpaddlq_u16(acc8a + acc8b);
+    return vaddvq_u32(res);
+
+}
+
+uint32_t SAD24xN(uint16_t width, uint16_t height, uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride) {
+    (void)width;
+    return SAD16xN(0, height, src, src_stride, ref, ref_stride) +
+        SAD8xN(0, height, src + 16, src_stride, ref + 16, ref_stride);
+}
+
+uint32_t SAD32xN(uint16_t width, uint16_t height,
+    uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride) {
+    (void)width;
+
+    uint16x8_t acc8a = vdupq_n_u16(0);
+    uint16x8_t acc8b = vdupq_n_u16(0);
+    uint16x8_t acc8c = vdupq_n_u16(0);
+    uint16x8_t acc8d = vdupq_n_u16(0);
+#pragma unroll 8
+    for (int y = 0; y < height; y++) {
+        uint8x16x2_t s0 = vld1q_u8_x2(src);
+        uint8x16x2_t r0 = vld1q_u8_x2(ref);
+        SAD8x2x1_ABL(acc8a, acc8b,s0.val[0], r0.val[0]);
+        SAD8x2x1_ABL(acc8c, acc8d, s0.val[1], r0.val[1]);
+        src += src_stride;
+        ref += ref_stride;
+    }
+    uint32x4_t res = vpaddlq_u16(acc8a + acc8b + acc8c + acc8d);
+    return vaddvq_u32(res);
+
+}
+uint32_t SAD48xN(uint16_t width, uint16_t height,
+    uint8_t* __restrict src, uint16_t src_stride, uint8_t* __restrict ref, uint16_t ref_stride) {
+    (void)width;
+    uint32_t res = 0;
+
+    res += SAD32xN(0, height, src, src_stride, ref, ref_stride);
+    res += SAD16xN(0, height, src + 32, src_stride, ref + 32, ref_stride);
+
+    return res;
+}
+
+uint32_t SAD64xN(uint16_t width, uint16_t height,
+                uint8_t* __restrict src, uint16_t src_stride,
+                uint8_t* __restrict ref, uint16_t ref_stride) {
+
+    (void)width;
+    return SAD32xN(0, height, src, src_stride, ref, ref_stride) +
+        SAD32xN(0, height, src + 32, src_stride, ref + 32, ref_stride);
+}
+
+/// calculate sad for 8xHx2 blocks
+// over W consecutive positions
+FORCE_INLINE  uint32_t sad_8xHx2x8w(const uint32_t H, uint8_t* __restrict src, uint32_t src_stride,
+    uint8_t* __restrict ref, uint32_t ref_stride,
+    uint16_t* __restrict res_low, uint16_t* __restrict res_high) {
+    uint32_t best_sad = 0xffffff;
+    uint32_t best_pos = 0;
+
+    uint16x8_t vsads_low[8];
+    uint16x8_t vsads_high[8];
+#pragma unroll
+    for (uint32_t w = 0;w < 8;w++) {
+        vsads_low[w] = vdupq_n_u16(0);
+        vsads_high[w] = vdupq_n_u16(0);
+    }
+#define SAD8x2_ABL(s,f,r,l,h) l[f] = vabal_u8(l[f], vget_low_u8(s), vget_low_u8(r));\
+                              h[f] = vabal_high_u8(h[f], s, r);
+    for (uint32_t y = 0;y < H;y += 2) {
+        const uint8x16_t s0 = vld1q_u8(src);
+        const uint8x16_t r0 = vld1q_u8(ref);
+        const uint8x16_t r1 = vld1q_u8(ref + 16);
+        src += src_stride;
+        ref += ref_stride;
+        const uint8x16_t s1 = vld1q_u8(src);
+        SAD8x2_ABL(s0, 0, r0, vsads_low, vsads_high);
+        SAD8x2_ABL(s1, 0, vld1q_u8(ref + 0), vsads_low, vsads_high);
+        SAD8x2_ABL(s0, 1, vextq_u8(r0, r1, 1), vsads_low, vsads_high);
+        SAD8x2_ABL(s1, 1, vld1q_u8(ref + 1), vsads_low, vsads_high);
+        SAD8x2_ABL(s0, 2, vextq_u8(r0, r1, 2), vsads_low, vsads_high);
+        SAD8x2_ABL(s1, 2, vld1q_u8(ref + 2), vsads_low, vsads_high);
+        SAD8x2_ABL(s0, 3, vextq_u8(r0, r1, 3), vsads_low, vsads_high);
+        SAD8x2_ABL(s1, 3, vld1q_u8(ref + 3), vsads_low, vsads_high);
+        SAD8x2_ABL(s0, 4, vextq_u8(r0, r1, 4), vsads_low, vsads_high);
+        SAD8x2_ABL(s1, 4, vld1q_u8(ref + 4), vsads_low, vsads_high);
+        SAD8x2_ABL(s0, 5, vextq_u8(r0, r1, 5), vsads_low, vsads_high);
+        SAD8x2_ABL(s1, 5, vld1q_u8(ref + 5), vsads_low, vsads_high);
+        SAD8x2_ABL(s0, 6, vextq_u8(r0, r1, 6), vsads_low, vsads_high);
+        SAD8x2_ABL(s1, 6, vld1q_u8(ref + 6), vsads_low, vsads_high);
+        SAD8x2_ABL(s0, 7, vextq_u8(r0, r1, 7), vsads_low, vsads_high);
+        SAD8x2_ABL(s1, 7, vld1q_u8(ref + 7), vsads_low, vsads_high);
+        src += src_stride;
+        ref += ref_stride;
+    }
+
+#pragma unroll
+    for (uint32_t w = 0;w < 8;w++) {
+        res_low[w] = vaddvq_u16(vsads_low[w]);
+        res_high[w] = vaddvq_u16(vsads_high[w]);
+        if (res_low[w] + res_high[w] < best_sad) {
+            best_sad = res_low[w] + res_high[w];
+            best_pos = w;
+        }
+    }
+    return (best_sad << 3) | best_pos;
+}
+FORCE_INLINE  uint32_t sad_8xHx1x8w(const uint32_t H, uint8_t* __restrict src, uint32_t src_stride,
+    uint8_t* __restrict ref, uint32_t ref_stride) {
+    uint32_t best_sad = 0xffffff;
+    uint32_t best_pos = 0;
+
+    uint16x8_t accs[8];
+#pragma unroll
+    for (uint32_t w = 0;w < 8;w++) {
+        accs[w] = vdupq_n_u16(0);
+    }
+
+    #pragma unroll 4
+    for (uint32_t y = 0;y < H;y ++) {
+        const uint8x8_t s0 = vld1_u8(src);
+        const uint8x8x2_t r = vld1_u8_x2(ref);
+        accs[0] = vabal_u8(accs[0], s0,r.val[0]);
+        accs[1] = vabal_u8(accs[1], s0,vld1_u8(ref + 1));
+        accs[2] = vabal_u8(accs[2], s0,vext_u8(r.val[0], r.val[1], 2));
+        accs[3] = vabal_u8(accs[3], s0,vld1_u8(ref + 3));
+        accs[4] = vabal_u8(accs[4], s0,vext_u8(r.val[0], r.val[1], 4));
+        accs[5] = vabal_u8(accs[5], s0,vld1_u8(ref + 5));
+        accs[6] = vabal_u8(accs[6], s0,vext_u8(r.val[0], r.val[1], 6));
+        accs[7] = vabal_u8(accs[7], s0,vld1_u8(ref + 7));
+
+        src += src_stride;
+        ref += ref_stride;
+    }
+
+#pragma unroll
+    for (uint32_t w = 0;w < 8;w++) {
+        uint32_t sad = vaddvq_u16(accs[w]);
+        if (sad < best_sad) {
+            best_sad = sad;
+            best_pos = w;
+        }
+    }
+    return (best_sad << 3) | best_pos;
+}
+
+typedef uint32_t(*SAD_KernelNxM_t)(
+    uint16_t width,
+    uint16_t height,
+    uint8_t* __restrict src, uint16_t src_stride,
+    uint8_t* __restrict ref, uint16_t ref_stride);
+
+
+/*******************************************************************************
+ * Requirement: width   = 4, 6, 8, 12, 16, 24, 32, 48 or 64 to use SIMD
+ * otherwise C version is used
+ * Requirement: block_height <= 64
+ * Requirement: block_height % 2 = 0 when width = 4, 6 or 8
+*******************************************************************************/
+void svt_sad_loop_kernel_neon_intrin(
+    uint8_t* src, // input parameter, source samples Ptr
+    uint32_t  src_stride, // input parameter, source stride
+    uint8_t* ref, // input parameter, reference samples Ptr
+    uint32_t  ref_stride, // input parameter, reference stride
+    uint32_t  block_height, // input parameter, block height (M)
+    uint32_t  block_width, // input parameter, block width (N)
+    uint64_t* best_sad, int16_t* x_search_center, int16_t* y_search_center,
+    uint32_t src_stride_raw, // input parameter, source stride (no line skipping)
+    uint8_t skip_search_line, int16_t search_area_width, int16_t search_area_height) {
+    int16_t        x_best = *x_search_center, y_best = *y_search_center;
+    uint32_t       low_sum = 0xffffff;
+    uint32_t       tem_sum = 0;
+    int16_t        i, j;
+    const bool is_skip = skip_search_line && (block_height <= 16) && (block_width == 16);
+    if (is_skip) {
+        ref += src_stride_raw;
+        src_stride_raw <<= 1;
+        search_area_height >>= 1;
+    }
+
+    if (block_width == 16 && !(block_height & 1) && (block_height <= 32)) {
+        const uint16_t reminder = search_area_width % 8;
+        for (i = 0; i < search_area_height; i++) {
+            const uint16_t y = is_skip ? 1 + (i << 1) : i;
+            for (j = 0; j < search_area_width - reminder; j += 8) {
+                uint16_t sads8low[8];
+                uint16_t sads8high[8];
+                uint32_t best8 = sad_8xHx2x8w(block_height, src, src_stride, ref + j, ref_stride, sads8low, sads8high);
+                if ((best8 >> 3) < low_sum) {
+                    low_sum = best8 >> 3;
+                    x_best = j + (best8 & 0x7);
+                    y_best = y;
+                }
+            }
+            for (; j < search_area_width; j++) {
+                tem_sum = SAD16xN(block_width, block_height, src, src_stride, ref + j, ref_stride);
+                if (tem_sum < low_sum) {
+                    low_sum = tem_sum;
+                    x_best = j;
+                    y_best = y;
+                }
+            }
+
+            ref += src_stride_raw;
+        }
+
+    }
+    else if (block_width == 8 && (block_height <= 32)) {
+        const uint16_t reminder = search_area_width % 8;
+        for (i = 0; i < search_area_height; i++) {
+            for (j = 0; j < search_area_width - reminder; j += 8) {
+                uint32_t best8 = sad_8xHx1x8w(block_height, src, src_stride, ref + j, ref_stride);
+                if ((best8 >> 3) < low_sum) {
+                    low_sum = best8 >> 3;
+                    x_best = j + (best8 & 0x7);
+                    y_best = i;
+                }
+            }
+            for (; j < search_area_width; j++) {
+                tem_sum = SAD8xN(block_width, block_height, src, src_stride, ref + j, ref_stride);
+                if (tem_sum < low_sum) {
+                    low_sum = tem_sum;
+                    x_best = j;
+                    y_best = i;
+                }
+            }
+            ref += src_stride_raw;
+        }
+    }
+    else {
+        const SAD_KernelNxM_t SADFunckList[] = { sad_sad,SAD4xN, SAD8xN, SAD12xN, SAD16xN, sad_sad, SAD24xN,
+    sad_sad, SAD32xN, sad_sad, sad_sad, sad_sad, SAD48xN, sad_sad, sad_sad, sad_sad, SAD64xN };
+        SAD_KernelNxM_t fp_sad = SADFunckList[block_width >> 2];
+        if (block_width & 3) {
+            if (block_width == 6)
+                fp_sad = SAD6xN;
+            else
+                fp_sad = sad_sad;
+        }
+        for (i = 0; i < search_area_height; i++) {
+            const uint16_t y = is_skip ? 1 + (i << 1) : i;
+            for (j = 0; j < search_area_width; j++) {
+                tem_sum = fp_sad(block_width, block_height, src, src_stride, ref + j, ref_stride);
+                if (tem_sum < low_sum) {
+                    low_sum = tem_sum;
+                    x_best = j;
+                    y_best = y;
+                }
+            }
+            ref += src_stride_raw;
+        }
+    }
+    *best_sad = low_sum;
+    *x_search_center = x_best;
+    *y_search_center = y_best;
+}
+
+
+/*******************************************
+ * svt_ext_eight_sad_calculation_8x8_16x16
+ *******************************************/
+FORCE_INLINE void svt_ext_eight_sad_calculation_8x8_16x16_neon(
+    uint8_t* src, uint32_t src_stride, uint8_t* ref, uint32_t ref_stride, uint32_t mv,
+    uint32_t start_16x16_pos, uint32_t* p_best_sad_8x8, uint32_t* p_best_sad_16x16,
+    uint32_t* p_best_mv8x8, uint32_t* p_best_mv16x16, uint32_t p_eight_sad16x16[16][8],
+    uint32_t p_eight_sad8x8[64][8], Bool sub_sad) {
+    const uint32_t start_8x8_pos = 4 * start_16x16_pos;
+    int16_t x_mv = _MVXT(mv);
+    uint32_t y_mv =  _MVYT(mv) << 16;
+
+    (void)p_eight_sad8x8;
+
+    p_best_sad_8x8 += start_8x8_pos;
+    p_best_mv8x8 += start_8x8_pos;
+    p_best_sad_16x16 += start_16x16_pos;
+    p_best_mv16x16 += start_16x16_pos;
+    const uint32_t scale = (sub_sad) ? (1) : (0);
+    uint16_t sad8[4][8];
+    sad_8xHx2x8w((8 >> scale), src, (src_stride << scale), ref,
+        (ref_stride << scale), sad8[0], sad8[1]);
+    sad_8xHx2x8w((8 >> scale), src + (src_stride << 3), (src_stride << scale), ref + (ref_stride << 3),
+        (ref_stride << scale), sad8[2], sad8[3]);
+
+    uint16x8_t vsad80 = vld1q_u16(sad8[0]) << scale;
+    uint16x8_t vsad81 = vld1q_u16(sad8[1]) << scale;
+    uint16x8_t vsad82 = vld1q_u16(sad8[2]) << scale;
+    uint16x8_t vsad83 = vld1q_u16(sad8[3]) << scale;
+
+    uint16_t sad0 = vminvq_u16(vsad80);
+    uint16_t sad1 = vminvq_u16(vsad81);
+    uint16_t sad2 = vminvq_u16(vsad82);
+    uint16_t sad3 = vminvq_u16(vsad83);
+
+    uint16x8_t vsad16 = (vsad80 + vsad81 + vsad82 + vsad83);
+    uint16_t sad16 = vminvq_u16(vsad16);
+
+    if ((sad0) < p_best_sad_8x8[0]) {
+        p_best_sad_8x8[0] = (uint32_t)(sad0);
+        uint16_t i = get_idx_of_element(vsad80, sad0);
+        x_mv = _MVXT(mv) + (int16_t)i * 4;
+        p_best_mv8x8[0] = (y_mv) | ((uint16_t)x_mv);
+    }
+    if ((sad1) < p_best_sad_8x8[1]) {
+        p_best_sad_8x8[1] = (uint32_t)(sad1);
+        uint16_t i = get_idx_of_element(vsad81, sad1);
+        x_mv = _MVXT(mv) + (int16_t)i * 4;
+        p_best_mv8x8[1] = (y_mv) | ((uint16_t)x_mv);
+    }
+    if ((sad2) < p_best_sad_8x8[2]) {
+        p_best_sad_8x8[2] = (uint32_t)(sad2);
+        uint16_t i = get_idx_of_element(vsad82, sad2);
+        x_mv = _MVXT(mv) + (int16_t)i * 4;
+        p_best_mv8x8[2] = (y_mv) | ((uint16_t)x_mv);
+    }
+    if ((sad3) < p_best_sad_8x8[3]) {
+        p_best_sad_8x8[3] = (uint32_t)(sad3);
+        uint16_t i = get_idx_of_element(vsad83, sad3);
+        x_mv = _MVXT(mv) + (int16_t)i * 4;
+        p_best_mv8x8[3] = (y_mv) | ((uint16_t)x_mv);
+    }
+
+    vst1q_u32(p_eight_sad16x16[start_16x16_pos], vmovl_u16(vget_low_u16(vsad16)));
+    vst1q_u32(p_eight_sad16x16[start_16x16_pos] + 4, vmovl_high_u16(vsad16));
+    if (sad16 < p_best_sad_16x16[0]) {
+        p_best_sad_16x16[0] = (uint32_t)sad16;
+        uint16_t i = get_idx_of_element(vsad16, sad16);
+        x_mv = _MVXT(mv) + (int16_t)i * 4;
+        p_best_mv16x16[0] = (y_mv) | ((uint16_t)x_mv);
+    }
+}
+
+void svt_ext_all_sad_calculation_8x8_16x16_neon(uint8_t* src, uint32_t src_stride, uint8_t* ref,
+    uint32_t ref_stride, uint32_t mv, uint8_t out_8x8,
+    uint32_t* p_best_sad_8x8,
+    uint32_t* p_best_sad_16x16,
+    uint32_t* p_best_mv8x8, uint32_t* p_best_mv16x16,
+    uint32_t p_eight_sad16x16[16][8],
+    uint32_t p_eight_sad8x8[64][8], Bool sub_sad) {
+    static const char offsets[16] = { 0, 1, 4, 5, 2, 3, 6, 7, 8, 9, 12, 13, 10, 11, 14, 15 };
+    (void)out_8x8;
+    //---- 16x16 : 0, 1, 4, 5, 2, 3, 6, 7, 8, 9, 12, 13, 10, 11, 14, 15
+    for (int y = 0; y < 4; y++) {
+        for (int x = 0; x < 4; x++) {
+            const uint32_t block_index = 16 * y * src_stride + 16 * x;
+            const uint32_t search_position_index = 16 * y * ref_stride + 16 * x;
+            svt_ext_eight_sad_calculation_8x8_16x16_neon(src + block_index,
+                src_stride,
+                ref + search_position_index,
+                ref_stride,
+                mv,
+                offsets[4 * y + x],
+                p_best_sad_8x8,
+                p_best_sad_16x16,
+                p_best_mv8x8,
+                p_best_mv16x16,
+                p_eight_sad16x16,
+                p_eight_sad8x8,
+                sub_sad);
+        }
+    }
+}
+
diff --git a/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_NEON.h b/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_NEON.h
new file mode 100644
index 0000000..67bcbc9
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/EbComputeSAD_NEON.h
@@ -0,0 +1,79 @@
+/*
+* Copyright(c) 2019 Intel Corporation
+*
+* This source code is subject to the terms of the BSD 2 Clause License and
+* the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+* was not distributed with this source code in the LICENSE file, you can
+* obtain it at https://www.aomedia.org/license/software-license. If the Alliance for Open
+* Media Patent License 1.0 was not distributed with this source code in the
+* PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
+*/
+
+#ifndef EbComputeSAD_NEON_h
+#define EbComputeSAD_NEON_h
+
+#include "EbDefinitions.h"
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+uint32_t svt_compute8x_m_sad_neon_intrin(
+    const uint8_t *src, // input parameter, source samples Ptr
+    uint32_t       src_stride, // input parameter, source stride
+    const uint8_t *ref, // input parameter, reference samples Ptr
+    uint32_t       ref_stride, // input parameter, reference stride
+    uint32_t       height, // input parameter, block height (M)
+    uint32_t       width); // input parameter, block width (N)
+
+uint32_t svt_compute16x_m_sad_neon_intrin(
+    const uint8_t *src, // input parameter, source samples Ptr
+    uint32_t       src_stride, // input parameter, source stride
+    const uint8_t *ref, // input parameter, reference samples Ptr
+    uint32_t       ref_stride, // input parameter, reference stride
+    uint32_t       height, // input parameter, block height (M)
+    uint32_t       width); // input parameter, block width (N)
+
+uint32_t svt_compute24x_m_sad_neon_intrin(
+    const uint8_t *src, // input parameter, source samples Ptr
+    uint32_t       src_stride, // input parameter, source stride
+    const uint8_t *ref, // input parameter, reference samples Ptr
+    uint32_t       ref_stride, // input parameter, reference stride
+    uint32_t       height, // input parameter, block height (M)
+    uint32_t       width); // input parameter, block width (N)
+
+uint32_t svt_compute32x_m_sad_neon_intrin(
+    const uint8_t *src, // input parameter, source samples Ptr
+    uint32_t       src_stride, // input parameter, source stride
+    const uint8_t *ref, // input parameter, reference samples Ptr
+    uint32_t       ref_stride, // input parameter, reference stride
+    uint32_t       height, // input parameter, block height (M)
+    uint32_t       width); // input parameter, block width (N)
+
+uint32_t svt_compute48x_m_sad_neon_intrin(
+    const uint8_t *src, // input parameter, source samples Ptr
+    uint32_t       src_stride, // input parameter, source stride
+    const uint8_t *ref, // input parameter, reference samples Ptr
+    uint32_t       ref_stride, // input parameter, reference stride
+    uint32_t       height, // input parameter, block height (M)
+    uint32_t       width); // input parameter, block width (N)
+
+uint32_t svt_compute64x_m_sad_neon_intrin(
+    const uint8_t *src, // input parameter, source samples Ptr
+    uint32_t       src_stride, // input parameter, source stride
+    const uint8_t *ref, // input parameter, reference samples Ptr
+    uint32_t       ref_stride, // input parameter, reference stride
+    uint32_t       height, // input parameter, block height (M)
+    uint32_t       width); // input parameter, block width (N)
+
+uint32_t svt_compute128x_m_sad_neon_intrin(
+    const uint8_t *src, // input parameter, source samples Ptr
+    uint32_t       src_stride, // input parameter, source stride
+    const uint8_t *ref, // input parameter, reference samples Ptr
+    uint32_t       ref_stride, // input parameter, reference stride
+    uint32_t       height, // input parameter, block height (M)
+    uint32_t       width); // input parameter, block width (N)
+
+#ifdef __cplusplus
+}
+#endif
+#endif // EbComputeSAD_NEON_h
diff --git a/Source/Lib/Encoder/ASM_NEON/EbRestorationPick_NEON.c b/Source/Lib/Encoder/ASM_NEON/EbRestorationPick_NEON.c
new file mode 100644
index 0000000..45701d2
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/EbRestorationPick_NEON.c
@@ -0,0 +1,304 @@
+/*
+* Copyright(c) 2019 Intel Corporation
+*
+* This source code is subject to the terms of the BSD 2 Clause License and
+* the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+* was not distributed with this source code in the LICENSE file, you can
+* obtain it at https://www.aomedia.org/license/software-license. If the Alliance for Open
+* Media Patent License 1.0 was not distributed with this source code in the
+* PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
+*/
+
+#ifndef ARCH_AARCH64
+#error "ARM neon support is required"
+#endif
+
+#include "EbDefinitions.h"
+#include "EbRestoration.h"
+#include <arm_neon.h>
+#include <math.h>
+
+static INLINE void neon_mul_s16_s32(const int16x8_t* a, const int16x8_t* b, int32x4_t* out) {
+    int32x4_t a_32[2];
+    int32x4_t b_32[2];
+
+    a_32[0] = vmovl_s16(vget_high_s16(*a));
+    a_32[1] = vmovl_s16(vget_low_s16(*a));
+
+    b_32[0] = vmovl_s16(vget_high_s16(*b));
+    b_32[1] = vmovl_s16(vget_low_s16(*b));
+
+    out[0] = vmulq_s32(a_32[0], b_32[0]);
+    out[1] = vmulq_s32(a_32[1], b_32[1]);
+}
+
+void svt_get_proj_subspace_neon(const uint8_t* __restrict src8, int width, int height, int src_stride,
+    const uint8_t* __restrict dat8, int dat_stride, int use_highbitdepth,
+    int32_t* __restrict flt0, int flt0_stride, int32_t* __restrict flt1, int flt1_stride,
+    int* xq, const SgrParamsType* params) {
+    int32_t       i, j;
+    double    H[2][2] = { {0, 0}, {0, 0} };
+    double    C[2] = { 0, 0 };
+    double    det;
+    double    x[2];
+    const int size = width * height;
+
+    // Default
+    xq[0] = 0;
+    xq[1] = 0;
+
+    int32x4_t h_00, h_01, h_11;
+    int32x4_t c_0, c_1;
+    h_00 = vdupq_n_s32(0);
+    h_01 = vdupq_n_s32(0);
+    h_11 = vdupq_n_s32(0);
+    c_0 = vdupq_n_s32(0);
+    c_1 = vdupq_n_s32(0);
+
+    int16x8_t u_256, s_256;
+    int16x8_t f1_256, f2_256;
+    int32x4_t out[2];
+
+    if (!use_highbitdepth) {
+        const uint8_t* src = src8;
+        const uint8_t* dat = dat8;
+        if (params->r[0] > 0 || params->r[1] > 0) {
+            if (params->r[0] > 0 && params->r[1] > 0) {
+                for (int i = 0; i < height; ++i) {
+                    int j = 0, neon_cnt = 0;
+
+                    for (; neon_cnt < width / 8; j += 8, ++neon_cnt) {
+                        u_256 = vreinterpretq_s16_u16(vshll_n_u8(vld1_u8(dat + i * dat_stride + j), SGRPROJ_RST_BITS));
+                        s_256 = vreinterpretq_s16_u16(vshll_n_u8(vld1_u8(src + i * src_stride + j), SGRPROJ_RST_BITS));
+                        s_256 = vsubq_s16(s_256, u_256);
+
+                        int16x8_t f1_256_temp_0 = vreinterpretq_s16_s32(vld1q_s32(
+                            (const int32_t*)(flt0 + i * flt0_stride + j)));
+                        int16x8_t f1_256_temp_1 = vreinterpretq_s16_s32(vld1q_s32(
+                            (const int32_t*)(flt0 + i * flt0_stride + j + 4)));
+
+                        f1_256 = vpaddq_s16(f1_256_temp_0, f1_256_temp_1);
+                        f1_256 = vsubq_s16(f1_256, u_256);
+
+                        int16x8_t f2_256_temp_0 = vreinterpretq_s16_s32(vld1q_s32(
+                            (const int32_t*)(flt1 + i * flt1_stride + j)));
+                        int16x8_t f2_256_temp_1 = vreinterpretq_s16_s32(vld1q_s32(
+                            (const int32_t*)(flt1 + i * flt1_stride + j + 4)));
+
+                        f2_256 = vpaddq_s16(f2_256_temp_0, f2_256_temp_1);
+                        f2_256 = vsubq_s16(f2_256, u_256);
+
+                        //    H[0][0] += f1 * f1;
+                        neon_mul_s16_s32(&f1_256, &f1_256, out);
+                        h_00 = vaddq_s32(h_00, out[0]);
+                        h_00 = vaddq_s32(h_00, out[1]);
+
+                        //    H[1][1] += f2 * f2;
+                        neon_mul_s16_s32(&f2_256, &f2_256, out);
+                        h_11 = vaddq_s32(h_11, out[0]);
+                        h_11 = vaddq_s32(h_11, out[1]);
+
+                        //    H[0][1] += f1 * f2;
+                        neon_mul_s16_s32(&f1_256, &f2_256, out);
+                        h_01 = vaddq_s32(h_01, out[0]);
+                        h_01 = vaddq_s32(h_01, out[1]);
+
+                        //    C[0] += f1 * s;
+                        neon_mul_s16_s32(&f1_256, &s_256, out);
+                        c_0 = vaddq_s32(c_0, out[0]);
+                        c_0 = vaddq_s32(c_0, out[1]);
+
+                        //    C[1] += f2 * s;
+                        neon_mul_s16_s32(&f2_256, &s_256, out);
+                        c_1 = vaddq_s32(c_1, out[0]);
+                        c_1 = vaddq_s32(c_1, out[1]);
+                    }
+
+                    //Complement when width not divided by 8
+                    int32_t  sh00 = 0;
+                    int32_t  sh01 = 0;
+                    int32_t  sh11 = 0;
+                    int32_t  sc0 = 0;
+                    int32_t  sc1 = 0;
+                    for (; j < width; ++j) {
+                        const int32_t u  = ((int32_t)(dat[i * dat_stride + j]) << SGRPROJ_RST_BITS);
+                        const int32_t s  = ((int32_t)(src[i * src_stride + j]) << SGRPROJ_RST_BITS) - u;
+                        const int32_t f1 = ((int32_t)flt0[i * flt0_stride + j]) - u;
+                        const int32_t f2 = ((int32_t)flt1[i * flt1_stride + j]) - u;
+                        sh00 += f1 * f1;
+                        sh11 += f2 * f2;
+                        sh01 += f1 * f2;
+                        sc0 += f1 * s;
+                        sc1 += f2 * s;
+                    }
+
+                    H[0][0] += (double)vaddlvq_s32(h_00) + sh00;
+                    H[1][1] += (double)vaddlvq_s32(h_11) + sh11;
+                    H[0][1] += (double)vaddlvq_s32(h_01) + sh01;
+
+                    C[0] += (double)vaddlvq_s32(c_0) + sc0;
+                    C[1] += (double)vaddlvq_s32(c_1) + sc1;
+
+                    h_00 = vdupq_n_s32(0);
+                    h_01 = vdupq_n_s32(0);
+                    h_11 = vdupq_n_s32(0);
+                    c_0  = vdupq_n_s32(0);
+                    c_1  = vdupq_n_s32(0);
+                }
+            }
+
+            else if (params->r[0] <= 0 && params->r[1] > 0) {
+                for (int i = 0; i < height; ++i) {
+                    int j = 0, neon_cnt = 0;
+
+                    for (; neon_cnt < width / 8; j += 8, ++neon_cnt) {
+                        u_256 = vreinterpretq_s16_u16(vshll_n_u8(vld1_u8(dat + i * dat_stride + j), SGRPROJ_RST_BITS));
+                        s_256 = vreinterpretq_s16_u16(vshll_n_u8(vld1_u8(src + i * src_stride + j), SGRPROJ_RST_BITS));
+                        s_256 = vsubq_s16(s_256, u_256);
+
+                        int16x8_t f2_256_temp_0 = vreinterpretq_s16_s32(vld1q_s32(
+                            (const int32_t*)(flt1 + i * flt1_stride + j)));
+                        int16x8_t f2_256_temp_1 = vreinterpretq_s16_s32(vld1q_s32(
+                            (const int32_t*)(flt1 + i * flt1_stride + j + 4)));
+
+                        f2_256 = vpaddq_s16(f2_256_temp_0, f2_256_temp_1);
+                        f2_256 = vsubq_s16(f2_256, u_256);
+
+                        //    H[1][1] += f2 * f2;
+                        neon_mul_s16_s32(&f2_256, &f2_256, out);
+                        h_11 = vaddq_s32(h_11, out[0]);
+                        h_11 = vaddq_s32(h_11, out[1]);
+
+                        //    C[1] += f2 * s;
+                        neon_mul_s16_s32(&f2_256, &s_256, out);
+                        c_1 = vaddq_s32(c_1, out[0]);
+                        c_1 = vaddq_s32(c_1, out[1]);
+                    }
+
+                    //Complement when width not divided by 8
+                    int32_t  sh11 = 0;
+                    int32_t  sc1 = 0;
+                    for (; j < width; ++j) {
+                        const int32_t u  = ((int32_t)(dat[i * dat_stride + j]) << SGRPROJ_RST_BITS);
+                        const int32_t s  = ((int32_t)(src[i * src_stride + j]) << SGRPROJ_RST_BITS) - u;
+                        const int32_t f2 = ((int32_t)flt1[i * flt1_stride + j]) - u;
+                        sh11 += f2 * f2;
+                        sc1 += f2 * s;
+                    }
+
+                    H[1][1] += (double)vaddlvq_s32(h_11) + sh11;
+
+                    C[1] += (double)vaddlvq_s32(c_1) + sc1;
+
+                    h_11 = vdupq_n_s32(0);
+                    c_1  = vdupq_n_s32(0);
+                }
+            }
+
+            else if (params->r[0] > 0 && params->r[1] <= 0) {
+                for (int i = 0; i < height; ++i) {
+                    int j = 0, neon_cnt = 0;
+
+                    for (; neon_cnt < width / 8; j += 8, ++neon_cnt) {
+                     u_256 = vreinterpretq_s16_u16(vshll_n_u8(vld1_u8(dat + i * dat_stride + j), SGRPROJ_RST_BITS));
+                        s_256 = vreinterpretq_s16_u16(vshll_n_u8(vld1_u8(src + i * src_stride + j), SGRPROJ_RST_BITS));
+                        s_256 = vsubq_s16(s_256, u_256);
+
+                        int16x8_t f1_256_temp_0 = vreinterpretq_s16_s32(vld1q_s32(
+                            (const int32_t*)(flt0 + i * flt0_stride + j)));
+                        int16x8_t f1_256_temp_1 = vreinterpretq_s16_s32(vld1q_s32(
+                            (const int32_t*)(flt0 + i * flt0_stride + j + 4)));
+
+                        f1_256 = vpaddq_s16(f1_256_temp_0, f1_256_temp_1);
+                        f1_256 = vsubq_s16(f1_256, u_256);
+
+                        //    H[0][0] += f1 * f1;
+                        neon_mul_s16_s32(&f1_256, &f1_256, out);
+                        h_00 = vaddq_s32(h_00, out[0]);
+                        h_00 = vaddq_s32(h_00, out[1]);
+
+                        //    C[0] += f1 * s;
+                        neon_mul_s16_s32(&f1_256, &s_256, out);
+                        c_0 = vaddq_s32(c_0, out[0]);
+                        c_0 = vaddq_s32(c_0, out[1]);
+                    }
+
+                    //Complement when width not divided by 8
+                    int32_t  sh00 = 0;
+                    int32_t  sc0 = 0;
+                    for (; j < width; ++j) {
+                        const int32_t u  = ((int32_t)(dat[i * dat_stride + j]) << SGRPROJ_RST_BITS);
+                        const int32_t s  = ((int32_t)(src[i * src_stride + j]) << SGRPROJ_RST_BITS) - u;
+                        const int32_t f1 = ((int32_t)flt0[i * flt0_stride + j]) - u;
+                        sh00 += f1 * f1;
+                        sc0 += f1 * s;
+                    }
+
+                    H[0][0] += (double)vaddlvq_s32(h_00) + sh00;
+
+                    C[0] += (double)vaddlvq_s32(c_0) + sc0;
+
+                    h_00 = vdupq_n_s32(0);
+                    c_0  = vdupq_n_s32(0);
+                }
+            }
+        }
+    } else {
+        const uint16_t* src = CONVERT_TO_SHORTPTR(src8);
+        const uint16_t* dat = CONVERT_TO_SHORTPTR(dat8);
+        for (i = 0; i < height; ++i) {
+            for (j = 0; j < width; ++j) {
+                const double u = (double)(dat[i * dat_stride + j] << SGRPROJ_RST_BITS);
+                const double s = (double)(src[i * src_stride + j] << SGRPROJ_RST_BITS) - u;
+                const double f1 = (params->r[0] > 0) ? (double)flt0[i * flt0_stride + j] - u : 0;
+                const double f2 = (params->r[1] > 0) ? (double)flt1[i * flt1_stride + j] - u : 0;
+                H[0][0] += f1 * f1;
+                H[1][1] += f2 * f2;
+                H[0][1] += f1 * f2;
+                C[0] += f1 * s;
+                C[1] += f2 * s;
+            }
+        }
+    }
+
+    H[0][0] /= size;
+    H[0][1] /= size;
+    H[1][1] /= size;
+    H[1][0] = H[0][1];
+    C[0] /= size;
+    C[1] /= size;
+    if (params->r[0] == 0) {
+        // H matrix is now only the scalar H[1][1]
+        // C vector is now only the scalar C[1]
+        det = H[1][1];
+        if (det < 1e-8)
+            return; // ill-posed, return default values
+        x[0] = 0;
+        x[1] = C[1] / det;
+
+        xq[0] = 0;
+        xq[1] = (int)rint(x[1] * (1 << SGRPROJ_PRJ_BITS));
+    }
+    else if (params->r[1] == 0) {
+        // H matrix is now only the scalar H[0][0]
+        // C vector is now only the scalar C[0]
+        det = H[0][0];
+        if (det < 1e-8)
+            return; // ill-posed, return default values
+        x[0] = C[0] / det;
+        x[1] = 0;
+
+        xq[0] = (int)rint(x[0] * (1 << SGRPROJ_PRJ_BITS));
+        xq[1] = 0;
+    }
+    else {
+        det = (H[0][0] * H[1][1] - H[0][1] * H[1][0]);
+        if (det < 1e-8)
+            return; // ill-posed, return default values
+        x[0] = (H[1][1] * C[0] - H[0][1] * C[1]) / det;
+        x[1] = (H[0][0] * C[1] - H[1][0] * C[0]) / det;
+
+        xq[0] = (int)rint(x[0] * (1 << SGRPROJ_PRJ_BITS));
+        xq[1] = (int)rint(x[1] * (1 << SGRPROJ_PRJ_BITS));
+    }
+}
diff --git a/Source/Lib/Encoder/ASM_NEON/picksrt_neon.c b/Source/Lib/Encoder/ASM_NEON/picksrt_neon.c
new file mode 100644
index 0000000..814d9e2
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/picksrt_neon.c
@@ -0,0 +1,149 @@
+/*
+ * Copyright (c) 2020, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <arm_neon.h>
+#include <math.h>
+
+#include "EbInterPrediction.h"
+#include "EbSuperRes.h"
+#include "EbDefinitions.h"
+#include "EbRestoration.h"
+
+int64_t svt_av1_lowbd_pixel_proj_error_neon(
+  const uint8_t *src8, int32_t width, int32_t height, int32_t src_stride,
+  const uint8_t *dat8, int32_t dat_stride, int32_t *flt0, int32_t flt0_stride,
+  int32_t *flt1, int32_t flt1_stride, int32_t xq[2], const SgrParamsType *params){
+
+  int i, j, k;
+  const int32_t shift = SGRPROJ_RST_BITS + SGRPROJ_PRJ_BITS;
+  const int32x4_t zero = vdupq_n_s32(0);
+  uint64x2_t sum64 = vreinterpretq_u64_s32(zero);
+  const uint8_t *src = src8;
+  const uint8_t *dat = dat8;
+
+  int64_t err = 0;
+  if (params->r[0] > 0 && params->r[1] > 0) {
+    for (i = 0; i < height; ++i) {
+      int32x4_t err0 = zero;
+      for (j = 0; j <= width - 8; j += 8) {
+        const uint8x8_t d0 = vld1_u8(&dat[j]);
+        const uint8x8_t s0 = vld1_u8(&src[j]);
+        const int16x8_t flt0_16b =
+            vcombine_s16(vqmovn_s32(vld1q_s32(&flt0[j])),
+                         vqmovn_s32(vld1q_s32(&flt0[j + 4])));
+        const int16x8_t flt1_16b =
+            vcombine_s16(vqmovn_s32(vld1q_s32(&flt1[j])),
+                         vqmovn_s32(vld1q_s32(&flt1[j + 4])));
+        const int16x8_t u0 =
+            vreinterpretq_s16_u16(vshll_n_u8(d0, SGRPROJ_RST_BITS));
+        const int16x8_t flt0_0_sub_u = vsubq_s16(flt0_16b, u0);
+        const int16x8_t flt1_0_sub_u = vsubq_s16(flt1_16b, u0);
+        const int16x4_t flt0_16b_sub_u_lo = vget_low_s16(flt0_0_sub_u);
+        const int16x4_t flt0_16b_sub_u_hi = vget_high_s16(flt0_0_sub_u);
+        const int16x4_t flt1_16b_sub_u_lo = vget_low_s16(flt1_0_sub_u);
+        const int16x4_t flt1_16b_sub_u_hi = vget_high_s16(flt1_0_sub_u);
+
+        int32x4_t v0 = vmull_n_s16(flt0_16b_sub_u_lo, (int16_t)xq[0]);
+        v0 = vmlal_n_s16(v0, flt1_16b_sub_u_lo, (int16_t)xq[1]);
+        int32x4_t v1 = vmull_n_s16(flt0_16b_sub_u_hi, (int16_t)xq[0]);
+        v1 = vmlal_n_s16(v1, flt1_16b_sub_u_hi, (int16_t)xq[1]);
+        const int16x4_t vr0 = vqrshrn_n_s32(v0, 11);
+        const int16x4_t vr1 = vqrshrn_n_s32(v1, 11);
+        const int16x8_t e0 = vaddq_s16(vcombine_s16(vr0, vr1),
+                                       vreinterpretq_s16_u16(vsubl_u8(d0, s0)));
+        const int16x4_t e0_lo = vget_low_s16(e0);
+        const int16x4_t e0_hi = vget_high_s16(e0);
+        err0 = vmlal_s16(err0, e0_lo, e0_lo);
+        err0 = vmlal_s16(err0, e0_hi, e0_hi);
+      }
+      for (k = j; k < width; ++k) {
+        const int32_t u = dat[k] << SGRPROJ_RST_BITS;
+        int32_t v = xq[0] * (flt0[k] - u) + xq[1] * (flt1[k] - u);
+        const int32_t e = ROUND_POWER_OF_TWO(v, 11) + dat[k] - src[k];
+        err += e * e;
+      }
+      dat += dat_stride;
+      src += src_stride;
+      flt0 += flt0_stride;
+      flt1 += flt1_stride;
+      sum64 = vpadalq_u32(sum64, vreinterpretq_u32_s32(err0));
+    }
+
+  } else if (params->r[0] > 0 || params->r[1] > 0) {
+    const int xq_active = (params->r[0] > 0) ? xq[0] : xq[1];
+    const int32_t *flt = (params->r[0] > 0) ? flt0 : flt1;
+    const int flt_stride = (params->r[0] > 0) ? flt0_stride : flt1_stride;
+    for (i = 0; i < height; ++i) {
+      int32x4_t err0 = zero;
+      for (j = 0; j <= width - 8; j += 8) {
+        const uint8x8_t d0 = vld1_u8(&dat[j]);
+        const uint8x8_t s0 = vld1_u8(&src[j]);
+        const uint16x8_t d0s0 = vsubl_u8(d0, s0);
+        const uint16x8x2_t d0w =
+            vzipq_u16(vmovl_u8(d0), vreinterpretq_u16_s32(zero));
+
+        const int32x4_t flt_16b_lo = vld1q_s32(&flt[j]);
+        const int32x4_t flt_16b_hi = vld1q_s32(&flt[j + 4]);
+
+        int32x4_t v0 = vmulq_n_s32(flt_16b_lo, xq_active);
+        v0 = vmlsq_n_s32(v0, vreinterpretq_s32_u16(d0w.val[0]),
+                         xq_active << SGRPROJ_RST_BITS);
+        int32x4_t v1 = vmulq_n_s32(flt_16b_hi, xq_active);
+        v1 = vmlsq_n_s32(v1, vreinterpretq_s32_u16(d0w.val[1]),
+                         xq_active << SGRPROJ_RST_BITS);
+        const int16x4_t vr0 = vqrshrn_n_s32(v0, 11);
+        const int16x4_t vr1 = vqrshrn_n_s32(v1, 11);
+        const int16x8_t e0 =
+            vaddq_s16(vcombine_s16(vr0, vr1), vreinterpretq_s16_u16(d0s0));
+        const int16x4_t e0_lo = vget_low_s16(e0);
+        const int16x4_t e0_hi = vget_high_s16(e0);
+        err0 = vmlal_s16(err0, e0_lo, e0_lo);
+        err0 = vmlal_s16(err0, e0_hi, e0_hi);
+      }
+      for (k = j; k < width; ++k) {
+        const int32_t u = dat[k] << SGRPROJ_RST_BITS;
+        int32_t v = xq_active * (flt[k] - u);
+        const int32_t e = ROUND_POWER_OF_TWO(v, shift) + dat[k] - src[k];
+        err += e * e;
+      }
+      dat += dat_stride;
+      src += src_stride;
+      flt += flt_stride;
+      sum64 = vpadalq_u32(sum64, vreinterpretq_u32_s32(err0));
+    }
+  } else {
+    uint32x4_t err0 = vreinterpretq_u32_s32(zero);
+    for (i = 0; i < height; ++i) {
+      for (j = 0; j <= width - 16; j += 16) {
+        const uint8x16_t d = vld1q_u8(&dat[j]);
+        const uint8x16_t s = vld1q_u8(&src[j]);
+        const uint8x16_t diff = vabdq_u8(d, s);
+        const uint8x8_t diff0 = vget_low_u8(diff);
+        const uint8x8_t diff1 = vget_high_u8(diff);
+        err0 = vpadalq_u16(err0, vmull_u8(diff0, diff0));
+        err0 = vpadalq_u16(err0, vmull_u8(diff1, diff1));
+      }
+      for (k = j; k < width; ++k) {
+        const int32_t e = dat[k] - src[k];
+        err += e * e;
+      }
+      dat += dat_stride;
+      src += src_stride;
+    }
+    sum64 = vpaddlq_u32(err0);
+  }
+#if defined(__aarch64__)
+  err += vaddvq_u64(sum64);
+#else
+  err += vget_lane_u64(vadd_u64(vget_low_u64(sum64), vget_high_u64(sum64)), 0);
+#endif  // __aarch64__
+  return err;
+}
diff --git a/Source/Lib/Encoder/ASM_NEON/subpel_variance_neon.c b/Source/Lib/Encoder/ASM_NEON/subpel_variance_neon.c
new file mode 100644
index 0000000..621023b
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/subpel_variance_neon.c
@@ -0,0 +1,481 @@
+/*
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+
+#include <arm_neon.h>
+#include "common_dsp_rtcd.h"
+#include "aom_dsp_rtcd.h"
+#include "subpel_variance_neon.h"
+
+////////////////////////////////////////////////////////////////////////////////
+// 8 bit
+////////////////////////////////////////////////////////////////////////////////
+// 2 tap bilinear filters
+#define BIL_SUBPEL_BITS 3
+#define BIL_SUBPEL_SHIFTS (1 << BIL_SUBPEL_BITS)
+static const uint8_t bilinear_filters_2t[BIL_SUBPEL_SHIFTS][2] = {
+    {128, 0},
+    {112, 16},
+    {96, 32},
+    {80, 48},
+    {64, 64},
+    {48, 80},
+    {32, 96},
+    {16, 112},
+};
+
+/*
+#include "config/aom_dsp_rtcd.h"
+#include "config/aom_config.h"
+
+#include "aom_ports/mem.h"
+#include "aom/aom_integer.h"
+
+#include "aom_dsp/aom_filter.h"
+#include "aom_dsp/variance.h"
+*/
+
+// Load 2 sets of 4 bytes when alignment is not guaranteed.
+static INLINE uint8x8_t load_unaligned_u8(const uint8_t *buf, int stride) {
+  uint32_t a;
+  uint32x2_t a_u32 = vdup_n_u32(0);
+  if (stride == 4) return vld1_u8(buf);
+  svt_memcpy_c(&a, buf, 4);
+  buf += stride;
+  a_u32 = vld1_lane_u32(&a, a_u32, 0);
+  svt_memcpy_c(&a, buf, 4);
+  a_u32 = vld1_lane_u32(&a, a_u32, 1);
+  return vreinterpret_u8_u32(a_u32);
+}
+
+// Process a block exactly 4 wide and a multiple of 2 high.
+static void var_filter_block2d_bil_w4(const uint8_t *src_ptr,
+                                      uint8_t *output_ptr,
+                                      unsigned int src_pixels_per_line,
+                                      int pixel_step,
+                                      unsigned int output_height,
+                                      const uint8_t *filter) {
+  const uint8x8_t f0 = vdup_n_u8(filter[0]);
+  const uint8x8_t f1 = vdup_n_u8(filter[1]);
+  unsigned int i;
+  for (i = 0; i < output_height; i += 2) {
+    const uint8x8_t src_0 = load_unaligned_u8(src_ptr, src_pixels_per_line);
+    const uint8x8_t src_1 =
+        load_unaligned_u8(src_ptr + pixel_step, src_pixels_per_line);
+    const uint16x8_t a = vmull_u8(src_0, f0);
+    const uint16x8_t b = vmlal_u8(a, src_1, f1);
+    const uint8x8_t out = vrshrn_n_u16(b, FILTER_BITS);
+    vst1_u8(output_ptr, out);
+    src_ptr += 2 * src_pixels_per_line;
+    output_ptr += 8;
+  }
+}
+
+static void var_filter_block2d_bil_w8(const uint8_t *src_ptr,
+                                      uint8_t *output_ptr,
+                                      unsigned int src_pixels_per_line,
+                                      int pixel_step,
+                                      unsigned int output_height,
+                                      unsigned int output_width,
+                                      const uint8_t *filter) {
+  const uint8x8_t f0 = vdup_n_u8(filter[0]);
+  const uint8x8_t f1 = vdup_n_u8(filter[1]);
+  unsigned int i;
+  for (i = 0; i < output_height; ++i) {
+    const uint8x8_t src_0 = vld1_u8(&src_ptr[0]);
+    const uint8x8_t src_1 = vld1_u8(&src_ptr[pixel_step]);
+    const uint16x8_t a = vmull_u8(src_0, f0);
+    const uint16x8_t b = vmlal_u8(a, src_1, f1);
+    const uint8x8_t out = vrshrn_n_u16(b, FILTER_BITS);
+    vst1_u8(output_ptr, out);
+    // Next row...
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_width;
+  }
+}
+
+// Process a block which is a mutiple of 16 wide and any height.
+static void var_filter_block2d_bil_w16(const uint8_t *src_ptr,
+                                       uint8_t *output_ptr,
+                                       unsigned int src_pixels_per_line,
+                                       int pixel_step,
+                                       unsigned int output_height,
+                                       unsigned int output_width,
+                                       const uint8_t *filter) {
+  const uint8x8_t f0 = vdup_n_u8(filter[0]);
+  const uint8x8_t f1 = vdup_n_u8(filter[1]);
+  unsigned int i, j;
+  for (i = 0; i < output_height; ++i) {
+    for (j = 0; j < output_width; j += 16) {
+      const uint8x16_t src_0 = vld1q_u8(&src_ptr[j]);
+      const uint8x16_t src_1 = vld1q_u8(&src_ptr[j + pixel_step]);
+      const uint16x8_t a = vmull_u8(vget_low_u8(src_0), f0);
+      const uint16x8_t b = vmlal_u8(a, vget_low_u8(src_1), f1);
+      const uint8x8_t out_lo = vrshrn_n_u16(b, FILTER_BITS);
+      const uint16x8_t c = vmull_u8(vget_high_u8(src_0), f0);
+      const uint16x8_t d = vmlal_u8(c, vget_high_u8(src_1), f1);
+      const uint8x8_t out_hi = vrshrn_n_u16(d, FILTER_BITS);
+      vst1q_u8(output_ptr + j, vcombine_u8(out_lo, out_hi));
+    }
+    src_ptr += src_pixels_per_line;
+    output_ptr += output_width;
+  }
+}
+
+static inline void variance_filter_block2d_bil_hv_w8(const uint8_t* src_ptr,
+  const uint8_t* ref,
+  unsigned int src_pixels_per_line,
+  int ref_step,
+  int pixel_step,
+  unsigned int output_height,
+  unsigned int output_width,
+  const uint8_t* filterx,
+  const uint8_t* filtery, uint32_t* sse, int* sum) {
+  const uint8x8_t fxl = vdup_n_u8(filterx[0]);
+  const uint8x8_t fxh = vdup_n_u8(filterx[1]);
+  const uint8x8_t fyl = vdup_n_u8(filtery[0]);
+  const uint8x8_t fyh = vdup_n_u8(filtery[1]);
+
+  int16x8_t v_sum = vdupq_n_s16(0);
+  int32x4_t v_sse_lo = vdupq_n_s32(0);
+  int32x4_t v_sse_hi = vdupq_n_s32(0);
+
+  unsigned int i, j;
+  for (j = 0; j < output_width; j += 8) {
+    uint8x8_t src_0 = vld1_u8(&src_ptr[j]);
+    uint8x8_t src_1 = vld1_u8(&src_ptr[j + pixel_step]);
+    uint16x8_t a = vmull_u8(src_0, fxl);
+    uint8x8_t res_prev = vrshrn_n_u16(vmlal_u8(a, src_1, fxh), FILTER_BITS);
+    src_ptr += src_pixels_per_line;
+
+    for (i = 0; i < output_height - 1; ++i) {
+      src_0 = vld1_u8(&src_ptr[j]);
+      src_1 = vld1_u8(&src_ptr[j + pixel_step]);
+      a = vmull_u8(src_0, fxl);
+      uint8x8_t res_cur = vrshrn_n_u16(vmlal_u8(a, src_1, fxh), FILTER_BITS);
+      uint16x8_t t = vmull_u8(res_prev, fyl);
+      t = vmlal_u8(t, res_cur, fyh);
+      const uint8x8_t out = vrshrn_n_u16(t, FILTER_BITS);
+
+      // calc stdev
+      uint8x8_t vref = vld1_u8(ref + j);
+      const int16x8_t sv_diff = vreinterpretq_s16_u16(vsubl_u8(out, vref));
+      v_sum = vaddq_s16(v_sum, sv_diff);
+      v_sse_lo =
+        vmlal_s16(v_sse_lo, vget_low_s16(sv_diff), vget_low_s16(sv_diff));
+      v_sse_hi =
+        vmlal_s16(v_sse_hi, vget_high_s16(sv_diff), vget_high_s16(sv_diff));
+
+      res_prev = res_cur;
+      src_ptr += src_pixels_per_line;
+      ref += ref_step;
+    }
+    src_ptr -= src_pixels_per_line * (output_height);
+    ref -= ref_step * (output_height - 1);
+  }
+  *sum = vaddlvq_s16(v_sum);
+  *sse = vaddvq_s32(vaddq_s32(v_sse_lo, v_sse_hi));
+}
+
+unsigned int aom_sub_pixel_variance8x8_neon(const uint8_t *src, int src_stride,
+                                            int xoffset, int yoffset,
+                                            const uint8_t *dst, int dst_stride,
+                                            unsigned int *sse) {
+  int sum;
+  variance_filter_block2d_bil_hv_w8(src, dst, src_stride, dst_stride, 1, 9, 8, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return  *sse - ((sum * sum) >> 6);
+}
+
+unsigned int aom_sub_pixel_variance16x16_neon(const uint8_t *src,
+                                              int src_stride, int xoffset,
+                                              int yoffset, const uint8_t *dst,
+                                              int dst_stride,
+                                              unsigned int *sse) {
+   int sum;
+  variance_filter_block2d_bil_hv_w8(src, dst, src_stride, dst_stride, 1, 17, 16, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return  *sse - ((sum * sum) >> 8);
+
+}
+
+unsigned int aom_sub_pixel_variance32x32_neon(const uint8_t *src,
+                                              int src_stride, int xoffset,
+                                              int yoffset, const uint8_t *dst,
+                                              int dst_stride,
+                                              unsigned int *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(src, dst, src_stride, dst_stride, 1, 33, 32, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  uint32_t var = *sse - (unsigned int)(((int64_t)sum * sum) >> 10);
+
+  return  var;
+}
+
+unsigned int aom_sub_pixel_variance64x64_neon(const uint8_t *src,
+                                              int src_stride, int xoffset,
+                                              int yoffset, const uint8_t *dst,
+                                              int dst_stride,
+                                              unsigned int *sse) {
+
+  const int h = 65;
+  const int w = 16;
+  int sum = 0;
+  *sse = 0;
+  for (int i = 0; i < 64 / w;i++) {
+    uint32_t sse_tmp;
+    int sum_tmp;
+    variance_filter_block2d_bil_hv_w8(src, dst, src_stride, dst_stride, 1, h, w, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], &sse_tmp, &sum_tmp);
+    src += w; dst += w;
+    *sse += sse_tmp;
+    sum += sum_tmp;
+  }
+
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 12);
+}
+
+unsigned int aom_sub_pixel_variance4x4_neon(const uint8_t *a, int a_stride,
+                                            int xoffset, int yoffset,
+                                            const uint8_t *b, int b_stride,
+                                            uint32_t *sse) {
+  uint8_t temp0[4 * (4 + 2)];
+  uint8_t temp1[4 * 4];
+
+  var_filter_block2d_bil_w4(a, temp0, a_stride, 1, (4 + 2),
+                            bilinear_filters_2t[xoffset]);
+  var_filter_block2d_bil_w4(temp0, temp1, 4, 4, 4,
+                            bilinear_filters_2t[yoffset]);
+
+  return aom_variance4x4(temp1, 4, b, b_stride, sse);
+}
+
+unsigned int aom_sub_pixel_variance4x8_neon(const uint8_t *a, int a_stride,
+                                            int xoffset, int yoffset,
+                                            const uint8_t *b, int b_stride,
+                                            uint32_t *sse) {
+  uint8_t temp0[4 * (8 + 2)];
+  uint8_t temp1[4 * 8];
+
+  var_filter_block2d_bil_w4(a, temp0, a_stride, 1, (8 + 2),
+                            bilinear_filters_2t[xoffset]);
+  var_filter_block2d_bil_w4(temp0, temp1, 4, 4, 8,
+                            bilinear_filters_2t[yoffset]);
+
+  return aom_variance4x8(temp1, 4, b, b_stride, sse);
+}
+
+unsigned int aom_sub_pixel_variance8x4_neon(const uint8_t *a, int a_stride,
+                                            int xoffset, int yoffset,
+                                            const uint8_t *b, int b_stride,
+                                            uint32_t *sse) {
+  uint8_t temp0[8 * (4 + 1)];
+  uint8_t temp1[8 * 4];
+
+  var_filter_block2d_bil_w8(a, temp0, a_stride, 1, (4 + 1), 8,
+                            bilinear_filters_2t[xoffset]);
+  var_filter_block2d_bil_w8(temp0, temp1, 8, 8, 4, 8,
+                            bilinear_filters_2t[yoffset]);
+
+  return aom_variance8x4(temp1, 8, b, b_stride, sse);
+}
+
+unsigned int aom_sub_pixel_variance8x16_neon(const uint8_t *a, int a_stride,
+                                             int xoffset, int yoffset,
+                                             const uint8_t *b, int b_stride,
+                                             uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 17, 8, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 7);
+}
+
+unsigned int aom_sub_pixel_variance16x8_neon(const uint8_t *a, int a_stride,
+                                             int xoffset, int yoffset,
+                                             const uint8_t *b, int b_stride,
+                                             uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 9, 16, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 7);
+}
+
+unsigned int aom_sub_pixel_variance16x32_neon(const uint8_t *a, int a_stride,
+                                              int xoffset, int yoffset,
+                                              const uint8_t *b, int b_stride,
+                                              uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 33, 16, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 9);
+
+}
+
+unsigned int aom_sub_pixel_variance32x16_neon(const uint8_t *a, int a_stride,
+                                              int xoffset, int yoffset,
+                                              const uint8_t *b, int b_stride,
+                                              uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 17, 32, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 9);
+}
+
+unsigned int aom_sub_pixel_variance32x64_neon(const uint8_t *a, int a_stride,
+                                              int xoffset, int yoffset,
+                                              const uint8_t *b, int b_stride,
+                                              uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 65, 32, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 11);
+}
+
+unsigned int aom_sub_pixel_variance64x32_neon(const uint8_t *a, int a_stride,
+                                              int xoffset, int yoffset,
+                                              const uint8_t *b, int b_stride,
+                                              uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 33, 64, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 11);
+}
+
+unsigned int aom_sub_pixel_variance64x128_neon(const uint8_t *a, int a_stride,
+                                               int xoffset, int yoffset,
+                                               const uint8_t *b, int b_stride,
+                                               uint32_t *sse) {
+
+  const int h = 129;
+  const int w = 8;
+  int sum = 0;
+  *sse = 0;
+  for (int i = 0; i < 64 / w;i++) {
+    uint32_t sse_tmp;
+    int sum_tmp;
+    variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, h, w, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], &sse_tmp, &sum_tmp);
+    a += w; b += w;
+    *sse += sse_tmp;
+    sum += sum_tmp;
+  }
+
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 13);
+
+}
+
+unsigned int aom_sub_pixel_variance128x64_neon(const uint8_t *a, int a_stride,
+                                               int xoffset, int yoffset,
+                                               const uint8_t *b, int b_stride,
+                                               uint32_t *sse) {
+
+  const int h = 65;
+  const int w = 16;
+  int sum = 0;
+  *sse = 0;
+  for (int i = 0; i < 128 / w;i++) {
+    uint32_t sse_tmp;
+    int sum_tmp;
+    variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, h, w, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], &sse_tmp, &sum_tmp);
+    a += w; b += w;
+    *sse += sse_tmp;
+    sum += sum_tmp;
+  }
+
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 13);
+
+}
+
+unsigned int aom_sub_pixel_variance128x128_neon(const uint8_t *a, int a_stride,
+                                                int xoffset, int yoffset,
+                                                const uint8_t *b, int b_stride,
+                                                uint32_t *sse) {
+
+  const int h = 129;
+  const int w = 8;
+  int sum = 0;
+  *sse = 0;
+  for (int i = 0; i < 128 / w;i++) {
+    uint32_t sse_tmp;
+    int sum_tmp;
+    variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, h, w, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], &sse_tmp, &sum_tmp);
+    a += w; b += w;
+    *sse += sse_tmp;
+    sum += sum_tmp;
+  }
+
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 14);
+}
+
+// Realtime mode doesn't use 4x rectangular blocks.
+#if !CONFIG_REALTIME_ONLY
+unsigned int aom_sub_pixel_variance4x16_neon(const uint8_t *a, int a_stride,
+                                             int xoffset, int yoffset,
+                                             const uint8_t *b, int b_stride,
+                                             uint32_t *sse) {
+  uint8_t temp0[4 * (16 + 2)];
+  uint8_t temp1[4 * 16];
+
+  var_filter_block2d_bil_w4(a, temp0, a_stride, 1, (16 + 2),
+                            bilinear_filters_2t[xoffset]);
+  var_filter_block2d_bil_w4(temp0, temp1, 4, 4, 16,
+                            bilinear_filters_2t[yoffset]);
+
+  return aom_variance4x16(temp1, 4, b, b_stride, sse);
+}
+
+unsigned int aom_sub_pixel_variance8x32_neon(const uint8_t *a, int a_stride,
+                                             int xoffset, int yoffset,
+                                             const uint8_t *b, int b_stride,
+                                             uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 33, 8, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 8);
+}
+
+unsigned int aom_sub_pixel_variance16x4_neon(const uint8_t *a, int a_stride,
+                                             int xoffset, int yoffset,
+                                             const uint8_t *b, int b_stride,
+                                             uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 5, 16, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 6);
+
+}
+
+unsigned int aom_sub_pixel_variance64x16_neon(const uint8_t *a, int a_stride,
+                                              int xoffset, int yoffset,
+                                              const uint8_t *b, int b_stride,
+                                              uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 17, 64, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 10);
+
+}
+
+unsigned int aom_sub_pixel_variance16x64_neon(const uint8_t *a, int a_stride,
+                                              int xoffset, int yoffset,
+                                              const uint8_t *b, int b_stride,
+                                              uint32_t *sse) {
+
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 65, 16, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 10);
+}
+
+unsigned int aom_sub_pixel_variance32x8_neon(const uint8_t *a, int a_stride,
+                                             int xoffset, int yoffset,
+                                             const uint8_t *b, int b_stride,
+                                             uint32_t *sse) {
+  int sum;
+  variance_filter_block2d_bil_hv_w8(a, b, a_stride, b_stride, 1, 9, 32, bilinear_filters_2t[xoffset], bilinear_filters_2t[yoffset], sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 8);
+}
+#endif  // !CONFIG_REALTIME_ONLY
diff --git a/Source/Lib/Encoder/ASM_NEON/subpel_variance_neon.h b/Source/Lib/Encoder/ASM_NEON/subpel_variance_neon.h
new file mode 100644
index 0000000..993d2ec
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/subpel_variance_neon.h
@@ -0,0 +1,116 @@
+/*
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#ifndef SUBPIXEL_VARIANCE_NEON_H
+#define SUBPIXEL_VARIANCE_NEON_H
+
+#include "common_dsp_rtcd.h"
+
+#include "EbDefinitions.h"
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+unsigned int aom_highbd_10_variance128x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance128x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance16x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance16x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance16x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+unsigned int aom_highbd_10_variance16x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+unsigned int aom_highbd_10_variance16x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance32x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance32x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance32x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance32x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+unsigned int aom_highbd_10_variance4x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+unsigned int aom_highbd_10_variance4x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance4x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance64x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance64x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+unsigned int aom_highbd_10_variance64x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance64x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance8x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance8x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+unsigned int aom_highbd_10_variance8x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_highbd_10_variance8x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+uint32_t aom_sub_pixel_variance128x128_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance128x64_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance16x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance16x32_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance16x4_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance16x64_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance16x8_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance32x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance32x32_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance32x64_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance32x8_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance4x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance4x4_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance4x8_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance64x128_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance64x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance64x32_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance64x64_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance8x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance8x32_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance8x4_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+uint32_t aom_sub_pixel_variance8x8_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+unsigned int aom_variance128x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance128x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance16x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance16x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance16x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance32x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance32x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance32x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance4x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance4x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance64x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance64x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance64x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance8x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance8x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+unsigned int aom_variance8x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+
+#define aom_variance128x128 aom_variance128x128_neon
+#define aom_variance128x64 aom_variance128x64_neon
+#define aom_variance16x16 aom_variance16x16_neon
+#define aom_variance16x32 aom_variance16x32_neon
+#define aom_variance16x8 aom_variance16x8_neon
+#define aom_variance32x16 aom_variance32x16_neon
+#define aom_variance32x32 aom_variance32x32_neon
+#define aom_variance32x64 aom_variance32x64_neon
+#define aom_variance4x4 aom_variance4x4_neon
+#define aom_variance4x8 aom_variance4x8_neon
+#define aom_variance64x128 aom_variance64x128_neon
+#define aom_variance64x32 aom_variance64x32_neon
+#define aom_variance64x64 aom_variance64x64_neon
+#define aom_variance8x16 aom_variance8x16_neon
+#define aom_variance8x4 aom_variance8x4_neon
+#define aom_variance8x8 aom_variance8x8_neon
+
+unsigned int aom_variance4x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#define aom_variance4x16 svt_aom_variance4x16_c
+unsigned int aom_variance8x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#define aom_variance8x32 svt_aom_variance8x32_c
+unsigned int aom_variance16x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#define aom_variance16x4 svt_aom_variance16x4_c
+unsigned int aom_variance64x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#define aom_variance64x16 svt_aom_variance64x16_c
+unsigned int aom_variance16x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#define aom_variance16x64 svt_aom_variance16x64_c
+unsigned int aom_variance32x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#define aom_variance32x8 svt_aom_variance32x8_c
+
+#ifdef __cplusplus
+}
+#endif
+#endif // SUBPIXEL_VARIANCE_NEON_H
diff --git a/Source/Lib/Encoder/ASM_NEON/sum_neon.h b/Source/Lib/Encoder/ASM_NEON/sum_neon.h
new file mode 100644
index 0000000..28212ad
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/sum_neon.h
@@ -0,0 +1,74 @@
+/*
+ *  Copyright (c) 2019, Alliance for Open Media. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <arm_neon.h>
+#include "common_dsp_rtcd.h"
+
+static INLINE int horizontal_add_s16x8(const int16x8_t a) {
+#if defined(__aarch64__)
+  return vaddlvq_s16(a);
+#else
+  const int32x4_t b = vpaddlq_s16(a);
+  const int64x2_t c = vpaddlq_s32(b);
+  const int32x2_t d = vadd_s32(vreinterpret_s32_s64(vget_low_s64(c)),
+                               vreinterpret_s32_s64(vget_high_s64(c)));
+  return vget_lane_s32(d, 0);
+#endif
+}
+
+static INLINE int horizontal_add_s32x4(const int32x4_t a) {
+#if defined(__aarch64__)
+  return vaddvq_s32(a);
+#else
+  const int64x2_t b = vpaddlq_s32(a);
+  const int32x2_t c = vadd_s32(vreinterpret_s32_s64(vget_low_s64(b)),
+                               vreinterpret_s32_s64(vget_high_s64(b)));
+  return vget_lane_s32(c, 0);
+#endif
+}
+
+static INLINE uint32_t horizontal_long_add_u16x8(const uint16x8_t vec_lo,
+                                                 const uint16x8_t vec_hi) {
+#if defined(__aarch64__)
+  return vaddlvq_u16(vec_lo) + vaddlvq_u16(vec_hi);
+#else
+  const uint32x4_t vec_l_lo =
+      vaddl_u16(vget_low_u16(vec_lo), vget_high_u16(vec_lo));
+  const uint32x4_t vec_l_hi =
+      vaddl_u16(vget_low_u16(vec_hi), vget_high_u16(vec_hi));
+  const uint32x4_t a = vaddq_u32(vec_l_lo, vec_l_hi);
+  const uint64x2_t b = vpaddlq_u32(a);
+  const uint32x2_t c = vadd_u32(vreinterpret_u32_u64(vget_low_u64(b)),
+                                vreinterpret_u32_u64(vget_high_u64(b)));
+  return vget_lane_u32(c, 0);
+#endif
+}
+
+static INLINE uint32_t horizontal_add_u16x8(const uint16x8_t a) {
+#if defined(__aarch64__)
+  return vaddlvq_u16(a);
+#else
+  const uint32x4_t b = vpaddlq_u16(a);
+  const uint64x2_t c = vpaddlq_u32(b);
+  const uint32x2_t d = vadd_u32(vreinterpret_u32_u64(vget_low_u64(c)),
+                                vreinterpret_u32_u64(vget_high_u64(c)));
+  return vget_lane_u32(d, 0);
+#endif
+}
+
+static INLINE uint32_t horizontal_add_u16x4(const uint16x4_t a) {
+#if defined(__aarch64__)
+  return vaddlv_u16(a);
+#else
+  const uint32x2_t b = vpaddl_u16(a);
+  const uint64x1_t c = vpaddl_u32(b);
+  return vget_lane_u32(vreinterpret_u32_u64(c), 0);
+#endif
+}
diff --git a/Source/Lib/Encoder/ASM_NEON/variance_neon.c b/Source/Lib/Encoder/ASM_NEON/variance_neon.c
new file mode 100644
index 0000000..d8ff80a
--- /dev/null
+++ b/Source/Lib/Encoder/ASM_NEON/variance_neon.c
@@ -0,0 +1,647 @@
+/*
+ * Copyright (c) 2016, Alliance for Open Media. All rights reserved
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <arm_neon.h>
+#include "common_dsp_rtcd.h"
+#include "sum_neon.h"
+
+// w * h must be less than 2048 or local variable v_sum may overflow.
+static void variance_neon_w8(const uint8_t *a, int a_stride, const uint8_t *b,
+                             int b_stride, int w, int h, uint32_t *sse,
+                             int *sum) {
+  int i, j;
+  int16x8_t v_sum = vdupq_n_s16(0);
+  int32x4_t v_sse_lo = vdupq_n_s32(0);
+  int32x4_t v_sse_hi = vdupq_n_s32(0);
+
+  for (i = 0; i < h; ++i) {
+    for (j = 0; j < w; j += 8) {
+      const uint8x8_t v_a = vld1_u8(&a[j]);
+      const uint8x8_t v_b = vld1_u8(&b[j]);
+      const uint16x8_t v_diff = vsubl_u8(v_a, v_b);
+      const int16x8_t sv_diff = vreinterpretq_s16_u16(v_diff);
+      v_sum = vaddq_s16(v_sum, sv_diff);
+      v_sse_lo =
+          vmlal_s16(v_sse_lo, vget_low_s16(sv_diff), vget_low_s16(sv_diff));
+      v_sse_hi =
+          vmlal_s16(v_sse_hi, vget_high_s16(sv_diff), vget_high_s16(sv_diff));
+    }
+    a += a_stride;
+    b += b_stride;
+  }
+
+  *sum = horizontal_add_s16x8(v_sum);
+  *sse = (unsigned int)horizontal_add_s32x4(vaddq_s32(v_sse_lo, v_sse_hi));
+}
+
+void aom_get8x8var_neon(const uint8_t *a, int a_stride, const uint8_t *b,
+                        int b_stride, unsigned int *sse, int *sum) {
+  variance_neon_w8(a, a_stride, b, b_stride, 8, 8, sse, sum);
+}
+
+void aom_get16x16var_neon(const uint8_t *a, int a_stride, const uint8_t *b,
+                          int b_stride, unsigned int *sse, int *sum) {
+  variance_neon_w8(a, a_stride, b, b_stride, 16, 16, sse, sum);
+}
+
+// TODO(yunqingwang): Perform variance of two/four 8x8 blocks similar to that of
+// AVX2.
+void aom_get_sse_sum_8x8_quad_neon(const uint8_t *a, int a_stride,
+                                   const uint8_t *b, int b_stride,
+                                   unsigned int *sse, int *sum) {
+  // Loop over 4 8x8 blocks. Process one 8x32 block.
+  for (int k = 0; k < 4; k++) {
+    variance_neon_w8(a + (k * 8), a_stride, b + (k * 8), b_stride, 8, 8,
+                     &sse[k], &sum[k]);
+  }
+}
+
+unsigned int aom_variance8x8_neon(const uint8_t *a, int a_stride,
+                                  const uint8_t *b, int b_stride,
+                                  unsigned int *sse) {
+  int sum;
+  variance_neon_w8(a, a_stride, b, b_stride, 8, 8, sse, &sum);
+  return *sse - ((sum * sum) >> 6);
+}
+
+unsigned int aom_variance16x16_neon(const uint8_t *a, int a_stride,
+                                    const uint8_t *b, int b_stride,
+                                    unsigned int *sse) {
+  int sum;
+  variance_neon_w8(a, a_stride, b, b_stride, 16, 16, sse, &sum);
+  return *sse - (((unsigned int)((int64_t)sum * sum)) >> 8);
+}
+
+unsigned int aom_variance32x32_neon(const uint8_t *a, int a_stride,
+                                    const uint8_t *b, int b_stride,
+                                    unsigned int *sse) {
+  int sum;
+  variance_neon_w8(a, a_stride, b, b_stride, 32, 32, sse, &sum);
+  return *sse - (unsigned int)(((int64_t)sum * sum) >> 10);
+}
+
+unsigned int aom_variance32x64_neon(const uint8_t *a, int a_stride,
+                                    const uint8_t *b, int b_stride,
+                                    unsigned int *sse) {
+  int sum1, sum2;
+  uint32_t sse1, sse2;
+  variance_neon_w8(a, a_stride, b, b_stride, 32, 32, &sse1, &sum1);
+  variance_neon_w8(a + (32 * a_stride), a_stride, b + (32 * b_stride), b_stride,
+                   32, 32, &sse2, &sum2);
+  *sse = sse1 + sse2;
+  sum1 += sum2;
+  return *sse - (unsigned int)(((int64_t)sum1 * sum1) >> 11);
+}
+
+unsigned int aom_variance64x32_neon(const uint8_t *a, int a_stride,
+                                    const uint8_t *b, int b_stride,
+                                    unsigned int *sse) {
+  int sum1, sum2;
+  uint32_t sse1, sse2;
+  variance_neon_w8(a, a_stride, b, b_stride, 64, 16, &sse1, &sum1);
+  variance_neon_w8(a + (16 * a_stride), a_stride, b + (16 * b_stride), b_stride,
+                   64, 16, &sse2, &sum2);
+  *sse = sse1 + sse2;
+  sum1 += sum2;
+  return *sse - (unsigned int)(((int64_t)sum1 * sum1) >> 11);
+}
+
+unsigned int aom_variance64x64_neon(const uint8_t *a, int a_stride,
+                                    const uint8_t *b, int b_stride,
+                                    unsigned int *sse) {
+  int sum1, sum2;
+  uint32_t sse1, sse2;
+
+  variance_neon_w8(a, a_stride, b, b_stride, 64, 16, &sse1, &sum1);
+  variance_neon_w8(a + (16 * a_stride), a_stride, b + (16 * b_stride), b_stride,
+                   64, 16, &sse2, &sum2);
+  sse1 += sse2;
+  sum1 += sum2;
+
+  variance_neon_w8(a + (16 * 2 * a_stride), a_stride, b + (16 * 2 * b_stride),
+                   b_stride, 64, 16, &sse2, &sum2);
+  sse1 += sse2;
+  sum1 += sum2;
+
+  variance_neon_w8(a + (16 * 3 * a_stride), a_stride, b + (16 * 3 * b_stride),
+                   b_stride, 64, 16, &sse2, &sum2);
+  *sse = sse1 + sse2;
+  sum1 += sum2;
+  return *sse - (unsigned int)(((int64_t)sum1 * sum1) >> 12);
+}
+
+unsigned int aom_variance128x128_neon(const uint8_t *a, int a_stride,
+                                      const uint8_t *b, int b_stride,
+                                      unsigned int *sse) {
+  int sum1, sum2;
+  uint32_t sse1, sse2;
+  sum1 = sse1 = 0;
+  for (int i = 0; i < 16; i++) {
+    variance_neon_w8(a + (8 * i * a_stride), a_stride, b + (8 * i * b_stride),
+                     b_stride, 128, 8, &sse2, &sum2);
+    sse1 += sse2;
+    sum1 += sum2;
+  }
+
+  *sse = sse1;
+
+  return *sse - (unsigned int)(((int64_t)sum1 * sum1) >> 14);
+}
+
+unsigned int aom_variance16x8_neon(const unsigned char *src_ptr,
+                                   int source_stride,
+                                   const unsigned char *ref_ptr,
+                                   int recon_stride, unsigned int *sse) {
+  int i;
+  int16x4_t d22s16, d23s16, d24s16, d25s16, d26s16, d27s16, d28s16, d29s16;
+  uint32x2_t d0u32, d10u32;
+  int64x1_t d0s64, d1s64;
+  uint8x16_t q0u8, q1u8, q2u8, q3u8;
+  uint16x8_t q11u16, q12u16, q13u16, q14u16;
+  int32x4_t q8s32, q9s32, q10s32;
+  int64x2_t q0s64, q1s64, q5s64;
+
+  q8s32 = vdupq_n_s32(0);
+  q9s32 = vdupq_n_s32(0);
+  q10s32 = vdupq_n_s32(0);
+
+  for (i = 0; i < 4; i++) {
+    q0u8 = vld1q_u8(src_ptr);
+    src_ptr += source_stride;
+    q1u8 = vld1q_u8(src_ptr);
+    src_ptr += source_stride;
+    __builtin_prefetch(src_ptr);
+
+    q2u8 = vld1q_u8(ref_ptr);
+    ref_ptr += recon_stride;
+    q3u8 = vld1q_u8(ref_ptr);
+    ref_ptr += recon_stride;
+    __builtin_prefetch(ref_ptr);
+
+    q11u16 = vsubl_u8(vget_low_u8(q0u8), vget_low_u8(q2u8));
+    q12u16 = vsubl_u8(vget_high_u8(q0u8), vget_high_u8(q2u8));
+    q13u16 = vsubl_u8(vget_low_u8(q1u8), vget_low_u8(q3u8));
+    q14u16 = vsubl_u8(vget_high_u8(q1u8), vget_high_u8(q3u8));
+
+    d22s16 = vreinterpret_s16_u16(vget_low_u16(q11u16));
+    d23s16 = vreinterpret_s16_u16(vget_high_u16(q11u16));
+    q8s32 = vpadalq_s16(q8s32, vreinterpretq_s16_u16(q11u16));
+    q9s32 = vmlal_s16(q9s32, d22s16, d22s16);
+    q10s32 = vmlal_s16(q10s32, d23s16, d23s16);
+
+    d24s16 = vreinterpret_s16_u16(vget_low_u16(q12u16));
+    d25s16 = vreinterpret_s16_u16(vget_high_u16(q12u16));
+    q8s32 = vpadalq_s16(q8s32, vreinterpretq_s16_u16(q12u16));
+    q9s32 = vmlal_s16(q9s32, d24s16, d24s16);
+    q10s32 = vmlal_s16(q10s32, d25s16, d25s16);
+
+    d26s16 = vreinterpret_s16_u16(vget_low_u16(q13u16));
+    d27s16 = vreinterpret_s16_u16(vget_high_u16(q13u16));
+    q8s32 = vpadalq_s16(q8s32, vreinterpretq_s16_u16(q13u16));
+    q9s32 = vmlal_s16(q9s32, d26s16, d26s16);
+    q10s32 = vmlal_s16(q10s32, d27s16, d27s16);
+
+    d28s16 = vreinterpret_s16_u16(vget_low_u16(q14u16));
+    d29s16 = vreinterpret_s16_u16(vget_high_u16(q14u16));
+    q8s32 = vpadalq_s16(q8s32, vreinterpretq_s16_u16(q14u16));
+    q9s32 = vmlal_s16(q9s32, d28s16, d28s16);
+    q10s32 = vmlal_s16(q10s32, d29s16, d29s16);
+  }
+
+  q10s32 = vaddq_s32(q10s32, q9s32);
+  q0s64 = vpaddlq_s32(q8s32);
+  q1s64 = vpaddlq_s32(q10s32);
+
+  d0s64 = vadd_s64(vget_low_s64(q0s64), vget_high_s64(q0s64));
+  d1s64 = vadd_s64(vget_low_s64(q1s64), vget_high_s64(q1s64));
+
+  q5s64 = vmull_s32(vreinterpret_s32_s64(d0s64), vreinterpret_s32_s64(d0s64));
+  vst1_lane_u32((uint32_t *)sse, vreinterpret_u32_s64(d1s64), 0);
+
+  d10u32 = vshr_n_u32(vreinterpret_u32_s64(vget_low_s64(q5s64)), 7);
+  d0u32 = vsub_u32(vreinterpret_u32_s64(d1s64), d10u32);
+
+  return vget_lane_u32(d0u32, 0);
+}
+
+unsigned int aom_variance8x16_neon(const unsigned char *src_ptr,
+                                   int source_stride,
+                                   const unsigned char *ref_ptr,
+                                   int recon_stride, unsigned int *sse) {
+  int i;
+  uint8x8_t d0u8, d2u8, d4u8, d6u8;
+  int16x4_t d22s16, d23s16, d24s16, d25s16;
+  uint32x2_t d0u32, d10u32;
+  int64x1_t d0s64, d1s64;
+  uint16x8_t q11u16, q12u16;
+  int32x4_t q8s32, q9s32, q10s32;
+  int64x2_t q0s64, q1s64, q5s64;
+
+  q8s32 = vdupq_n_s32(0);
+  q9s32 = vdupq_n_s32(0);
+  q10s32 = vdupq_n_s32(0);
+
+  for (i = 0; i < 8; i++) {
+    d0u8 = vld1_u8(src_ptr);
+    src_ptr += source_stride;
+    d2u8 = vld1_u8(src_ptr);
+    src_ptr += source_stride;
+    __builtin_prefetch(src_ptr);
+
+    d4u8 = vld1_u8(ref_ptr);
+    ref_ptr += recon_stride;
+    d6u8 = vld1_u8(ref_ptr);
+    ref_ptr += recon_stride;
+    __builtin_prefetch(ref_ptr);
+
+    q11u16 = vsubl_u8(d0u8, d4u8);
+    q12u16 = vsubl_u8(d2u8, d6u8);
+
+    d22s16 = vreinterpret_s16_u16(vget_low_u16(q11u16));
+    d23s16 = vreinterpret_s16_u16(vget_high_u16(q11u16));
+    q8s32 = vpadalq_s16(q8s32, vreinterpretq_s16_u16(q11u16));
+    q9s32 = vmlal_s16(q9s32, d22s16, d22s16);
+    q10s32 = vmlal_s16(q10s32, d23s16, d23s16);
+
+    d24s16 = vreinterpret_s16_u16(vget_low_u16(q12u16));
+    d25s16 = vreinterpret_s16_u16(vget_high_u16(q12u16));
+    q8s32 = vpadalq_s16(q8s32, vreinterpretq_s16_u16(q12u16));
+    q9s32 = vmlal_s16(q9s32, d24s16, d24s16);
+    q10s32 = vmlal_s16(q10s32, d25s16, d25s16);
+  }
+
+  q10s32 = vaddq_s32(q10s32, q9s32);
+  q0s64 = vpaddlq_s32(q8s32);
+  q1s64 = vpaddlq_s32(q10s32);
+
+  d0s64 = vadd_s64(vget_low_s64(q0s64), vget_high_s64(q0s64));
+  d1s64 = vadd_s64(vget_low_s64(q1s64), vget_high_s64(q1s64));
+
+  q5s64 = vmull_s32(vreinterpret_s32_s64(d0s64), vreinterpret_s32_s64(d0s64));
+  vst1_lane_u32((uint32_t *)sse, vreinterpret_u32_s64(d1s64), 0);
+
+  d10u32 = vshr_n_u32(vreinterpret_u32_s64(vget_low_s64(q5s64)), 7);
+  d0u32 = vsub_u32(vreinterpret_u32_s64(d1s64), d10u32);
+
+  return vget_lane_u32(d0u32, 0);
+}
+
+unsigned int aom_mse16x16_neon(const unsigned char *src_ptr, int source_stride,
+                               const unsigned char *ref_ptr, int recon_stride,
+                               unsigned int *sse) {
+  int i;
+  int16x4_t d22s16, d23s16, d24s16, d25s16, d26s16, d27s16, d28s16, d29s16;
+  int64x1_t d0s64;
+  uint8x16_t q0u8, q1u8, q2u8, q3u8;
+  int32x4_t q7s32, q8s32, q9s32, q10s32;
+  uint16x8_t q11u16, q12u16, q13u16, q14u16;
+  int64x2_t q1s64;
+
+  q7s32 = vdupq_n_s32(0);
+  q8s32 = vdupq_n_s32(0);
+  q9s32 = vdupq_n_s32(0);
+  q10s32 = vdupq_n_s32(0);
+
+  for (i = 0; i < 8; i++) {  // mse16x16_neon_loop
+    q0u8 = vld1q_u8(src_ptr);
+    src_ptr += source_stride;
+    q1u8 = vld1q_u8(src_ptr);
+    src_ptr += source_stride;
+    q2u8 = vld1q_u8(ref_ptr);
+    ref_ptr += recon_stride;
+    q3u8 = vld1q_u8(ref_ptr);
+    ref_ptr += recon_stride;
+
+    q11u16 = vsubl_u8(vget_low_u8(q0u8), vget_low_u8(q2u8));
+    q12u16 = vsubl_u8(vget_high_u8(q0u8), vget_high_u8(q2u8));
+    q13u16 = vsubl_u8(vget_low_u8(q1u8), vget_low_u8(q3u8));
+    q14u16 = vsubl_u8(vget_high_u8(q1u8), vget_high_u8(q3u8));
+
+    d22s16 = vreinterpret_s16_u16(vget_low_u16(q11u16));
+    d23s16 = vreinterpret_s16_u16(vget_high_u16(q11u16));
+    q7s32 = vmlal_s16(q7s32, d22s16, d22s16);
+    q8s32 = vmlal_s16(q8s32, d23s16, d23s16);
+
+    d24s16 = vreinterpret_s16_u16(vget_low_u16(q12u16));
+    d25s16 = vreinterpret_s16_u16(vget_high_u16(q12u16));
+    q9s32 = vmlal_s16(q9s32, d24s16, d24s16);
+    q10s32 = vmlal_s16(q10s32, d25s16, d25s16);
+
+    d26s16 = vreinterpret_s16_u16(vget_low_u16(q13u16));
+    d27s16 = vreinterpret_s16_u16(vget_high_u16(q13u16));
+    q7s32 = vmlal_s16(q7s32, d26s16, d26s16);
+    q8s32 = vmlal_s16(q8s32, d27s16, d27s16);
+
+    d28s16 = vreinterpret_s16_u16(vget_low_u16(q14u16));
+    d29s16 = vreinterpret_s16_u16(vget_high_u16(q14u16));
+    q9s32 = vmlal_s16(q9s32, d28s16, d28s16);
+    q10s32 = vmlal_s16(q10s32, d29s16, d29s16);
+  }
+
+  q7s32 = vaddq_s32(q7s32, q8s32);
+  q9s32 = vaddq_s32(q9s32, q10s32);
+  q10s32 = vaddq_s32(q7s32, q9s32);
+
+  q1s64 = vpaddlq_s32(q10s32);
+  d0s64 = vadd_s64(vget_low_s64(q1s64), vget_high_s64(q1s64));
+
+  vst1_lane_u32((uint32_t *)sse, vreinterpret_u32_s64(d0s64), 0);
+  return vget_lane_u32(vreinterpret_u32_s64(d0s64), 0);
+}
+
+unsigned int aom_get4x4sse_cs_neon(const unsigned char *src_ptr,
+                                   int source_stride,
+                                   const unsigned char *ref_ptr,
+                                   int recon_stride) {
+  int16x4_t d22s16, d24s16, d26s16, d28s16;
+  int64x1_t d0s64;
+  uint8x8_t d0u8, d1u8, d2u8, d3u8, d4u8, d5u8, d6u8, d7u8;
+  int32x4_t q7s32, q8s32, q9s32, q10s32;
+  uint16x8_t q11u16, q12u16, q13u16, q14u16;
+  int64x2_t q1s64;
+
+  d0u8 = vld1_u8(src_ptr);
+  src_ptr += source_stride;
+  d4u8 = vld1_u8(ref_ptr);
+  ref_ptr += recon_stride;
+  d1u8 = vld1_u8(src_ptr);
+  src_ptr += source_stride;
+  d5u8 = vld1_u8(ref_ptr);
+  ref_ptr += recon_stride;
+  d2u8 = vld1_u8(src_ptr);
+  src_ptr += source_stride;
+  d6u8 = vld1_u8(ref_ptr);
+  ref_ptr += recon_stride;
+  d3u8 = vld1_u8(src_ptr);
+  d7u8 = vld1_u8(ref_ptr);
+
+  q11u16 = vsubl_u8(d0u8, d4u8);
+  q12u16 = vsubl_u8(d1u8, d5u8);
+  q13u16 = vsubl_u8(d2u8, d6u8);
+  q14u16 = vsubl_u8(d3u8, d7u8);
+
+  d22s16 = vget_low_s16(vreinterpretq_s16_u16(q11u16));
+  d24s16 = vget_low_s16(vreinterpretq_s16_u16(q12u16));
+  d26s16 = vget_low_s16(vreinterpretq_s16_u16(q13u16));
+  d28s16 = vget_low_s16(vreinterpretq_s16_u16(q14u16));
+
+  q7s32 = vmull_s16(d22s16, d22s16);
+  q8s32 = vmull_s16(d24s16, d24s16);
+  q9s32 = vmull_s16(d26s16, d26s16);
+  q10s32 = vmull_s16(d28s16, d28s16);
+
+  q7s32 = vaddq_s32(q7s32, q8s32);
+  q9s32 = vaddq_s32(q9s32, q10s32);
+  q9s32 = vaddq_s32(q7s32, q9s32);
+
+  q1s64 = vpaddlq_s32(q9s32);
+  d0s64 = vadd_s64(vget_low_s64(q1s64), vget_high_s64(q1s64));
+
+  return vget_lane_u32(vreinterpret_u32_s64(d0s64), 0);
+}
+
+// Load 4 sets of 4 bytes when alignment is not guaranteed.
+static INLINE uint8x16_t load_unaligned_u8q(const uint8_t *buf, int stride) {
+  uint32_t a;
+  uint32x4_t a_u32 = vdupq_n_u32(0);
+  if (stride == 4) return vld1q_u8(buf);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  a_u32 = vld1q_lane_u32(&a, a_u32, 0);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  a_u32 = vld1q_lane_u32(&a, a_u32, 1);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  a_u32 = vld1q_lane_u32(&a, a_u32, 2);
+  memcpy(&a, buf, 4);
+  buf += stride;
+  a_u32 = vld1q_lane_u32(&a, a_u32, 3);
+  return vreinterpretq_u8_u32(a_u32);
+}
+
+// The variance helper functions use int16_t for sum. 8 values are accumulated
+// and then added (at which point they expand up to int32_t). To avoid overflow,
+// there can be no more than 32767 / 255 ~= 128 values accumulated in each
+// column. For a 32x32 buffer, this results in 32 / 8 = 4 values per row * 32
+// rows = 128. Asserts have been added to each function to warn against reaching
+// this limit.
+
+// Process a block of width 4 four rows at a time.
+static void variance_neon_w4x4(const uint8_t *a, int a_stride, const uint8_t *b,
+                               int b_stride, int h, uint32_t *sse, int *sum) {
+  const int32x4_t zero = vdupq_n_s32(0);
+  int16x8_t sum_s16 = vreinterpretq_s16_s32(zero);
+  int32x4_t sse_s32 = zero;
+
+  // Since width is only 4, sum_s16 only loads a half row per loop.
+  assert(h <= 256);
+
+  int i;
+  for (i = 0; i < h; i += 4) {
+    const uint8x16_t a_u8 = load_unaligned_u8q(a, a_stride);
+    const uint8x16_t b_u8 = load_unaligned_u8q(b, b_stride);
+    const int16x8_t diff_lo_s16 =
+        vreinterpretq_s16_u16(vsubl_u8(vget_low_u8(a_u8), vget_low_u8(b_u8)));
+    const int16x8_t diff_hi_s16 =
+        vreinterpretq_s16_u16(vsubl_u8(vget_high_u8(a_u8), vget_high_u8(b_u8)));
+
+    sum_s16 = vaddq_s16(sum_s16, diff_lo_s16);
+    sum_s16 = vaddq_s16(sum_s16, diff_hi_s16);
+
+    sse_s32 = vmlal_s16(sse_s32, vget_low_s16(diff_lo_s16),
+                        vget_low_s16(diff_lo_s16));
+    sse_s32 = vmlal_s16(sse_s32, vget_high_s16(diff_lo_s16),
+                        vget_high_s16(diff_lo_s16));
+
+    sse_s32 = vmlal_s16(sse_s32, vget_low_s16(diff_hi_s16),
+                        vget_low_s16(diff_hi_s16));
+    sse_s32 = vmlal_s16(sse_s32, vget_high_s16(diff_hi_s16),
+                        vget_high_s16(diff_hi_s16));
+
+    a += 4 * a_stride;
+    b += 4 * b_stride;
+  }
+
+  *sum = horizontal_add_s16x8(sum_s16);
+  *sse = (uint32_t)horizontal_add_s32x4(sse_s32);
+}
+
+// Process a block of any size where the width is divisible by 16.
+static void variance_neon_w16(const uint8_t *a, int a_stride, const uint8_t *b,
+                              int b_stride, int w, int h, uint32_t *sse,
+                              int *sum) {
+  const int32x4_t zero = vdupq_n_s32(0);
+  int16x8_t sum_s16 = vreinterpretq_s16_s32(zero);
+  int32x4_t sse_s32 = zero;
+
+  // The loop loads 16 values at a time but doubles them up when accumulating
+  // into sum_s16.
+  assert(w / 8 * h <= 128);
+
+  int i, j;
+  for (i = 0; i < h; ++i) {
+    for (j = 0; j < w; j += 16) {
+      const uint8x16_t a_u8 = vld1q_u8(a + j);
+      const uint8x16_t b_u8 = vld1q_u8(b + j);
+
+      const int16x8_t diff_lo_s16 =
+          vreinterpretq_s16_u16(vsubl_u8(vget_low_u8(a_u8), vget_low_u8(b_u8)));
+      const int16x8_t diff_hi_s16 = vreinterpretq_s16_u16(
+          vsubl_u8(vget_high_u8(a_u8), vget_high_u8(b_u8)));
+
+      sum_s16 = vaddq_s16(sum_s16, diff_lo_s16);
+      sum_s16 = vaddq_s16(sum_s16, diff_hi_s16);
+
+      sse_s32 = vmlal_s16(sse_s32, vget_low_s16(diff_lo_s16),
+                          vget_low_s16(diff_lo_s16));
+      sse_s32 = vmlal_s16(sse_s32, vget_high_s16(diff_lo_s16),
+                          vget_high_s16(diff_lo_s16));
+
+      sse_s32 = vmlal_s16(sse_s32, vget_low_s16(diff_hi_s16),
+                          vget_low_s16(diff_hi_s16));
+      sse_s32 = vmlal_s16(sse_s32, vget_high_s16(diff_hi_s16),
+                          vget_high_s16(diff_hi_s16));
+    }
+    a += a_stride;
+    b += b_stride;
+  }
+
+  *sum = horizontal_add_s16x8(sum_s16);
+  *sse = (uint32_t)horizontal_add_s32x4(sse_s32);
+}
+
+// Process a block of width 8 two rows at a time.
+static void variance_neon_w8x2(const uint8_t *a, int a_stride, const uint8_t *b,
+                               int b_stride, int h, uint32_t *sse, int *sum) {
+  const int32x4_t zero = vdupq_n_s32(0);
+  int16x8_t sum_s16 = vreinterpretq_s16_s32(zero);
+  int32x4_t sse_s32 = zero;
+
+  // Each column has it's own accumulator entry in sum_s16.
+  assert(h <= 128);
+
+  int i = 0;
+  do {
+    const uint8x8_t a_0_u8 = vld1_u8(a);
+    const uint8x8_t a_1_u8 = vld1_u8(a + a_stride);
+    const uint8x8_t b_0_u8 = vld1_u8(b);
+    const uint8x8_t b_1_u8 = vld1_u8(b + b_stride);
+    const int16x8_t diff_0_s16 =
+        vreinterpretq_s16_u16(vsubl_u8(a_0_u8, b_0_u8));
+    const int16x8_t diff_1_s16 =
+        vreinterpretq_s16_u16(vsubl_u8(a_1_u8, b_1_u8));
+    sum_s16 = vaddq_s16(sum_s16, diff_0_s16);
+    sum_s16 = vaddq_s16(sum_s16, diff_1_s16);
+    sse_s32 =
+        vmlal_s16(sse_s32, vget_low_s16(diff_0_s16), vget_low_s16(diff_0_s16));
+    sse_s32 =
+        vmlal_s16(sse_s32, vget_low_s16(diff_1_s16), vget_low_s16(diff_1_s16));
+    sse_s32 = vmlal_s16(sse_s32, vget_high_s16(diff_0_s16),
+                        vget_high_s16(diff_0_s16));
+    sse_s32 = vmlal_s16(sse_s32, vget_high_s16(diff_1_s16),
+                        vget_high_s16(diff_1_s16));
+    a += a_stride + a_stride;
+    b += b_stride + b_stride;
+    i += 2;
+  } while (i < h);
+
+  *sum = horizontal_add_s16x8(sum_s16);
+  *sse = (uint32_t)horizontal_add_s32x4(sse_s32);
+}
+
+#define VARIANCE_NXM(n, m, shift)                                           \
+  unsigned int aom_variance##n##x##m##_neon(const uint8_t *a, int a_stride, \
+                                            const uint8_t *b, int b_stride, \
+                                            unsigned int *sse) {            \
+    int sum;                                                                \
+    if (n == 4)                                                             \
+      variance_neon_w4x4(a, a_stride, b, b_stride, m, sse, &sum);           \
+    else if (n == 8)                                                        \
+      variance_neon_w8x2(a, a_stride, b, b_stride, m, sse, &sum);           \
+    else                                                                    \
+      variance_neon_w16(a, a_stride, b, b_stride, n, m, sse, &sum);         \
+    if (n * m < 16 * 16)                                                    \
+      return *sse - ((sum * sum) >> shift);                                 \
+    else                                                                    \
+      return *sse - (uint32_t)(((int64_t)sum * sum) >> shift);              \
+  }
+
+static void variance_neon_wide_block(const uint8_t *a, int a_stride,
+                                     const uint8_t *b, int b_stride, int w,
+                                     int h, uint32_t *sse, int *sum) {
+  const int32x4_t zero = vdupq_n_s32(0);
+  int32x4_t v_diff = zero;
+  int64x2_t v_sse = vreinterpretq_s64_s32(zero);
+
+  int s, i, j;
+  for (s = 0; s < 16; s++) {
+    int32x4_t sse_s32 = zero;
+    int16x8_t sum_s16 = vreinterpretq_s16_s32(zero);
+    for (i = (s * h) >> 4; i < (((s + 1) * h) >> 4); ++i) {
+      for (j = 0; j < w; j += 16) {
+        const uint8x16_t a_u8 = vld1q_u8(a + j);
+        const uint8x16_t b_u8 = vld1q_u8(b + j);
+
+        const int16x8_t diff_lo_s16 = vreinterpretq_s16_u16(
+            vsubl_u8(vget_low_u8(a_u8), vget_low_u8(b_u8)));
+        const int16x8_t diff_hi_s16 = vreinterpretq_s16_u16(
+            vsubl_u8(vget_high_u8(a_u8), vget_high_u8(b_u8)));
+
+        sum_s16 = vaddq_s16(sum_s16, diff_lo_s16);
+        sum_s16 = vaddq_s16(sum_s16, diff_hi_s16);
+
+        sse_s32 = vmlal_s16(sse_s32, vget_low_s16(diff_lo_s16),
+                            vget_low_s16(diff_lo_s16));
+        sse_s32 = vmlal_s16(sse_s32, vget_high_s16(diff_lo_s16),
+                            vget_high_s16(diff_lo_s16));
+        sse_s32 = vmlal_s16(sse_s32, vget_low_s16(diff_hi_s16),
+                            vget_low_s16(diff_hi_s16));
+        sse_s32 = vmlal_s16(sse_s32, vget_high_s16(diff_hi_s16),
+                            vget_high_s16(diff_hi_s16));
+      }
+
+      a += a_stride;
+      b += b_stride;
+    }
+
+    v_diff = vpadalq_s16(v_diff, sum_s16);
+    v_sse = vpadalq_s32(v_sse, sse_s32);
+  }
+  int diff = horizontal_add_s32x4(v_diff);
+#if defined(__aarch64__)
+  uint32_t sq = (uint32_t)vaddvq_u64(vreinterpretq_u64_s64(v_sse));
+#else
+  uint32_t sq = vget_lane_u32(
+      vreinterpret_u32_s64(vadd_s64(vget_low_s64(v_sse), vget_high_s64(v_sse))),
+      0);
+#endif
+
+  *sum = diff;
+  *sse = sq;
+}
+
+#define VARIANCE_NXM_WIDE(W, H)                                             \
+  unsigned int aom_variance##W##x##H##_neon(const uint8_t *a, int a_stride, \
+                                            const uint8_t *b, int b_stride, \
+                                            uint32_t *sse) {                \
+    int sum;                                                                \
+    variance_neon_wide_block(a, a_stride, b, b_stride, W, H, sse, &sum);    \
+    return *sse - (uint32_t)(((int64_t)sum * sum) / (W * H));               \
+  }
+
+VARIANCE_NXM(4, 4, 4)
+VARIANCE_NXM(4, 8, 5)
+VARIANCE_NXM(8, 4, 5)
+VARIANCE_NXM(16, 32, 9)
+VARIANCE_NXM(32, 16, 9)
+VARIANCE_NXM_WIDE(128, 64)
+VARIANCE_NXM_WIDE(64, 128)
diff --git a/Source/Lib/Encoder/ASM_SSE2/CMakeLists.txt b/Source/Lib/Encoder/ASM_SSE2/CMakeLists.txt
index 0ebbe79..6f09383 100644
--- a/Source/Lib/Encoder/ASM_SSE2/CMakeLists.txt
+++ b/Source/Lib/Encoder/ASM_SSE2/CMakeLists.txt
@@ -20,7 +20,9 @@ include_directories(../../../API
     ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_SSE2/
      )
 
-check_both_flags_add(-msse2)
+if(NOT HAVE_AARCH64_PLATFORM)
+    check_both_flags_add(-msse2)
+endif()
 
 if(CMAKE_C_COMPILER_ID STREQUAL "Intel" AND NOT WIN32)
     check_both_flags_add(-static-intel -w)
@@ -37,10 +39,12 @@ set(all_files
     variance_sse2.c
     )
 
-set(asm_files
-    highbd_variance_impl_sse2.asm
-    subpel_variance_sse2.asm
-    x64RegisterUtil.asm)
+if(NOT HAVE_AARCH64_PLATFORM)
+    set(asm_files
+        highbd_variance_impl_sse2.asm
+        subpel_variance_sse2.asm
+        x64RegisterUtil.asm)
+endif()
 
 add_library(ENCODER_ASM_SSE2 OBJECT ${all_files})
 
diff --git a/Source/Lib/Encoder/ASM_SSE2/EbComputeMean_Intrinsic_SSE2.c b/Source/Lib/Encoder/ASM_SSE2/EbComputeMean_Intrinsic_SSE2.c
index 0249345..e206e8b 100644
--- a/Source/Lib/Encoder/ASM_SSE2/EbComputeMean_Intrinsic_SSE2.c
+++ b/Source/Lib/Encoder/ASM_SSE2/EbComputeMean_Intrinsic_SSE2.c
@@ -9,7 +9,7 @@
 * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
 */
 
-#include <emmintrin.h>
+#include "simd.h"
 #include "EbComputeMean_SSE2.h"
 
 uint64_t svt_aom_compute_subd_mean_of_squared_values8x8_sse2_intrin(
diff --git a/Source/Lib/Encoder/ASM_SSE2/EbMeSadCalculation_Intrinsic_SSE2.c b/Source/Lib/Encoder/ASM_SSE2/EbMeSadCalculation_Intrinsic_SSE2.c
index a845f98..48a71fb 100644
--- a/Source/Lib/Encoder/ASM_SSE2/EbMeSadCalculation_Intrinsic_SSE2.c
+++ b/Source/Lib/Encoder/ASM_SSE2/EbMeSadCalculation_Intrinsic_SSE2.c
@@ -10,7 +10,7 @@
 */
 
 #include "EbDefinitions.h"
-#include <emmintrin.h>
+#include "simd.h"
 #include <stdint.h>
 
 void svt_initialize_buffer_32bits_sse2_intrin(uint32_t *pointer, uint32_t count128, uint32_t count32, uint32_t value) {
diff --git a/Source/Lib/Encoder/ASM_SSE2/EbVariance_SSE2.h b/Source/Lib/Encoder/ASM_SSE2/EbVariance_SSE2.h
index 28722b0..52d760d 100644
--- a/Source/Lib/Encoder/ASM_SSE2/EbVariance_SSE2.h
+++ b/Source/Lib/Encoder/ASM_SSE2/EbVariance_SSE2.h
@@ -14,7 +14,7 @@
 
 #include "EbDefinitions.h"
 #include <assert.h>
-#include <emmintrin.h> // SSE2
+#include "simd.h"
 #include "aom_dsp_rtcd.h"
 
 // Read 4 samples from each of row and row + 1. Interleave the two rows and
diff --git a/Source/Lib/Encoder/ASM_SSE2/encodetxb_sse2.c b/Source/Lib/Encoder/ASM_SSE2/encodetxb_sse2.c
index 3f8bf86..58c3e26 100644
--- a/Source/Lib/Encoder/ASM_SSE2/encodetxb_sse2.c
+++ b/Source/Lib/Encoder/ASM_SSE2/encodetxb_sse2.c
@@ -10,7 +10,7 @@
  */
 
 #include <assert.h>
-#include <emmintrin.h> // SSE2
+#include "simd.h"
 #include <stdint.h>
 #include "EbDefinitions.h"
 #include "EbCabacContextModel.h"
diff --git a/Source/Lib/Encoder/ASM_SSE2/fft_sse2.c b/Source/Lib/Encoder/ASM_SSE2/fft_sse2.c
index a84b2b8..3aa68eb 100644
--- a/Source/Lib/Encoder/ASM_SSE2/fft_sse2.c
+++ b/Source/Lib/Encoder/ASM_SSE2/fft_sse2.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
  */
 
-#include <xmmintrin.h>
+#include "simd.h"
 
 #include "EbDefinitions.h"
 #include "fft_common.h"
diff --git a/Source/Lib/Encoder/ASM_SSE2/highbd_variance_sse2.c b/Source/Lib/Encoder/ASM_SSE2/highbd_variance_sse2.c
index b8f6604..decb312 100644
--- a/Source/Lib/Encoder/ASM_SSE2/highbd_variance_sse2.c
+++ b/Source/Lib/Encoder/ASM_SSE2/highbd_variance_sse2.c
@@ -30,6 +30,8 @@ uint32_t svt_aom_highbd_calc16x16var_sse2(const uint16_t *src, int32_t src_strid
 #ifdef __cplusplus
 }
 #endif // __cplusplus
+
+#ifndef ARCH_AARCH64
 static void highbd_8_variance_sse2(const uint16_t *src, int32_t src_stride, const uint16_t *ref, int32_t ref_stride,
                                    int32_t w, int32_t h, uint32_t *sse, int32_t *sum, HighVarianceFn var_fn,
                                    int32_t block_size) {
@@ -120,3 +122,4 @@ void svt_aom_highbd_8_mse16x16_sse2(const uint8_t *src8, int32_t src_stride, con
     /*TODO: Remove calculate unused sum.*/
     highbd_8_variance_sse2(src, src_stride, ref, ref_stride, 16, 16, sse, &sum, svt_aom_highbd_calc16x16var_sse2, 16);
 }
+#endif
diff --git a/Source/Lib/Encoder/ASM_SSE2/variance_sse2.c b/Source/Lib/Encoder/ASM_SSE2/variance_sse2.c
index 9330cc4..725389e 100644
--- a/Source/Lib/Encoder/ASM_SSE2/variance_sse2.c
+++ b/Source/Lib/Encoder/ASM_SSE2/variance_sse2.c
@@ -10,7 +10,7 @@
  */
 #include "EbDefinitions.h"
 #include <assert.h>
-#include <emmintrin.h> // SSE2
+#include "simd.h"
 #include "aom_dsp_rtcd.h"
 #include "EbVariance_SSE2.h"
 #include "synonyms.h"
@@ -256,7 +256,9 @@ DECL(8);
 DECL(16);
 
 #undef DECL
-
+#ifdef ARCH_AARCH64
+#define FN(w, h, wf, wlog2, hlog2, cast_prod, cast)
+#else
 #define FN(w, h, wf, wlog2, hlog2, cast_prod, cast)                                                          \
     unsigned int svt_aom_sub_pixel_variance##w##x##h##_sse2(const uint8_t *src,                              \
                                                             int            src_stride,                       \
@@ -288,7 +290,7 @@ DECL(16);
         *sse_ptr = sse;                                                                                      \
         return sse - (unsigned int)(cast_prod(cast se * se) >> (wlog2 + hlog2));                             \
     }
-
+#endif
 FN(128, 128, 16, 7, 7, (int64_t), (int64_t));
 FN(128, 64, 16, 7, 6, (int64_t), (int64_t));
 FN(64, 128, 16, 6, 7, (int64_t), (int64_t));
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/CMakeLists.txt b/Source/Lib/Encoder/ASM_SSE4_1/CMakeLists.txt
index a4756ca..ef75c9c 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/CMakeLists.txt
+++ b/Source/Lib/Encoder/ASM_SSE4_1/CMakeLists.txt
@@ -21,7 +21,9 @@ include_directories(../../../API
     ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_AVX2/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_AVX512/)
 
-check_both_flags_add(-msse4.1)
+if(NOT HAVE_AARCH64_PLATFORM)
+    check_both_flags_add(-msse4.1)
+endif()
 
 if(CMAKE_C_COMPILER_ID STREQUAL "Intel" AND NOT WIN32)
     check_both_flags_add(-static-intel -w)
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/EbCdef_sse4.c b/Source/Lib/Encoder/ASM_SSE4_1/EbCdef_sse4.c
index e756066..c2db8a6 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/EbCdef_sse4.c
+++ b/Source/Lib/Encoder/ASM_SSE4_1/EbCdef_sse4.c
@@ -10,7 +10,7 @@
 */
 
 #include "EbDefinitions.h"
-#include <immintrin.h>
+#include "simd.h"
 #include <math.h>
 
 static INLINE void mse_4x4_16bit_2x_subsampled_sse4_1(const uint16_t **src, const uint16_t *dst, const int32_t dstride,
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/EbComputeSAD_Intrinsic_SSE4_1.c b/Source/Lib/Encoder/ASM_SSE4_1/EbComputeSAD_Intrinsic_SSE4_1.c
index dba9960..bea9310 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/EbComputeSAD_Intrinsic_SSE4_1.c
+++ b/Source/Lib/Encoder/ASM_SSE4_1/EbComputeSAD_Intrinsic_SSE4_1.c
@@ -14,7 +14,7 @@
 #include "EbDefinitions.h"
 #include "EbComputeSAD_C.h"
 #include "EbUtility.h"
-#include <smmintrin.h>
+#include "simd.h"
 
 #include "EbComputeSAD.h"
 #include "mcomp.h"
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/EbTemporalFiltering_SSE4_1.c b/Source/Lib/Encoder/ASM_SSE4_1/EbTemporalFiltering_SSE4_1.c
index 8dc524d..13a09b5 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/EbTemporalFiltering_SSE4_1.c
+++ b/Source/Lib/Encoder/ASM_SSE4_1/EbTemporalFiltering_SSE4_1.c
@@ -10,8 +10,7 @@
  */
 
 #include <assert.h>
-#include <smmintrin.h> /* SSE4.1 */
-#include <emmintrin.h>
+#include "simd.h"
 
 #include "EbDefinitions.h"
 #include "EbTemporalFiltering_constants.h"
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/av1_quantize_sse4_1.c b/Source/Lib/Encoder/ASM_SSE4_1/av1_quantize_sse4_1.c
index a82ef76..00780bd 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/av1_quantize_sse4_1.c
+++ b/Source/Lib/Encoder/ASM_SSE4_1/av1_quantize_sse4_1.c
@@ -9,9 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 #include "aom_dsp_rtcd.h"
-#include <emmintrin.h>
-#include <smmintrin.h>
-#include <xmmintrin.h>
+#include "simd.h"
 #include "synonyms.h"
 
 static INLINE void read_coeff(const TranLow *coeff, intptr_t offset, __m128i *c0, __m128i *c1) {
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/av1_txfm1d_sse4.h b/Source/Lib/Encoder/ASM_SSE4_1/av1_txfm1d_sse4.h
index 9b9292f..4722522 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/av1_txfm1d_sse4.h
+++ b/Source/Lib/Encoder/ASM_SSE4_1/av1_txfm1d_sse4.h
@@ -11,8 +11,7 @@
 
 #ifndef AV1_TXMF1D_SSE2_H_
 #define AV1_TXMF1D_SSE2_H_
-#include <emmintrin.h>
-#include <smmintrin.h>
+#include "simd.h"
 #include "av1_txfm_sse4.h"
 
 #ifdef __cplusplus
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/av1_txfm_sse4.h b/Source/Lib/Encoder/ASM_SSE4_1/av1_txfm_sse4.h
index 169c02b..0406595 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/av1_txfm_sse4.h
+++ b/Source/Lib/Encoder/ASM_SSE4_1/av1_txfm_sse4.h
@@ -12,7 +12,7 @@
 #ifndef AV1_TXFM_SSE4_H_
 #define AV1_TXFM_SSE4_H_
 
-#include <smmintrin.h>
+#include "simd.h"
 #include "EbTransforms.h"
 
 #ifdef __cplusplus
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/corner_match_sse4.c b/Source/Lib/Encoder/ASM_SSE4_1/corner_match_sse4.c
index c886936..948220b 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/corner_match_sse4.c
+++ b/Source/Lib/Encoder/ASM_SSE4_1/corner_match_sse4.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <smmintrin.h>
+#include "simd.h"
 #include "corner_match.h"
 #include "EbDefinitions.h"
 
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/encodetxb_sse4.c b/Source/Lib/Encoder/ASM_SSE4_1/encodetxb_sse4.c
index b05cd7d..d149187 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/encodetxb_sse4.c
+++ b/Source/Lib/Encoder/ASM_SSE4_1/encodetxb_sse4.c
@@ -9,8 +9,7 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
-#include <emmintrin.h> // SSE2
-#include <smmintrin.h> /* SSE4.1 */
+#include "simd.h"
 
 #include "EbDefinitions.h"
 #include "synonyms.h"
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/highbd_fwd_txfm_sse4.c b/Source/Lib/Encoder/ASM_SSE4_1/highbd_fwd_txfm_sse4.c
index 2e834ec..46a0cc9 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/highbd_fwd_txfm_sse4.c
+++ b/Source/Lib/Encoder/ASM_SSE4_1/highbd_fwd_txfm_sse4.c
@@ -11,11 +11,10 @@
 */
 
 #include <assert.h>
-#include <smmintrin.h> /* SSE4.1 */
+#include "simd.h"
 
 #include "EbDefinitions.h"
 #include "aom_dsp_rtcd.h"
-#include <emmintrin.h>
 #include "EbTransforms.h"
 #include "av1_txfm1d_sse4.h"
 
diff --git a/Source/Lib/Encoder/ASM_SSE4_1/pickrst_sse4.c b/Source/Lib/Encoder/ASM_SSE4_1/pickrst_sse4.c
index aa2e0bb..4f89782 100644
--- a/Source/Lib/Encoder/ASM_SSE4_1/pickrst_sse4.c
+++ b/Source/Lib/Encoder/ASM_SSE4_1/pickrst_sse4.c
@@ -9,8 +9,7 @@
  * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
  */
 
-#include <emmintrin.h>
-#include <smmintrin.h>
+#include "simd.h"
 
 #include "aom_dsp_rtcd.h"
 #include "transpose_sse2.h"
diff --git a/Source/Lib/Encoder/ASM_SSSE3/CMakeLists.txt b/Source/Lib/Encoder/ASM_SSSE3/CMakeLists.txt
index 4501eb7..3f89d95 100644
--- a/Source/Lib/Encoder/ASM_SSSE3/CMakeLists.txt
+++ b/Source/Lib/Encoder/ASM_SSSE3/CMakeLists.txt
@@ -21,7 +21,10 @@ include_directories(${PROJECT_SOURCE_DIR}/Source/API/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_AVX2/
     ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_AVX512/)
 
-check_both_flags_add(-mssse3)
+if(NOT HAVE_AARCH64_PLATFORM)
+    check_both_flags_add(-mssse3)
+endif()
+ 
 
 if(CMAKE_C_COMPILER_ID STREQUAL "Intel" AND NOT WIN32)
     check_both_flags_add(-static-intel -w)
@@ -31,8 +34,10 @@ set(all_files
     variance_impl_ssse3.c
     )
 
-set(asm_files subpel_variance_ssse3.asm)
-
+if(NOT HAVE_AARCH64_PLATFORM)
+    set(asm_files subpel_variance_ssse3.asm)
+endif()
+ 
 add_library(ENCODER_ASM_SSSE3 OBJECT ${all_files})
 
 asm_compile_to_target(ENCODER_ASM_SSSE3 ${asm_files})
diff --git a/Source/Lib/Encoder/ASM_SSSE3/variance_impl_ssse3.c b/Source/Lib/Encoder/ASM_SSSE3/variance_impl_ssse3.c
index d7ebd26..c3913d4 100644
--- a/Source/Lib/Encoder/ASM_SSSE3/variance_impl_ssse3.c
+++ b/Source/Lib/Encoder/ASM_SSSE3/variance_impl_ssse3.c
@@ -9,7 +9,7 @@
  * PATENTS file, you can obtain it at https://www.aomedia.org/license/patent-license.
  */
 
-#include <tmmintrin.h>
+#include "simd.h"
 
 // #include "config/aom_config.h"
 #include "aom_dsp_rtcd.h"
@@ -33,7 +33,9 @@ DECL(4);
 DECL(8);
 DECL(16);
 #undef DECL
-
+#ifdef ARCH_AARCH64
+#define FN(w, h, wf, wlog2, hlog2, cast_prod, cast)
+#else
 #define FN(w, h, wf, wlog2, hlog2, cast_prod, cast)                                                           \
     unsigned int svt_aom_sub_pixel_variance##w##x##h##_ssse3(const uint8_t *src,                              \
                                                              int            src_stride,                       \
@@ -65,7 +67,7 @@ DECL(16);
         *sse_ptr = sse;                                                                                       \
         return sse - (unsigned int)(cast_prod(cast se * se) >> (wlog2 + hlog2));                              \
     }
-
+#endif
 FN(128, 128, 16, 7, 7, (int64_t), (int64_t));
 FN(128, 64, 16, 7, 6, (int64_t), (int64_t));
 FN(64, 128, 16, 6, 7, (int64_t), (int64_t));
diff --git a/Source/Lib/Encoder/CMakeLists.txt b/Source/Lib/Encoder/CMakeLists.txt
index e2a1348..5a02a96 100644
--- a/Source/Lib/Encoder/CMakeLists.txt
+++ b/Source/Lib/Encoder/CMakeLists.txt
@@ -82,6 +82,32 @@ if(NOT COMPILE_C_ONLY AND HAVE_X86_PLATFORM)
     add_subdirectory(ASM_AVX512)
 endif()
 
+if(NOT COMPILE_C_ONLY AND HAVE_AARCH64_PLATFORM)
+    # Include Encoder Subdirectories
+    include_directories(${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_SSE2/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_SSSE3/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_SSE4_1/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSE2/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSSE3/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSE4_1/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_NEON/)
+
+    link_directories(
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_SSE2/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Common/C_DEFAULT/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_SSSE3/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Common/ASM_SSE4_1/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSE2/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSSE3/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSE4_1/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_NEON/)
+    add_subdirectory(ASM_SSE2)
+    add_subdirectory(ASM_SSSE3)
+    add_subdirectory(ASM_SSE4_1)
+    add_subdirectory(ASM_NEON)
+endif()
+
+
 # Required for cmake to be able to tell Xcode how to link all of the object files
 if(CMAKE_GENERATOR STREQUAL "Xcode")
     file(WRITE ${PROJECT_BINARY_DIR}/dummy.c "")
@@ -114,6 +140,23 @@ if(NOT COMPILE_C_ONLY AND HAVE_X86_PLATFORM)
         $<TARGET_OBJECTS:ENCODER_ASM_SSE4_1>
         $<TARGET_OBJECTS:ENCODER_ASM_AVX2>
         $<TARGET_OBJECTS:ENCODER_ASM_AVX512>)
+elseif(NOT COMPILE_C_ONLY AND HAVE_AARCH64_PLATFORM)
+    add_library(SvtAv1Enc
+        ${all_files}
+        $<TARGET_OBJECTS:COMMON_CODEC>
+        $<TARGET_OBJECTS:FASTFEAT>
+        $<TARGET_OBJECTS:COMMON_C_DEFAULT>
+        $<TARGET_OBJECTS:COMMON_ASM_SSE2>
+        $<TARGET_OBJECTS:COMMON_ASM_SSSE3>
+        $<TARGET_OBJECTS:COMMON_ASM_SSE4_1>
+        $<TARGET_OBJECTS:COMMON_ASM_NEON>
+        $<TARGET_OBJECTS:ENCODER_GLOBALS>
+        $<TARGET_OBJECTS:ENCODER_CODEC>
+        $<TARGET_OBJECTS:ENCODER_C_DEFAULT>
+        $<TARGET_OBJECTS:ENCODER_ASM_SSE2>
+        $<TARGET_OBJECTS:ENCODER_ASM_SSSE3>
+        $<TARGET_OBJECTS:ENCODER_ASM_SSE4_1>
+        $<TARGET_OBJECTS:ENCODER_ASM_NEON>)
 else()
     add_library(SvtAv1Enc
         ${all_files}
diff --git a/Source/Lib/Encoder/Codec/CMakeLists.txt b/Source/Lib/Encoder/Codec/CMakeLists.txt
index e5446f5..30491a4 100644
--- a/Source/Lib/Encoder/Codec/CMakeLists.txt
+++ b/Source/Lib/Encoder/Codec/CMakeLists.txt
@@ -215,5 +215,17 @@ set(all_files
         EncModeConfig.c
         EncModeConfig.h
         )
+if(NOT COMPILE_C_ONLY AND HAVE_AARCH64_PLATFORM)
+# Include Encoder Subdirectories
+include_directories(
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSE2/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSSE3/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_SSE4_1/
+        ${PROJECT_SOURCE_DIR}/Source/Lib/Encoder/ASM_NEON/
+        )
+endif ()
 
+file(GLOB all_files
+    "*.h"
+    "*.c")
 add_library(ENCODER_CODEC OBJECT ${all_files})
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
index a478dbe..d5ee66c 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.c
@@ -46,12 +46,30 @@
     if (((uintptr_t)NULL != (uintptr_t)avx)    && (flags & HAS_AVX))    ptr = avx;                \
     if (((uintptr_t)NULL != (uintptr_t)avx2)   && (flags & HAS_AVX2))   ptr = avx2;               \
     SET_FUNCTIONS_AVX512(ptr, avx512)
-#else /* ARCH_X86_64 */
+#define SET_FUNCTIONS_ARCH(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
+    SET_FUNCTIONS_X86(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512)
+
+#elif defined(ARCH_AARCH64)
+#define SET_FUNCTIONS_AARCH64(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
+    if (((uintptr_t)NULL != (uintptr_t)mmx)    && (flags & HAS_NEON))    ptr = mmx;               \
+    if (((uintptr_t)NULL != (uintptr_t)sse)    && (flags & HAS_NEON))    ptr = sse;               \
+    if (((uintptr_t)NULL != (uintptr_t)sse2)   && (flags & HAS_NEON))    ptr = sse2;              \
+    if (((uintptr_t)NULL != (uintptr_t)sse3)   && (flags & HAS_NEON))    ptr = sse3;              \
+    if (((uintptr_t)NULL != (uintptr_t)ssse3)  && (flags & HAS_NEON))    ptr = ssse3;             \
+    if (((uintptr_t)NULL != (uintptr_t)sse4_1) && (flags & HAS_NEON))    ptr = sse4_1;            \
+    if (((uintptr_t)NULL != (uintptr_t)sse4_2) && (flags & HAS_NEON))    ptr = sse4_2;            \
+    if (((uintptr_t)NULL != (uintptr_t)neon)   && (flags & HAS_NEON))    ptr = neon;
+#define SET_FUNCTIONS_ARCH(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
+    SET_FUNCTIONS_AARCH64(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon)
+
+#else // ARCH_AARCH64
 #define SET_FUNCTIONS_X86(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512)
-#endif /* ARCH_X86_64 */
+#define SET_FUNCTIONS_AARCH64(ptr, c, neon)
+#define SET_FUNCTIONS_ARCH(ptr, c, neon)
+#endif // ARCH_X86_64
 
 #if EXCLUDE_HASH
-#define SET_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512)     \
+#define SET_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon)     \
     do {                                                                                          \
         if (check_pointer_was_set && ptr != 0) {                                                                           \
             printf("Error: %s:%i: Pointer \"%s\" is set before!\n", __FILE__, 0, #ptr);    \
@@ -62,10 +80,10 @@
             assert(0);                                                                            \
         }                                                                                         \
         ptr = c;                                                                                  \
-        SET_FUNCTIONS_X86(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512) \
+        SET_FUNCTIONS_ARCH(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
     } while (0)
 #else
-#define SET_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512)     \
+#define SET_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon)     \
     do {                                                                                          \
         if (check_pointer_was_set && ptr != 0) {                                                                           \
             printf("Error: %s:%i: Pointer \"%s\" is set before!\n", __FILE__, __LINE__, #ptr);    \
@@ -76,25 +94,49 @@
             assert(0);                                                                            \
         }                                                                                         \
         ptr = c;                                                                                  \
-        SET_FUNCTIONS_X86(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512) \
+        SET_FUNCTIONS_ARCH(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512, neon) \
     } while (0)
 #endif
 
 /* Macros SET_* use local variable EbCpuFlags flags and Bool check_pointer_was_set */
-#define SET_ONLY_C(ptr, c)                                  SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
-#define SET_SSE2(ptr, c, sse2)                              SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, 0)
-#define SET_SSE2_SSSE3(ptr, c, sse2, ssse3)                 SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, 0, 0)
-#define SET_SSE2_AVX2(ptr, c, sse2, avx2)                   SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, 0)
-#define SET_SSE2_AVX512(ptr, c, sse2, avx512)               SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, avx512)
-#define SET_SSE2_SSSE3_AVX2_AVX512(ptr, c, sse2, ssse3, avx2, avx512) SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, avx2, avx512)
-#define SET_SSSE3(ptr, c, ssse3)                            SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, 0, 0)
-#define SET_SSSE3_AVX2(ptr, c, ssse3, avx2)                 SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, avx2, 0)
-#define SET_SSE41(ptr, c, sse4_1)                           SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, 0, 0)
-#define SET_SSE41_AVX2(ptr, c, sse4_1, avx2)                SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, 0)
-#define SET_SSE41_AVX2_AVX512(ptr, c, sse4_1, avx2, avx512) SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, avx512)
-#define SET_AVX2(ptr, c, avx2)                              SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, avx2, 0)
-#define SET_AVX2_AVX512(ptr, c, avx2, avx512)               SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, avx2, avx512)
-#define SET_SSE2_AVX2_AVX512(ptr, c, sse2, avx2, avx512)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, avx512)
+#define SET_ONLY_C(ptr, c)                                  SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2(ptr, c, sse2)                              SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2_SSSE3(ptr, c, sse2, ssse3)                 SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2_AVX2(ptr, c, sse2, avx2)                   SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, 0, 0)
+#define SET_SSE2_AVX512(ptr, c, sse2, avx512)               SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, avx512, 0)
+#define SET_SSE2_SSSE3_AVX2_AVX512(ptr, c, sse2, ssse3, avx2, avx512) SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, avx2, avx512, 0)
+#define SET_SSSE3(ptr, c, ssse3)                            SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, 0, 0, 0)
+#define SET_SSSE3_AVX2(ptr, c, ssse3, avx2)                 SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, ssse3, 0, 0, 0, avx2, 0, 0)
+#define SET_SSE41(ptr, c, sse4_1)                           SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, 0, 0, 0)
+#define SET_SSE41_AVX2(ptr, c, sse4_1, avx2)                SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, 0, 0)
+#define SET_SSE41_AVX2_AVX512(ptr, c, sse4_1, avx2, avx512) SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, avx512, 0)
+#define SET_AVX2(ptr, c, avx2)                              SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, avx2, 0, 0)
+#define SET_AVX2_AVX512(ptr, c, avx2, avx512)               SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, avx2, avx512, 0)
+#define SET_SSE2_AVX2_AVX512(ptr, c, sse2, avx2, avx512)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, avx512, 0)
+
+#ifdef ARCH_AARCH64
+#define SET_NEON(ptr, c, neon)                              SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, neon)
+#define SET_SSE2_NEON(ptr, c, sse2, neon)                   SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, 0, neon)
+#define SET_SSE2_SSSE3_NEON(ptr, c, sse2, ssse3, neon)      SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, 0, 0, neon)
+#define SET_SSE2_AVX2_NEON(ptr, c, sse2, avx2, neon)        SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, 0, neon)
+#define SET_SSE2_AVX2_AVX512_NEON(ptr, c, sse2, avx2, avx512, neon)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, avx512, neon)
+#define SET_SSE2_SSSE3_AVX2_AVX512_NEON(ptr, c, sse2, ssse3, avx2, avx512, neon) SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, avx2, avx512, neon)
+#define SET_SSE41_AVX2_NEON(ptr, c, sse4_1, avx2, neon)     SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, 0, neon)
+#define SET_SSE41_AVX2_AVX512_NEON(ptr, c, sse4_1, avx2, avx512, neon) SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, avx512, neon)
+#define SET_ONLY_TEMP2_C(ptr, c, sse2)                      SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_ONLY_TEMP3_C(ptr, c, sse2, ssse3)               SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_ONLY_TEMP5_C(ptr, c, sse2, ssse3, sse4_1, avx2) SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_TEMP_FUNCTIONS(ptr, c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, avx2, avx512) \
+                                                            SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
+#else
+#define SET_SSE2_NEON(ptr, c, sse2, neon)                   SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2_SSSE3_NEON(ptr, c, sse2, ssse3, neon)      SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, 0, 0, 0)
+#define SET_SSE2_AVX2_NEON(ptr, c, sse2, avx2, neon)        SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, 0, 0)
+#define SET_SSE41_AVX2_NEON(ptr, c, sse4_1, avx2, neon)     SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, 0, 0)
+#define SET_SSE41_AVX2_AVX512_NEON(ptr, c, sse4_1, avx2, avx512, neon) SET_FUNCTIONS(ptr, c, 0, 0, 0, 0, 0, sse4_1, 0, 0, avx2, avx512, 0)
+#define SET_SSE2_AVX2_AVX512_NEON(ptr, c, sse2, avx2, avx512, neon)    SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, 0, 0, 0, 0, avx2, avx512, 0)
+#define SET_SSE2_SSSE3_AVX2_AVX512_NEON(ptr, c, sse2, ssse3, avx2, avx512, neon) SET_FUNCTIONS(ptr, c, 0, 0, sse2, 0, ssse3, 0, 0, 0, avx2, avx512, 0)
+#endif // ARCH_AARCH64
 
 void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     /* Avoid check that pointer is set double, after first  setup. */
@@ -118,11 +160,15 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_SSE41_AVX2(svt_compute_cdef_dist_8bit, svt_aom_compute_cdef_dist_8bit_c, svt_aom_compute_cdef_dist_8bit_sse4_1, svt_aom_compute_cdef_dist_8bit_avx2);
     SET_SSE41_AVX2_AVX512(svt_av1_compute_stats, svt_av1_compute_stats_c, svt_av1_compute_stats_sse4_1, svt_av1_compute_stats_avx2, svt_av1_compute_stats_avx512);
     SET_SSE41_AVX2_AVX512(svt_av1_compute_stats_highbd, svt_av1_compute_stats_highbd_c, svt_av1_compute_stats_highbd_sse4_1, svt_av1_compute_stats_highbd_avx2, svt_av1_compute_stats_highbd_avx512);
-    SET_SSE41_AVX2_AVX512(svt_av1_lowbd_pixel_proj_error, svt_av1_lowbd_pixel_proj_error_c, svt_av1_lowbd_pixel_proj_error_sse4_1, svt_av1_lowbd_pixel_proj_error_avx2, svt_av1_lowbd_pixel_proj_error_avx512);
+    SET_SSE41_AVX2_AVX512_NEON(svt_av1_lowbd_pixel_proj_error, svt_av1_lowbd_pixel_proj_error_c, svt_av1_lowbd_pixel_proj_error_sse4_1, svt_av1_lowbd_pixel_proj_error_avx2, svt_av1_lowbd_pixel_proj_error_avx512, svt_av1_lowbd_pixel_proj_error_neon);
     SET_SSE41_AVX2(svt_av1_highbd_pixel_proj_error, svt_av1_highbd_pixel_proj_error_c, svt_av1_highbd_pixel_proj_error_sse4_1, svt_av1_highbd_pixel_proj_error_avx2);
     SET_AVX2(svt_av1_calc_frame_error, svt_av1_calc_frame_error_c, svt_av1_calc_frame_error_avx2);
     SET_AVX2(svt_subtract_average, svt_subtract_average_c, svt_subtract_average_avx2);
+#ifdef ARCH_AARCH64
+    SET_NEON(svt_get_proj_subspace, svt_get_proj_subspace_c, svt_get_proj_subspace_neon);
+#else
     SET_AVX2(svt_get_proj_subspace, svt_get_proj_subspace_c, svt_get_proj_subspace_avx2);
+#endif
     SET_SSE41_AVX2(svt_aom_quantize_b, svt_aom_quantize_b_c_ii, svt_aom_quantize_b_sse4_1, svt_aom_quantize_b_avx2);
     SET_SSE41_AVX2(svt_aom_highbd_quantize_b, svt_aom_highbd_quantize_b_c, svt_aom_highbd_quantize_b_sse4_1, svt_aom_highbd_quantize_b_avx2);
     SET_AVX2(svt_av1_quantize_b_qm, svt_aom_quantize_b_c_ii, svt_av1_quantize_b_qm_avx2);
@@ -133,8 +179,11 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_SSE41_AVX2(svt_av1_highbd_quantize_fp, svt_av1_highbd_quantize_fp_c, svt_av1_highbd_quantize_fp_sse4_1, svt_av1_highbd_quantize_fp_avx2);
     SET_AVX2(svt_av1_quantize_fp_qm, svt_av1_quantize_fp_qm_c, svt_av1_quantize_fp_qm_avx2);
     SET_AVX2(svt_av1_highbd_quantize_fp_qm, svt_av1_highbd_quantize_fp_qm_c, svt_av1_highbd_quantize_fp_qm_avx2);
+#ifdef ARCH_AARCH64
+    SET_ONLY_TEMP2_C(svt_aom_highbd_8_mse16x16, svt_aom_highbd_8_mse16x16_c, svt_aom_highbd_8_mse16x16_sse2);
+#else
     SET_SSE2(svt_aom_highbd_8_mse16x16, svt_aom_highbd_8_mse16x16_c, svt_aom_highbd_8_mse16x16_sse2);
-
+#endif
     //SAD
     SET_SSE2_AVX2(svt_aom_mse16x16, svt_aom_mse16x16_c, svt_aom_mse16x16_sse2, svt_aom_mse16x16_avx2);
     SET_AVX2(svt_aom_sad4x4, svt_aom_sad4x4_c, svt_aom_sad4x4_avx2);
@@ -256,34 +305,54 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_SSE41_AVX2(svt_aom_obmc_variance128x128, svt_aom_obmc_variance128x128_c, svt_aom_obmc_variance128x128_sse4_1, svt_aom_obmc_variance128x128_avx2);
 
     //VARIANCE
-    SET_SSE2(svt_aom_variance4x4, svt_aom_variance4x4_c, svt_aom_variance4x4_sse2);
-    SET_SSE2(svt_aom_variance4x8, svt_aom_variance4x8_c, svt_aom_variance4x8_sse2);
+    SET_SSE2_NEON(svt_aom_variance4x4, svt_aom_variance4x4_c, svt_aom_variance4x4_sse2, aom_variance4x4_neon);
+    SET_SSE2_NEON(svt_aom_variance4x8, svt_aom_variance4x8_c, svt_aom_variance4x8_sse2, aom_variance4x8_neon);
     SET_SSE2(svt_aom_variance4x16, svt_aom_variance4x16_c, svt_aom_variance4x16_sse2);
-    SET_SSE2(svt_aom_variance8x4, svt_aom_variance8x4_c, svt_aom_variance8x4_sse2);
-    SET_SSE2(svt_aom_variance8x8, svt_aom_variance8x8_c, svt_aom_variance8x8_sse2);
-    SET_SSE2(svt_aom_variance8x16, svt_aom_variance8x16_c, svt_aom_variance8x16_sse2);
+    SET_SSE2_NEON(svt_aom_variance8x4, svt_aom_variance8x4_c, svt_aom_variance8x4_sse2, aom_variance8x4_neon);
+    SET_SSE2_NEON(svt_aom_variance8x8, svt_aom_variance8x8_c, svt_aom_variance8x8_sse2, aom_variance8x8_neon);
+    SET_SSE2_NEON(svt_aom_variance8x16, svt_aom_variance8x16_c, svt_aom_variance8x16_sse2, aom_variance8x16_neon);
     SET_SSE2(svt_aom_variance8x32, svt_aom_variance8x32_c, svt_aom_variance8x32_sse2);
     SET_SSE2_AVX2(svt_aom_variance16x4, svt_aom_variance16x4_c, svt_aom_variance16x4_sse2, svt_aom_variance16x4_avx2);
-    SET_SSE2_AVX2(svt_aom_variance16x8, svt_aom_variance16x8_c, svt_aom_variance16x8_sse2, svt_aom_variance16x8_avx2);
-    SET_SSE2_AVX2(svt_aom_variance16x16, svt_aom_variance16x16_c, svt_aom_variance16x16_sse2, svt_aom_variance16x16_avx2);
-    SET_SSE2_AVX2(svt_aom_variance16x32, svt_aom_variance16x32_c, svt_aom_variance16x32_sse2, svt_aom_variance16x32_avx2);
+    SET_SSE2_AVX2_NEON(svt_aom_variance16x8, svt_aom_variance16x8_c, svt_aom_variance16x8_sse2, svt_aom_variance16x8_avx2, aom_variance16x8_neon);
+    SET_SSE2_AVX2_NEON(svt_aom_variance16x16, svt_aom_variance16x16_c, svt_aom_variance16x16_sse2, svt_aom_variance16x16_avx2, aom_variance16x16_neon);
+    SET_SSE2_AVX2_NEON(svt_aom_variance16x32, svt_aom_variance16x32_c, svt_aom_variance16x32_sse2, svt_aom_variance16x32_avx2, aom_variance16x32_neon);
     SET_SSE2_AVX2(svt_aom_variance16x64, svt_aom_variance16x64_c, svt_aom_variance16x64_sse2, svt_aom_variance16x64_avx2);
     SET_SSE2_AVX2_AVX512(svt_aom_variance32x8, svt_aom_variance32x8_c, svt_aom_variance32x8_sse2, svt_aom_variance32x8_avx2, svt_aom_variance32x8_avx512);
-    SET_SSE2_AVX2_AVX512(svt_aom_variance32x16, svt_aom_variance32x16_c, svt_aom_variance32x16_sse2, svt_aom_variance32x16_avx2, svt_aom_variance32x16_avx512);
-    SET_SSE2_AVX2_AVX512(svt_aom_variance32x32, svt_aom_variance32x32_c, svt_aom_variance32x32_sse2, svt_aom_variance32x32_avx2, svt_aom_variance32x32_avx512);
-    SET_SSE2_AVX2_AVX512(svt_aom_variance32x64, svt_aom_variance32x64_c, svt_aom_variance32x64_sse2, svt_aom_variance32x64_avx2, svt_aom_variance32x64_avx512);
+    SET_SSE2_AVX2_AVX512_NEON(svt_aom_variance32x16, svt_aom_variance32x16_c, svt_aom_variance32x16_sse2, svt_aom_variance32x16_avx2, svt_aom_variance32x16_avx512, aom_variance32x16_neon);
+    SET_SSE2_AVX2_AVX512_NEON(svt_aom_variance32x32, svt_aom_variance32x32_c, svt_aom_variance32x32_sse2, svt_aom_variance32x32_avx2, svt_aom_variance32x32_avx512, aom_variance32x32_neon);
+    SET_SSE2_AVX2_AVX512_NEON(svt_aom_variance32x64, svt_aom_variance32x64_c, svt_aom_variance32x64_sse2, svt_aom_variance32x64_avx2, svt_aom_variance32x64_avx512, aom_variance32x64_neon);
     SET_SSE2_AVX2_AVX512(svt_aom_variance64x16, svt_aom_variance64x16_c, svt_aom_variance64x16_sse2, svt_aom_variance64x16_avx2, svt_aom_variance64x16_avx512);
-    SET_SSE2_AVX2_AVX512(svt_aom_variance64x32, svt_aom_variance64x32_c, svt_aom_variance64x32_sse2, svt_aom_variance64x32_avx2, svt_aom_variance64x32_avx512);
-    SET_SSE2_AVX2_AVX512(svt_aom_variance64x64, svt_aom_variance64x64_c, svt_aom_variance64x64_sse2, svt_aom_variance64x64_avx2, svt_aom_variance64x64_avx512);
-    SET_SSE2_AVX2_AVX512(svt_aom_variance64x128, svt_aom_variance64x128_c, svt_aom_variance64x128_sse2, svt_aom_variance64x128_avx2, svt_aom_variance64x128_avx512);
-    SET_SSE2_AVX2_AVX512(svt_aom_variance128x64, svt_aom_variance128x64_c, svt_aom_variance128x64_sse2, svt_aom_variance128x64_avx2, svt_aom_variance128x64_avx512);
-    SET_SSE2_AVX2_AVX512(svt_aom_variance128x128, svt_aom_variance128x128_c,svt_aom_variance128x128_sse2, svt_aom_variance128x128_avx2, svt_aom_variance128x128_avx512);
+    SET_SSE2_AVX2_AVX512_NEON(svt_aom_variance64x32, svt_aom_variance64x32_c, svt_aom_variance64x32_sse2, svt_aom_variance64x32_avx2, svt_aom_variance64x32_avx512, aom_variance64x32_neon);
+    SET_SSE2_AVX2_AVX512_NEON(svt_aom_variance64x64, svt_aom_variance64x64_c, svt_aom_variance64x64_sse2, svt_aom_variance64x64_avx2, svt_aom_variance64x64_avx512, aom_variance64x64_neon);
+    SET_SSE2_AVX2_AVX512_NEON(svt_aom_variance64x128, svt_aom_variance64x128_c, svt_aom_variance64x128_sse2, svt_aom_variance64x128_avx2, svt_aom_variance64x128_avx512, aom_variance64x128_neon);
+    SET_SSE2_AVX2_AVX512_NEON(svt_aom_variance128x64, svt_aom_variance128x64_c, svt_aom_variance128x64_sse2, svt_aom_variance128x64_avx2, svt_aom_variance128x64_avx512, aom_variance128x64_neon);
+    SET_SSE2_AVX2_AVX512_NEON(svt_aom_variance128x128, svt_aom_variance128x128_c,svt_aom_variance128x128_sse2, svt_aom_variance128x128_avx2, svt_aom_variance128x128_avx512, aom_variance128x128_neon);
 
     //VARIANCEHBP
     SET_ONLY_C(svt_aom_highbd_10_variance4x4, svt_aom_highbd_10_variance4x4_c);
     SET_ONLY_C(svt_aom_highbd_10_variance4x8, svt_aom_highbd_10_variance4x8_c);
     SET_ONLY_C(svt_aom_highbd_10_variance4x16, svt_aom_highbd_10_variance4x16_c);
     SET_ONLY_C(svt_aom_highbd_10_variance8x4, svt_aom_highbd_10_variance8x4_c);
+ #ifdef ARCH_AARCH64
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance8x8, svt_aom_highbd_10_variance8x8_c, svt_aom_highbd_10_variance8x8_sse2, svt_aom_highbd_10_variance8x8_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance8x16, svt_aom_highbd_10_variance8x16_c, svt_aom_highbd_10_variance8x16_sse2, svt_aom_highbd_10_variance8x16_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance8x32, svt_aom_highbd_10_variance8x32_c, svt_aom_highbd_10_variance8x32_sse2, svt_aom_highbd_10_variance8x32_avx2);
+    SET_ONLY_TEMP2_C(svt_aom_highbd_10_variance16x4, svt_aom_highbd_10_variance16x4_c, svt_aom_highbd_10_variance16x4_sse2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance16x8, svt_aom_highbd_10_variance16x8_c, svt_aom_highbd_10_variance16x8_sse2, svt_aom_highbd_10_variance16x8_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance16x16, svt_aom_highbd_10_variance16x16_c, svt_aom_highbd_10_variance16x16_sse2, svt_aom_highbd_10_variance16x16_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance16x32, svt_aom_highbd_10_variance16x32_c, svt_aom_highbd_10_variance16x32_sse2, svt_aom_highbd_10_variance16x32_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance16x64, svt_aom_highbd_10_variance16x64_c, svt_aom_highbd_10_variance16x64_sse2, svt_aom_highbd_10_variance16x64_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance32x8, svt_aom_highbd_10_variance32x8_c, svt_aom_highbd_10_variance32x8_sse2, svt_aom_highbd_10_variance32x8_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance32x16, svt_aom_highbd_10_variance32x16_c, svt_aom_highbd_10_variance32x16_sse2, svt_aom_highbd_10_variance32x16_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance32x32, svt_aom_highbd_10_variance32x32_c, svt_aom_highbd_10_variance32x32_sse2, svt_aom_highbd_10_variance32x32_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance32x64, svt_aom_highbd_10_variance32x64_c, svt_aom_highbd_10_variance32x64_sse2, svt_aom_highbd_10_variance32x64_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance64x16, svt_aom_highbd_10_variance64x16_c, svt_aom_highbd_10_variance64x16_sse2, svt_aom_highbd_10_variance64x16_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance64x32, svt_aom_highbd_10_variance64x32_c, svt_aom_highbd_10_variance64x32_sse2, svt_aom_highbd_10_variance64x32_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance64x64, svt_aom_highbd_10_variance64x64_c, svt_aom_highbd_10_variance64x64_sse2, svt_aom_highbd_10_variance64x64_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance64x128, svt_aom_highbd_10_variance64x128_c, svt_aom_highbd_10_variance64x128_sse2, svt_aom_highbd_10_variance64x128_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance128x64, svt_aom_highbd_10_variance128x64_c, svt_aom_highbd_10_variance128x64_sse2, svt_aom_highbd_10_variance128x64_avx2);
+    SET_ONLY_TEMP3_C(svt_aom_highbd_10_variance128x128, svt_aom_highbd_10_variance128x128_c, svt_aom_highbd_10_variance128x128_sse2, svt_aom_highbd_10_variance128x128_avx2);
+ #else
     SET_SSE2_AVX2(svt_aom_highbd_10_variance8x8, svt_aom_highbd_10_variance8x8_c, svt_aom_highbd_10_variance8x8_sse2, svt_aom_highbd_10_variance8x8_avx2);
     SET_SSE2_AVX2(svt_aom_highbd_10_variance8x16, svt_aom_highbd_10_variance8x16_c, svt_aom_highbd_10_variance8x16_sse2, svt_aom_highbd_10_variance8x16_avx2);
     SET_SSE2_AVX2(svt_aom_highbd_10_variance8x32, svt_aom_highbd_10_variance8x32_c, svt_aom_highbd_10_variance8x32_sse2, svt_aom_highbd_10_variance8x32_avx2);
@@ -302,29 +371,31 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_SSE2_AVX2(svt_aom_highbd_10_variance64x128, svt_aom_highbd_10_variance64x128_c, svt_aom_highbd_10_variance64x128_sse2, svt_aom_highbd_10_variance64x128_avx2);
     SET_SSE2_AVX2(svt_aom_highbd_10_variance128x64, svt_aom_highbd_10_variance128x64_c, svt_aom_highbd_10_variance128x64_sse2, svt_aom_highbd_10_variance128x64_avx2);
     SET_SSE2_AVX2(svt_aom_highbd_10_variance128x128, svt_aom_highbd_10_variance128x128_c, svt_aom_highbd_10_variance128x128_sse2, svt_aom_highbd_10_variance128x128_avx2);
-    SET_SSE2_SSSE3_AVX2_AVX512(svt_aom_sub_pixel_variance128x128, svt_aom_sub_pixel_variance128x128_c, svt_aom_sub_pixel_variance128x128_sse2, svt_aom_sub_pixel_variance128x128_ssse3, svt_aom_sub_pixel_variance128x128_avx2, svt_aom_sub_pixel_variance128x128_avx512);
-    SET_SSE2_SSSE3_AVX2_AVX512(svt_aom_sub_pixel_variance128x64, svt_aom_sub_pixel_variance128x64_c, svt_aom_sub_pixel_variance128x64_sse2, svt_aom_sub_pixel_variance128x64_ssse3, svt_aom_sub_pixel_variance128x64_avx2, svt_aom_sub_pixel_variance128x64_avx512);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x16, svt_aom_sub_pixel_variance16x16_c, NULL, NULL, svt_aom_sub_pixel_variance16x16_sse2, NULL, svt_aom_sub_pixel_variance16x16_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x16_avx2, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x32, svt_aom_sub_pixel_variance16x32_c, NULL, NULL, svt_aom_sub_pixel_variance16x32_sse2, NULL, svt_aom_sub_pixel_variance16x32_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x32_avx2, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x4, svt_aom_sub_pixel_variance16x4_c, NULL, NULL, svt_aom_sub_pixel_variance16x4_sse2, NULL, svt_aom_sub_pixel_variance16x4_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x4_avx2, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x64, svt_aom_sub_pixel_variance16x64_c, NULL, NULL, svt_aom_sub_pixel_variance16x64_sse2, NULL, svt_aom_sub_pixel_variance16x64_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x64_avx2, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x8, svt_aom_sub_pixel_variance16x8_c, NULL, NULL, svt_aom_sub_pixel_variance16x8_sse2, NULL, svt_aom_sub_pixel_variance16x8_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x8_avx2, NULL);
-    SET_SSE2_SSSE3_AVX2_AVX512(svt_aom_sub_pixel_variance32x16, svt_aom_sub_pixel_variance32x16_c, svt_aom_sub_pixel_variance32x16_sse2, svt_aom_sub_pixel_variance32x16_ssse3, svt_aom_sub_pixel_variance32x16_avx2, svt_aom_sub_pixel_variance32x16_avx512);
-    SET_SSE2_SSSE3_AVX2_AVX512(svt_aom_sub_pixel_variance32x32, svt_aom_sub_pixel_variance32x32_c, svt_aom_sub_pixel_variance32x32_sse2, svt_aom_sub_pixel_variance32x32_ssse3, svt_aom_sub_pixel_variance32x32_avx2, svt_aom_sub_pixel_variance32x32_avx512);
-    SET_SSE2_SSSE3_AVX2_AVX512(svt_aom_sub_pixel_variance32x64, svt_aom_sub_pixel_variance32x64_c, svt_aom_sub_pixel_variance32x64_sse2, svt_aom_sub_pixel_variance32x64_ssse3, svt_aom_sub_pixel_variance32x64_avx2, svt_aom_sub_pixel_variance32x64_avx512);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance32x8, svt_aom_sub_pixel_variance32x8_c, NULL, NULL, svt_aom_sub_pixel_variance32x8_sse2, NULL, svt_aom_sub_pixel_variance32x8_ssse3, NULL, NULL, NULL, NULL, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance4x16, svt_aom_sub_pixel_variance4x16_c, NULL, NULL, svt_aom_sub_pixel_variance4x16_sse2, NULL, svt_aom_sub_pixel_variance4x16_ssse3, NULL, NULL, NULL, NULL, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance4x4, svt_aom_sub_pixel_variance4x4_c, NULL, NULL, svt_aom_sub_pixel_variance4x4_sse2, NULL, svt_aom_sub_pixel_variance4x4_ssse3, NULL, NULL, NULL, NULL, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance4x8, svt_aom_sub_pixel_variance4x8_c, NULL, NULL, svt_aom_sub_pixel_variance4x8_sse2, NULL, svt_aom_sub_pixel_variance4x8_ssse3, NULL, NULL, NULL, NULL, NULL);
-    SET_SSE2_SSSE3_AVX2_AVX512(svt_aom_sub_pixel_variance64x128, svt_aom_sub_pixel_variance64x128_c, svt_aom_sub_pixel_variance64x128_sse2, svt_aom_sub_pixel_variance64x128_ssse3, svt_aom_sub_pixel_variance64x128_avx2, svt_aom_sub_pixel_variance64x128_avx512);
-    SET_SSE2_SSSE3(svt_aom_sub_pixel_variance64x16, svt_aom_sub_pixel_variance64x16_c, svt_aom_sub_pixel_variance64x16_sse2, svt_aom_sub_pixel_variance64x16_ssse3);
-    SET_SSE2_SSSE3_AVX2_AVX512(svt_aom_sub_pixel_variance64x32, svt_aom_sub_pixel_variance64x32_c, svt_aom_sub_pixel_variance64x32_sse2, svt_aom_sub_pixel_variance64x32_ssse3, svt_aom_sub_pixel_variance64x32_avx2, svt_aom_sub_pixel_variance64x32_avx512);
-    SET_SSE2_SSSE3_AVX2_AVX512(svt_aom_sub_pixel_variance64x64, svt_aom_sub_pixel_variance64x64_c, svt_aom_sub_pixel_variance64x64_sse2, svt_aom_sub_pixel_variance64x64_ssse3, svt_aom_sub_pixel_variance64x64_avx2, svt_aom_sub_pixel_variance64x64_avx512);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance8x16, svt_aom_sub_pixel_variance8x16_c, NULL, NULL, svt_aom_sub_pixel_variance8x16_sse2, NULL, svt_aom_sub_pixel_variance8x16_ssse3, NULL, NULL, NULL, NULL, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance8x32, svt_aom_sub_pixel_variance8x32_c, NULL, NULL, svt_aom_sub_pixel_variance8x32_sse2, NULL, svt_aom_sub_pixel_variance8x32_ssse3, NULL, NULL, NULL, NULL, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance8x4, svt_aom_sub_pixel_variance8x4_c, NULL, NULL, svt_aom_sub_pixel_variance8x4_sse2, NULL, svt_aom_sub_pixel_variance8x4_ssse3, NULL, NULL, NULL, NULL, NULL);
-    SET_FUNCTIONS(svt_aom_sub_pixel_variance8x8, svt_aom_sub_pixel_variance8x8_c, NULL, NULL, svt_aom_sub_pixel_variance8x8_sse2, NULL, svt_aom_sub_pixel_variance8x8_ssse3, NULL, NULL, NULL, NULL, NULL);
+#endif
 
+    SET_SSE2_SSSE3_AVX2_AVX512_NEON(svt_aom_sub_pixel_variance128x128, svt_aom_sub_pixel_variance128x128_c, svt_aom_sub_pixel_variance128x128_sse2, svt_aom_sub_pixel_variance128x128_ssse3, svt_aom_sub_pixel_variance128x128_avx2, svt_aom_sub_pixel_variance128x128_avx512, aom_sub_pixel_variance128x128_neon);
+    SET_SSE2_SSSE3_AVX2_AVX512_NEON(svt_aom_sub_pixel_variance128x64, svt_aom_sub_pixel_variance128x64_c, svt_aom_sub_pixel_variance128x64_sse2, svt_aom_sub_pixel_variance128x64_ssse3, svt_aom_sub_pixel_variance128x64_avx2, svt_aom_sub_pixel_variance128x64_avx512, aom_sub_pixel_variance128x64_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x16, svt_aom_sub_pixel_variance16x16_c, NULL, NULL, svt_aom_sub_pixel_variance16x16_sse2, NULL, svt_aom_sub_pixel_variance16x16_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x16_avx2, NULL, aom_sub_pixel_variance16x16_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x32, svt_aom_sub_pixel_variance16x32_c, NULL, NULL, svt_aom_sub_pixel_variance16x32_sse2, NULL, svt_aom_sub_pixel_variance16x32_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x32_avx2, NULL, aom_sub_pixel_variance16x32_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x4, svt_aom_sub_pixel_variance16x4_c, NULL, NULL, svt_aom_sub_pixel_variance16x4_sse2, NULL, svt_aom_sub_pixel_variance16x4_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x4_avx2, NULL, aom_sub_pixel_variance16x4_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x64, svt_aom_sub_pixel_variance16x64_c, NULL, NULL, svt_aom_sub_pixel_variance16x64_sse2, NULL, svt_aom_sub_pixel_variance16x64_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x64_avx2, NULL, aom_sub_pixel_variance16x64_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance16x8, svt_aom_sub_pixel_variance16x8_c, NULL, NULL, svt_aom_sub_pixel_variance16x8_sse2, NULL, svt_aom_sub_pixel_variance16x8_ssse3, NULL, NULL, NULL, svt_aom_sub_pixel_variance16x8_avx2, NULL, aom_sub_pixel_variance16x8_neon);
+    SET_SSE2_SSSE3_AVX2_AVX512_NEON(svt_aom_sub_pixel_variance32x16, svt_aom_sub_pixel_variance32x16_c, svt_aom_sub_pixel_variance32x16_sse2, svt_aom_sub_pixel_variance32x16_ssse3, svt_aom_sub_pixel_variance32x16_avx2, svt_aom_sub_pixel_variance32x16_avx512, aom_sub_pixel_variance32x16_neon);
+    SET_SSE2_SSSE3_AVX2_AVX512_NEON(svt_aom_sub_pixel_variance32x32, svt_aom_sub_pixel_variance32x32_c, svt_aom_sub_pixel_variance32x32_sse2, svt_aom_sub_pixel_variance32x32_ssse3, svt_aom_sub_pixel_variance32x32_avx2, svt_aom_sub_pixel_variance32x32_avx512, aom_sub_pixel_variance32x32_neon);
+    SET_SSE2_SSSE3_AVX2_AVX512_NEON(svt_aom_sub_pixel_variance32x64, svt_aom_sub_pixel_variance32x64_c, svt_aom_sub_pixel_variance32x64_sse2, svt_aom_sub_pixel_variance32x64_ssse3, svt_aom_sub_pixel_variance32x64_avx2, svt_aom_sub_pixel_variance32x64_avx512, aom_sub_pixel_variance32x64_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance32x8, svt_aom_sub_pixel_variance32x8_c, NULL, NULL, svt_aom_sub_pixel_variance32x8_sse2, NULL, svt_aom_sub_pixel_variance32x8_ssse3, NULL, NULL, NULL, NULL, NULL, aom_sub_pixel_variance32x8_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance4x16, svt_aom_sub_pixel_variance4x16_c, NULL, NULL, svt_aom_sub_pixel_variance4x16_sse2, NULL, svt_aom_sub_pixel_variance4x16_ssse3, NULL, NULL, NULL, NULL, NULL, aom_sub_pixel_variance4x16_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance4x4, svt_aom_sub_pixel_variance4x4_c, NULL, NULL, svt_aom_sub_pixel_variance4x4_sse2, NULL, svt_aom_sub_pixel_variance4x4_ssse3, NULL, NULL, NULL, NULL, NULL, aom_sub_pixel_variance4x4_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance4x8, svt_aom_sub_pixel_variance4x8_c, NULL, NULL, svt_aom_sub_pixel_variance4x8_sse2, NULL, svt_aom_sub_pixel_variance4x8_ssse3, NULL, NULL, NULL, NULL, NULL, aom_sub_pixel_variance4x8_neon);
+    SET_SSE2_SSSE3_AVX2_AVX512_NEON(svt_aom_sub_pixel_variance64x128, svt_aom_sub_pixel_variance64x128_c, svt_aom_sub_pixel_variance64x128_sse2, svt_aom_sub_pixel_variance64x128_ssse3, svt_aom_sub_pixel_variance64x128_avx2, svt_aom_sub_pixel_variance64x128_avx512, aom_sub_pixel_variance64x128_neon);
+    SET_SSE2_SSSE3_NEON(svt_aom_sub_pixel_variance64x16, svt_aom_sub_pixel_variance64x16_c, svt_aom_sub_pixel_variance64x16_sse2, svt_aom_sub_pixel_variance64x16_ssse3, aom_sub_pixel_variance64x16_neon);
+    SET_SSE2_SSSE3_AVX2_AVX512_NEON(svt_aom_sub_pixel_variance64x32, svt_aom_sub_pixel_variance64x32_c, svt_aom_sub_pixel_variance64x32_sse2, svt_aom_sub_pixel_variance64x32_ssse3, svt_aom_sub_pixel_variance64x32_avx2, svt_aom_sub_pixel_variance64x32_avx512, aom_sub_pixel_variance64x32_neon);
+    SET_SSE2_SSSE3_AVX2_AVX512_NEON(svt_aom_sub_pixel_variance64x64, svt_aom_sub_pixel_variance64x64_c, svt_aom_sub_pixel_variance64x64_sse2, svt_aom_sub_pixel_variance64x64_ssse3, svt_aom_sub_pixel_variance64x64_avx2, svt_aom_sub_pixel_variance64x64_avx512, aom_sub_pixel_variance64x64_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance8x16, svt_aom_sub_pixel_variance8x16_c, NULL, NULL, svt_aom_sub_pixel_variance8x16_sse2, NULL, svt_aom_sub_pixel_variance8x16_ssse3, NULL, NULL, NULL, NULL, NULL, aom_sub_pixel_variance8x16_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance8x32, svt_aom_sub_pixel_variance8x32_c, NULL, NULL, svt_aom_sub_pixel_variance8x32_sse2, NULL, svt_aom_sub_pixel_variance8x32_ssse3, NULL, NULL, NULL, NULL, NULL, aom_sub_pixel_variance8x32_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance8x4, svt_aom_sub_pixel_variance8x4_c, NULL, NULL, svt_aom_sub_pixel_variance8x4_sse2, NULL, svt_aom_sub_pixel_variance8x4_ssse3, NULL, NULL, NULL, NULL, NULL, aom_sub_pixel_variance8x4_neon);
+    SET_FUNCTIONS(svt_aom_sub_pixel_variance8x8, svt_aom_sub_pixel_variance8x8_c, NULL, NULL, svt_aom_sub_pixel_variance8x8_sse2, NULL, svt_aom_sub_pixel_variance8x8_ssse3, NULL, NULL, NULL, NULL, NULL, aom_sub_pixel_variance8x8_neon);
+ 
     //QIQ
     //transform
     SET_SSE41(svt_av1_fwd_txfm2d_4x4, svt_av1_transform_two_d_4x4_c, svt_av1_fwd_txfm2d_4x4_sse4_1);
@@ -407,7 +478,7 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_AVX2(svt_av1_get_gradient_hist, svt_av1_get_gradient_hist_c, svt_av1_get_gradient_hist_avx2);
     SET_SSE2_AVX2(svt_av1_get_nz_map_contexts, svt_av1_get_nz_map_contexts_c, svt_av1_get_nz_map_contexts_sse2, svt_av1_get_nz_map_contexts_avx2);
     SET_AVX2_AVX512(svt_search_one_dual, svt_search_one_dual_c, svt_search_one_dual_avx2, svt_search_one_dual_avx512);
-    SET_SSE41_AVX2_AVX512(svt_sad_loop_kernel, svt_sad_loop_kernel_c, svt_sad_loop_kernel_sse4_1_intrin, svt_sad_loop_kernel_avx2_intrin, svt_sad_loop_kernel_avx512_intrin);
+    SET_SSE41_AVX2_AVX512_NEON(svt_sad_loop_kernel, svt_sad_loop_kernel_c, svt_sad_loop_kernel_sse4_1_intrin, svt_sad_loop_kernel_avx2_intrin, svt_sad_loop_kernel_avx512_intrin, svt_sad_loop_kernel_neon_intrin);
     SET_SSE41_AVX2(svt_av1_apply_zz_based_temporal_filter_planewise_medium, svt_av1_apply_zz_based_temporal_filter_planewise_medium_c, svt_av1_apply_zz_based_temporal_filter_planewise_medium_sse4_1, svt_av1_apply_zz_based_temporal_filter_planewise_medium_avx2);
     SET_SSE41_AVX2(svt_av1_apply_zz_based_temporal_filter_planewise_medium_hbd, svt_av1_apply_zz_based_temporal_filter_planewise_medium_hbd_c, svt_av1_apply_zz_based_temporal_filter_planewise_medium_hbd_sse4_1, svt_av1_apply_zz_based_temporal_filter_planewise_medium_hbd_avx2);
     SET_SSE41_AVX2(svt_av1_apply_temporal_filter_planewise_medium, svt_av1_apply_temporal_filter_planewise_medium_c, svt_av1_apply_temporal_filter_planewise_medium_sse4_1, svt_av1_apply_temporal_filter_planewise_medium_avx2);
@@ -418,7 +489,7 @@ void svt_aom_setup_rtcd_internal(EbCpuFlags flags) {
     SET_SSE41_AVX2(downsample_2d, svt_aom_downsample_2d_c, svt_aom_downsample_2d_sse4_1, svt_aom_downsample_2d_avx2);
     SET_SSE41_AVX2(svt_ext_sad_calculation_8x8_16x16, svt_ext_sad_calculation_8x8_16x16_c, svt_ext_sad_calculation_8x8_16x16_sse4_1_intrin, svt_ext_sad_calculation_8x8_16x16_avx2_intrin);
     SET_SSE41(svt_ext_sad_calculation_32x32_64x64, svt_ext_sad_calculation_32x32_64x64_c, svt_ext_sad_calculation_32x32_64x64_sse4_intrin);
-    SET_SSE41_AVX2(svt_ext_all_sad_calculation_8x8_16x16, svt_ext_all_sad_calculation_8x8_16x16_c, svt_ext_all_sad_calculation_8x8_16x16_sse4_1, svt_ext_all_sad_calculation_8x8_16x16_avx2);
+    SET_SSE41_AVX2_NEON(svt_ext_all_sad_calculation_8x8_16x16, svt_ext_all_sad_calculation_8x8_16x16_c, svt_ext_all_sad_calculation_8x8_16x16_sse4_1, svt_ext_all_sad_calculation_8x8_16x16_avx2, svt_ext_all_sad_calculation_8x8_16x16_neon);
     SET_SSE41_AVX2(svt_ext_eight_sad_calculation_32x32_64x64, svt_ext_eight_sad_calculation_32x32_64x64_c, svt_ext_eight_sad_calculation_32x32_64x64_sse4_1, svt_ext_eight_sad_calculation_32x32_64x64_avx2);
     SET_SSE2(svt_initialize_buffer_32bits, svt_initialize_buffer_32bits_c, svt_initialize_buffer_32bits_sse2_intrin);
     SET_SSE41_AVX2(svt_nxm_sad_kernel_sub_sampled, svt_nxm_sad_kernel_helper_c, svt_nxm_sad_kernel_sub_sampled_helper_sse4_1, svt_nxm_sad_kernel_sub_sampled_helper_avx2);
diff --git a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
index 849add0..64587ac 100644
--- a/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
+++ b/Source/Lib/Encoder/Codec/aom_dsp_rtcd.h
@@ -211,6 +211,9 @@ extern "C" {
     RTCD_EXTERN int64_t(*svt_av1_block_error)(const TranLow *coeff, const TranLow *dqcoeff, intptr_t block_size, int64_t *ssz);
     RTCD_EXTERN void(*svt_smooth_v_predictor)(uint8_t *dst, ptrdiff_t stride, int32_t bw, int32_t bh, const uint8_t *above, const uint8_t *left);
     void svt_get_proj_subspace_c(const uint8_t *src8, int width, int height, int src_stride, const uint8_t *dat8, int dat_stride, int use_highbitdepth, int32_t *flt0, int flt0_stride, int32_t *flt1, int flt1_stride, int *xq, const SgrParamsType *params);
+#ifdef ARCH_AARCH64
+    void svt_get_proj_subspace_neon(const uint8_t *src8, int width, int height, int src_stride, const uint8_t *dat8, int dat_stride, int use_highbitdepth, int32_t *flt0, int flt0_stride, int32_t *flt1, int flt1_stride, int *xq, const SgrParamsType *params);
+#endif
     RTCD_EXTERN void(*svt_get_proj_subspace)(const uint8_t *src8, int width, int height, int src_stride, const uint8_t *dat8, int dat_stride, int use_highbitdepth, int32_t *flt0, int flt0_stride, int32_t *flt1, int flt1_stride, int *xq, const SgrParamsType *params);
     uint64_t svt_handle_transform16x64_c(int32_t *output);
     RTCD_EXTERN uint64_t(*svt_handle_transform16x64)(int32_t *output);
@@ -577,178 +580,157 @@ extern "C" {
     void svt_av1_calc_target_weighted_pred_above_c(uint8_t is16bit, MacroBlockD *xd, int rel_mi_col, uint8_t nb_mi_width, MbModeInfo *nb_mi, void *fun_ctxt, const int num_planes);
     RTCD_EXTERN void (*svt_av1_calc_target_weighted_pred_left)(uint8_t is16bit, MacroBlockD *xd, int rel_mi_row, uint8_t nb_mi_height, MbModeInfo *nb_mi, void *fun_ctxt, const int num_planes);
     void svt_av1_calc_target_weighted_pred_left_c(uint8_t is16bit, MacroBlockD *xd, int rel_mi_row, uint8_t nb_mi_height, MbModeInfo *nb_mi, void *fun_ctxt, const int num_planes);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance128x128_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance128x128_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance128x128_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance128x128_avx512(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance128x128_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance128x128)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance128x64_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance128x64_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance128x64_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance128x64_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance128x64_avx512(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance128x64_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance128x64)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance16x16_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance16x16_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x16_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x16_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance16x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance16x16)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance16x32_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance16x32_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x32_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x32_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance16x32_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance16x32)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance16x4_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance16x4_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x4_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x4_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance16x4_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance16x4)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance16x64_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance16x64_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x64_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x64_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance16x64_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance16x64)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance16x8_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance16x8_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x8_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance16x8_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance16x8_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance16x8)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance32x16_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance32x16_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x16_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x16_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x16_avx512(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance32x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance32x16)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance32x32_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance32x32_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x32_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x32_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x32_avx512(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance32x32_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance32x32)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance32x64_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance32x64_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x64_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x64_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x64_avx512(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance32x64_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance32x64)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance32x8_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance32x8_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance32x8_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance32x8_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance32x8)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance4x16_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance4x16_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance4x16_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance4x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance4x16)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance4x4_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance4x4_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance4x4_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance4x4_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance4x4)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance4x8_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance4x8_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance4x8_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance4x8_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance4x8)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance64x128_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance64x128_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x128_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x128_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x128_avx512(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance64x128_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance64x128)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance64x16_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance64x16_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x16_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance64x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance64x16)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance64x32_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance64x32_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x32_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x32_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x32_avx512(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance64x32_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance64x32)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance64x64_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance64x64_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x64_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x64_avx2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance64x64_avx512(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance64x64_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance64x64)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance8x16_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance8x16_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance8x16_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance8x16_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance8x16)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance8x32_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance8x32_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance8x32_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance8x32_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance8x32)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance8x4_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance8x4_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance8x4_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance8x4_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance8x4)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
     uint32_t svt_aom_sub_pixel_variance8x8_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#ifdef ARCH_X86_64
     uint32_t svt_aom_sub_pixel_variance8x8_sse2(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     uint32_t svt_aom_sub_pixel_variance8x8_ssse3(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#endif
+    uint32_t aom_sub_pixel_variance8x8_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     RTCD_EXTERN uint32_t(*svt_aom_sub_pixel_variance8x8)(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
     void svt_aom_ifft16x16_float_c(const float *input, float *temp, float *output);
     RTCD_EXTERN void(*svt_aom_ifft16x16_float)(const float *input, float *temp, float *output);
@@ -908,7 +890,7 @@ extern "C" {
     double svt_ssim_8x8_hbd_c(const uint16_t* s, uint32_t sp, const uint16_t* r, uint32_t rp);
     RTCD_EXTERN double (*svt_ssim_4x4_hbd)(const uint16_t* s, uint32_t sp, const uint16_t* r, uint32_t rp);
     double svt_ssim_4x4_hbd_c(const uint16_t* s, uint32_t sp, const uint16_t* r, uint32_t rp);
-#ifdef ARCH_X86_64
+#if defined(ARCH_X86_64) || defined(ARCH_AARCH64)
     int64_t svt_aom_sse_avx2(const uint8_t *a, int a_stride, const uint8_t *b, int b_stride, int width, int height);
     int64_t svt_aom_highbd_sse_avx2(const uint8_t *a8, int a_stride, const uint8_t *b8, int b_stride, int width, int height);
 
@@ -932,7 +914,9 @@ extern "C" {
     int64_t svt_av1_lowbd_pixel_proj_error_sse4_1(const uint8_t *src8, int32_t width, int32_t height, int32_t src_stride, const uint8_t *dat8, int32_t dat_stride, int32_t *flt0, int32_t flt0_stride, int32_t *flt1, int32_t flt1_stride, int32_t xq[2], const SgrParamsType *params);
     int64_t svt_av1_lowbd_pixel_proj_error_avx2(const uint8_t *src8, int32_t width, int32_t height, int32_t src_stride, const uint8_t *dat8, int32_t dat_stride, int32_t *flt0, int32_t flt0_stride, int32_t *flt1, int32_t flt1_stride, int32_t xq[2], const SgrParamsType *params);
     int64_t svt_av1_lowbd_pixel_proj_error_avx512(const uint8_t *src8, int32_t width, int32_t height, int32_t src_stride, const uint8_t *dat8, int32_t dat_stride, int32_t *flt0, int32_t flt0_stride, int32_t *flt1, int32_t flt1_stride, int32_t xq[2], const SgrParamsType *params);
-
+#ifdef ARCH_AARCH64
+    int64_t svt_av1_lowbd_pixel_proj_error_neon(const uint8_t *src8, int32_t width, int32_t height, int32_t src_stride, const uint8_t *dat8, int32_t dat_stride, int32_t *flt0, int32_t flt0_stride, int32_t *flt1, int32_t flt1_stride, int32_t xq[2], const SgrParamsType *params);
+#endif
     int64_t svt_av1_highbd_pixel_proj_error_sse4_1(const uint8_t *src8, int32_t width, int32_t height, int32_t src_stride, const uint8_t *dat8, int32_t dat_stride, int32_t *flt0, int32_t flt0_stride, int32_t *flt1, int32_t flt1_stride, int32_t xq[2], const SgrParamsType *params);
     int64_t svt_av1_highbd_pixel_proj_error_avx2(const uint8_t *src8, int32_t width, int32_t height, int32_t src_stride, const uint8_t *dat8, int32_t dat_stride, int32_t *flt0, int32_t flt0_stride, int32_t *flt1, int32_t flt1_stride, int32_t xq[2], const SgrParamsType *params);
 
@@ -1406,6 +1390,10 @@ extern "C" {
 
     unsigned int svt_aom_obmc_variance8x16_sse4_1(const uint8_t *pre, int pre_stride, const int32_t *wsrc, const int32_t *mask, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_obmc_variance8x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_obmc_variance8x32_sse4_1(const uint8_t *pre, int pre_stride, const int32_t *wsrc, const int32_t *mask, unsigned int *sse);
 
     unsigned int svt_aom_obmc_variance8x4_sse4_1(const uint8_t *pre, int pre_stride, const int32_t *wsrc, const int32_t *mask, unsigned int *sse);
@@ -1417,46 +1405,110 @@ extern "C" {
 
     unsigned int svt_aom_variance4x8_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance4x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance4x16_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
     unsigned int svt_aom_variance8x4_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance8x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance8x8_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance8x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance8x16_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance8x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance8x32_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance4x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance16x4_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
     unsigned int svt_aom_variance16x8_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance16x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance16x16_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance16x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance16x32_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance16x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance16x64_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
     unsigned int svt_aom_variance32x8_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
     unsigned int svt_aom_variance32x16_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance32x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance32x32_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance32x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance32x64_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance32x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance64x16_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
     unsigned int svt_aom_variance64x32_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance64x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance64x64_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance64x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance64x128_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance64x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance128x64_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance128x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance128x128_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifdef ARCH_AARCH64
+    unsigned int aom_variance128x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
+#endif
+
     unsigned int svt_aom_variance32x8_avx512(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
     unsigned int svt_aom_variance32x16_avx512(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
@@ -1492,6 +1544,7 @@ extern "C" {
     unsigned int svt_aom_variance128x64_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
     unsigned int svt_aom_variance128x128_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
 
+#ifndef ARCH_AARCH64
     unsigned int svt_aom_highbd_10_variance8x8_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
     unsigned int svt_aom_highbd_10_variance8x16_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
     unsigned int svt_aom_highbd_10_variance8x32_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
@@ -1510,7 +1563,7 @@ extern "C" {
     unsigned int svt_aom_highbd_10_variance64x128_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
     unsigned int svt_aom_highbd_10_variance128x64_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
     unsigned int svt_aom_highbd_10_variance128x128_sse2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
-
+#endif
     unsigned int svt_aom_highbd_10_variance8x8_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
     unsigned int svt_aom_highbd_10_variance8x16_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
     unsigned int svt_aom_highbd_10_variance8x32_avx2(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, unsigned int *sse);
@@ -1598,6 +1651,16 @@ extern "C" {
         uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16,
         uint32_t p_eight_sad16x16[16][8],
         uint32_t p_eight_sad8x8[64][8], Bool sub_sad);
+#ifdef ARCH_AARCH64
+    void svt_sad_loop_kernel_neon_intrin(uint8_t *src, uint32_t src_stride, uint8_t *ref, uint32_t ref_stride, uint32_t block_height, uint32_t block_width, uint64_t *best_sad, int16_t *x_search_center, int16_t *y_search_center, uint32_t src_stride_raw, uint8_t skip_search_line, int16_t search_area_width, int16_t search_area_height);
+    void svt_ext_all_sad_calculation_8x8_16x16_neon(uint8_t *src, uint32_t src_stride, uint8_t *ref,
+        uint32_t ref_stride, uint32_t mv,
+        uint8_t out_8x8,
+        uint32_t *p_best_sad_8x8, uint32_t *p_best_sad_16x16,
+        uint32_t *p_best_mv8x8, uint32_t *p_best_mv16x16,
+        uint32_t p_eight_sad16x16[16][8],
+        uint32_t p_eight_sad8x8[64][8], Bool sub_sad);
+#endif
     void svt_ext_all_sad_calculation_8x8_16x16_sse4_1(uint8_t *src, uint32_t src_stride, uint8_t *ref,
         uint32_t ref_stride, uint32_t mv,
         uint8_t out_8x8,
diff --git a/Source/Lib/Encoder/Globals/EbEncHandle.c b/Source/Lib/Encoder/Globals/EbEncHandle.c
index b211b66..9663031 100644
--- a/Source/Lib/Encoder/Globals/EbEncHandle.c
+++ b/Source/Lib/Encoder/Globals/EbEncHandle.c
@@ -139,6 +139,9 @@ static const char *get_asm_level_name_str(EbCpuFlags cpu_flags) {
         EbCpuFlags flags;
     } param_maps[] = {
         {"c",       0},
+#ifdef ARCH_AARCH64
+        {"neon",    EB_CPU_FLAGS_NEON}
+#else
         {"mmx",     EB_CPU_FLAGS_MMX},
         {"sse",     EB_CPU_FLAGS_SSE},
         {"sse2",    EB_CPU_FLAGS_SSE2},
@@ -149,6 +152,7 @@ static const char *get_asm_level_name_str(EbCpuFlags cpu_flags) {
         {"avx",     EB_CPU_FLAGS_AVX},
         {"avx2",    EB_CPU_FLAGS_AVX2},
         {"avx512",  EB_CPU_FLAGS_AVX512F}
+#endif // ARCH_AARCH64
     };
     const uint32_t para_map_size = sizeof(param_maps) / sizeof(param_maps[0]);
     int32_t i;
@@ -841,7 +845,7 @@ static EbErrorType load_default_buffer_configuration_settings(
         /******************************************************************
         * Platform detection, limit cpu flags to hardware available CPU
         ******************************************************************/
-#ifdef ARCH_X86_64
+#if defined(ARCH_X86_64) || defined(ARCH_AARCH64)
         const EbCpuFlags cpu_flags = svt_aom_get_cpu_flags();
         const EbCpuFlags cpu_flags_to_use = svt_aom_get_cpu_flags_to_use();
         scs->static_config.use_cpu_flags &= cpu_flags_to_use;
diff --git a/Source/Lib/Encoder/Globals/EbEncSettings.c b/Source/Lib/Encoder/Globals/EbEncSettings.c
index f408986..25fad36 100644
--- a/Source/Lib/Encoder/Globals/EbEncSettings.c
+++ b/Source/Lib/Encoder/Globals/EbEncSettings.c
@@ -469,8 +469,12 @@ EbErrorType svt_av1_verify_settings(SequenceControlSet *scs) {
     if (config->use_cpu_flags & EB_CPU_FLAGS_INVALID) {
         SVT_ERROR(
             "Instance %u: param '--asm' have invalid value.\n"
+#ifdef ARCH_AARCH64
+            "Value should be [0 - 2] or [c, neon, max]\n",
+#else
             "Value should be [0 - 11] or [c, mmx, sse, sse2, sse3, ssse3, sse4_1, sse4_2, avx, "
             "avx2, avx512, max]\n",
+#endif // ARCH_AARCH64
             channel_number + 1);
         return_error = EB_ErrorBadParameter;
     }
@@ -1494,6 +1498,12 @@ static EbErrorType str_to_asm(const char *nptr, EbCpuFlags *out) {
     } simds[] = {
         {"c", 0},
         {"0", 0},
+#ifdef ARCH_AARCH64
+        {"neon", (EB_CPU_FLAGS_NEON << 1) - 1},
+        {"1", (EB_CPU_FLAGS_NEON << 1) - 1},
+        {"max", EB_CPU_FLAGS_ALL},
+        {"2", EB_CPU_FLAGS_ALL},
+#else
         {"mmx", (EB_CPU_FLAGS_MMX << 1) - 1},
         {"1", (EB_CPU_FLAGS_MMX << 1) - 1},
         {"sse", (EB_CPU_FLAGS_SSE << 1) - 1},
@@ -1516,6 +1526,7 @@ static EbErrorType str_to_asm(const char *nptr, EbCpuFlags *out) {
         {"10", (EB_CPU_FLAGS_AVX512VL << 1) - 1},
         {"max", EB_CPU_FLAGS_ALL},
         {"11", EB_CPU_FLAGS_ALL},
+#endif // ARCH_AARCH64
     };
     const size_t simds_size = sizeof(simds) / sizeof(simds[0]);
 
-- 
2.39.3 (Apple Git-145)

